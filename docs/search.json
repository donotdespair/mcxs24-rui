[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "",
    "text": "Objective: Construct a Bayesian Vector Autoregression model to forecast major macroeconomic indicators for the United States, Australia, Japan, and China to facilitate an investigation into the prospective interdependencies between the economies of these nations.\nQuestion: This research project will examine how trade relationships, investment flows, monetary policy environments, and economic performances within the United States, Australia, Japan and China mutually influence each other, and assess the implications of these interactions for predicting future values of these economic indicators.\nMotivation: Since the onset of the COVID-19 pandemic, the global economic landscape has witnessed a series of unprecedented shifts in key macroeconomic indicators, spurred by governments‚Äô adoption of varied expansionary monetary policies. Initially, to buffer their economies, many nations implemented expansive monetary strategies, later swiftly transitioning to interest rate hikes in a bid to manage surging inflation rates‚Äîa scenario not seen in decades. The pandemic‚Äôs disruption to trade further exacerbated inflationary pressures for some economies, highlighting the intricate interdependencies among major economies with significant trade and financial ties. This period recorded stark contrast in inflation levels, with unprecedented highs in the US and Australia and notably low inflation in China and Japan. Amidst this turmoil, a divergence in economic paths also became apparent, thei9 United States and Australia have witness robust economic rebounds, whereas China and Japan saw more tepid recoveries. This research aims to dissect the nuanced web of economic interdependencies between the United States, Australia, Japan, and China, analyzing how their trade relationships, investment flows, and monetary policy environments have mutually influenced their economic performances. Additionally, it seeks to understand the ramifications of these dynamics for the predictive accuracy of future economic indicators, offering insights into the evolving global economic order."
  },
  {
    "objectID": "index.html#data-plots",
    "href": "index.html#data-plots",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Data Plots",
    "text": "Data Plots"
  },
  {
    "objectID": "index.html#stationarity-check",
    "href": "index.html#stationarity-check",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Stationarity Check",
    "text": "Stationarity Check\nStationary Tests\nThe Augmented Dickey-Fuller Test is used in this section to test the null hypothesis that a unit root is present in the time series and the time series is non-stationary.\n\n\n\nADF Test Results for CPI Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.258408\n4\n0.08079872\nJP_CPI\n\n\n-4.299916\n4\n0.01\nCN_CPI\n\n\n-3.55799\n4\n0.0394958\nUS_CPI\n\n\n-3.333834\n4\n0.06822768\nAU_CPI\n\n\n\n\n\nThe lag order chosen in the ADF test is 4, which is appropriate given our data is of quarterly frequency. The result of the ADF test on the CPI data shows that we do not have enough evidence to reject the null hypothesis that the CPI series is unit root non-stationary at 1% significance level, except for China.\n\n\n\nADF Test Results for Foreign Direct Investment (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-2.26286\n4\n0.4686993\nJP_FDI\n\n\n-2.7189\n4\n0.285367\nCN_FDI\n\n\n-3.130793\n4\n0.1197816\nUS_FDI\n\n\n-1.607673\n4\n0.7320914\nAU_FDI\n\n\n\n\n\nThe ADF results shows that we do not have enough evidence to reject the null hypothesis that the foreign direct investment data is not unit-root stationary for Australia, the United States and Japan at 1% significance level and we do have enough evidence to reject the null hypothesis for China at 1% significance level.\n\n\n\nADF Test Results for Exchange Rate (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-1.791111\n4\n0.6635047\nJP_XCH\n\n\n-1.565957\n4\n0.7575635\nCN_XCH\n\n\n-2.56175\n4\n0.3415666\nAU_XCH\n\n\n\n\n\nThe result of the ADF test on the exchange rate data shows that we do not have enough evidence to reject the null hypothesis that the exchange rate against the dollar series is unit root non-stationary at 1% significance level.\n\n\n\nADF Test Results for Balance of Payments (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-2.914308\n4\n0.2025054\nJP_BOP\n\n\n-2.50488\n4\n0.3695164\nCN_BOP\n\n\n-2.533578\n4\n0.3578101\nUS_BOP\n\n\n-2.058287\n4\n0.5516879\nAU_BOP\n\n\n\n\n\nThe result of the ADF test on the balance of payments data shows that we do not have enough evidence to reject the null hypothesis that the balance of payments series is unit root non-stationary at 1% significance level for all countries.\n\n\n\nADF Test Results for GDP (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.499383\n4\n0.04535076\nJP_GDP\n\n\n-2.472818\n4\n0.3802514\nCN_GDP\n\n\n0.7030938\n4\n0.99\nUS_GDP\n\n\n-2.030497\n4\n0.5639204\nAU_GDP\n\n\n\n\n\nThe result of the ADF test on the GDP data shows that we do not have enough evidence to reject the null hypothesis that the GDP series is unit root non-stationary at 5% significance level."
  },
  {
    "objectID": "index.html#model-1-standard-bvarp-model",
    "href": "index.html#model-1-standard-bvarp-model",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.1 Model 1: Standard BVAR(p) Model",
    "text": "3.1 Model 1: Standard BVAR(p) Model\n\n3.1.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, I_T)\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(I_T\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\\(x_{t}^T = \\left( \\begin{array}{cccc}1  & y_{t-1} & y_{t-2} & \\cdots & y_{t-p} \\end{array} \\right)\\)\n\nIn our specific application, the Y matrix is formulated as follows:\n\\[\nY = \\begin{pmatrix}\n    \\text{CPI}_{\\text{CN}, p+1} & \\text{XCH}_{\\text{CN}, p+1} & \\log(\\text{GDP})_{\\text{CN}, p+1} & \\text{CPI}_{\\text{US}, p+1} & \\log(\\text{GDP})_{\\text{US}, p+1} & \\text{CPI}_{\\text{JP}, p+1} & \\text{XCH}_{\\text{JP}, p+1} & \\log(\\text{GDP})_{\\text{JP}, p+1} & \\text{CPI}_{\\text{AU}, p+1} & \\text{XCH}_{\\text{AU}, p+1} & \\log(\\text{GDP})_{\\text{AU}, p+1} \\\\\n    \\text{CPI}_{\\text{CN}, p+2} & \\text{XCH}_{\\text{CN}, p+2} & \\log(\\text{GDP})_{\\text{CN}, p+2} & \\text{CPI}_{\\text{US}, p+2} & \\log(\\text{GDP})_{\\text{US}, p+2} & \\text{CPI}_{\\text{JP}, p+2} & \\text{XCH}_{\\text{JP}, p+2} & \\log(\\text{GDP})_{\\text{JP}, p+2} & \\text{CPI}_{\\text{AU}, p+2} & \\text{XCH}_{\\text{AU}, p+2} & \\log(\\text{GDP})_{\\text{AU}, p+2} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n    \\text{CPI}_{\\text{CN}, T} & \\text{XCH}_{\\text{CN}, T} & \\log(\\text{GDP})_{\\text{CN}, T} & \\text{CPI}_{\\text{US}, T} & \\log(\\text{GDP})_{\\text{US}, T} & \\text{CPI}_{\\text{JP}, T} & \\text{XCH}_{\\text{JP}, T} & \\log(\\text{GDP})_{\\text{JP}, T} & \\text{CPI}_{\\text{AU}, T} & \\text{XCH}_{\\text{AU}, T} & \\log(\\text{GDP})_{\\text{AU}, T}\n\\end{pmatrix}\n\\]\nThe Bayesian Vector Autoregression model as formulated above provides a robust framework for investigating the relationships among selected economic indicators across different nations. By employing this model, this research aims to quantitatively measure the influence of one country‚Äôs economic indicators on another, such as how lagged changes in China‚Äôs i consumer price index may influence the GDP growth rate of the United States and vice versa. The BVAR model, with its estimation of coefficients across various lags, offers a deep understanding of both immediate and more delayed economic interactions, which is crucial to analyzing the cyclical nature of trade relationships, investment flows, monetary policy environments, and economic performances and the transmission of these metrics across borders.\nThe strength of this BVAR model lies in its ability to incorporate prior economic knowledge and beliefs into the estimation process. By setting prior distributions for the matrix of coefficients A and the covariance matrix \\(\\Sigma\\), the model can be tailors to reflect established economic theories regarding international economic linkages and the time it takes for policy changes in one country to affect another. By calibrating the prior variances, particularly for the autoregressive coefficients, we can integrate prior knowledge or hypotheses, such as the presence of unit roots or the diminishing influence of distant lags on current values, into the analysis. When interpreting the estimation output, attention will be given to the posterior means and variances of the coefficients, which represent the model‚Äôs ‚Äúlearnt‚Äù understanding of the underlying economic structure. The analysis will be supplemented by forecast error variance decompositions to better understand the proportion of the movements in economic indicators that can be accounted for by their own shocks versus shocks to other variables.\nThe economic context underscoring this analysis is the increased globalization over the past decade, marking an era where economies are more intertwined than ever through trade, capital flows, and policy decisions. This period has witnessed not only the strengthening of global economic ties but also recent calls from political leaders advocating for a reduction in globalization. These contrasting dynamics highlight the complexity of the current global economic landscape, where the push for deeper integration coexists with growing sentiments for retrenchment. This dual trend sets the stage for our investigation, providing a rich context to explore how economic variables across nations influence each other amidst fluctuating levels of global interconnectedness. In this environment, understanding the cross-country spillover effects is vital for policymakers and businesses alike, as decisions made in one country can have far-reaching implications. By addressing these aspects, this research will contribute to the discourse on economic policy formulation, risk assessment, and strategic planning.\n\n\n3.1.2 Prior Settings\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of coefficient matrices A and the row-specific variance matrix \\(\\Sigma\\), and a Minnesota prior on the coefficients A. Specifically, we have:\n\\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0) \\]\n\\[p(\\Sigma) \\propto |\\Sigma|^{-\\frac{\\nu_0+N+1}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\]\n\\[A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\]\n\\[p(A|\\Sigma) \\propto |\\Sigma|^{-\\frac{K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\n\\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\]\n\\[p(A,\\Sigma) \\propto |\\Sigma|^{-\\frac{K+\\nu_0+N+1}{2}} \\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\nwhere\n\\[V_A = diag(\\kappa_2 \\quad \\kappa_1(\\mathbf{p} \\otimes I_N^T))\\]\n\\[\\mathbf{p} = [1 \\quad 2 \\quad ... \\quad p]\\] \\[I_N = [1 \\quad 1 \\quad ... \\quad 1] \\in \\mathbb{R}^N\\] \\[\\kappa_1 \\text{ is the overall shrinkage level for autoregressive slopes}\\] \\[\\kappa_2 \\text{ is the overall shrinkage lvel for the constant term }\\]\nAdditionally, we adopt commonly used values for the hyperparameters as established in the literature.\n\\[A_0 = 0\\]\n\\[v_0 = N+3\\]\n\\[S_0 = I_N\\]\n\\[\\kappa_1 = 0.2^2 \\quad \\kappa_2 = 10^2\\]\nThe hyperparameters \\(\\kappa_1\\) and \\(\\kappa_2\\) are specified in a way such that the coefficient associated with a lag l variable is shrunk more heavily as lag length increases whereas the intercepts are not shrunk to 0.\n\n\n3.1.3 Posterior Distributions\nThe posterior distribution specified above has the form\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{TN}{2}}|\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y)  = \\frac{p(A,\\Sigma,Y)}{p(Y)}\\propto p(A, \\Sigma, Y) \\propto p(Y|A,\\Sigma)\\times p(A,\\Sigma) =  p(Y|A,\\Sigma) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\]\n\\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^TY-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\] Hence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^TX + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^TY + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^TY + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.1.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\nwe use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution:\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nThe draws from joint predictive density can then be obtained using the following algorithm:\n\nSample S draws \\(\\left\\{ A^{(s)}, \\Sigma^{(s)} \\right\\}_{s=1}^{S}\\) from \\(p(A,\\Sigma|Y, X)\\)\nSample S draws \\(\\left\\{ Y_{t+h}^{(s)} \\right\\}_{s=1}^S\\) from \\(Y_{t+h}^{(s)} \\sim \\mathcal{N}_{hN}(Y_{t+h|t}(A^{(s)}), \\mathbb{V}ar[Y_{t+h|t}|A^{(s)}, \\Sigma^{(s)}])\\)\n\nwhere\n\\[\\underset{\\text{hN} \\times1}{Y_{t+h}} = \\begin{pmatrix}\n    Y_{t+1} \\\\\n    Y_{t+2} \\\\\n    \\vdots    \\\\\n    Y_{t+h}\n\\end{pmatrix}\\]\nTo derive the posterior predictive density for \\(Y_{t+h}\\), we first note that:\n\\[p(Y_{t+h}|Y_{t}) = \\int \\int p(Y_{t+h}|Y_{t},A, \\Sigma)\\times p(A,\\Sigma|Y,X)dAd\\Sigma\\] where\n\\[p(y_{t+h}|Y, X, A, \\Sigma, I_T) \\sim \\mathcal{N}_{hN} (Y_{t+h|t}(A), Var(Y_{t+h}|A, \\Sigma))\\]\n\\[Y_{t+h|t}(A) =\\begin{pmatrix}\n    Y_{t+1|t} \\\\\n    Y_{t+2|t} \\\\\n    \\vdots    \\\\\n    Y_{t+h|t}\n\\end{pmatrix} = \\begin{pmatrix}  \n\\mu_0 + A_1 Y_t + \\cdots + A_pY_{t-p+1|t} \\\\\n\\mu_0 + A_1 Y_{t+1|t} + \\cdots + A_p Y_{t-p+2|t}\\\\\n\\vdots \\\\\n\\mu_0 + A_1 Y_{t+h-1|t} + \\cdots + A_p Y_{t+h-p|t}\n\\end{pmatrix}\\]\n\\[\\underset{\\text{(hN-1)} \\times \\text{(hN-1)}}{\\mathbb{V}ar(Y_{t+h|t}|A, \\Sigma)} = \\begin{pmatrix}\n    \\Sigma & \\Sigma \\phi_1^T & \\cdots & \\Sigma \\phi_{h-1}^T \\\\\n    \\phi_1\\Sigma &\\Sigma + \\phi_1 \\Sigma \\phi_1^T  & \\cdots & \\Sigma \\phi_1^T + \\phi_1 \\Sigma \\phi_2^T+ \\cdots + \\phi_{h-2}\\Sigma \\phi_{h-1}^T + \\phi_{h-1} \\Sigma \\phi_{h}^T \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{h-1}\\Sigma\\ & \\phi_1\\Sigma  + \\phi_2 \\Sigma \\Phi_1^T + \\cdots + \\phi_{h} \\Sigma \\phi_{h-1}^T & \\cdots & \\Sigma + \\phi_1 \\Sigma \\phi_1^T + \\cdots+\\phi_{h-2}\\Sigma\\phi_{h-2}^T+\\phi_{h-1}\\Sigma\\phi_{h-1}^T\n\\end{pmatrix}\n\\]\nwhere \\(\\phi_i = J A^i J'\\) are the parameters of the VMA(\\(\\infty\\)) representation of VAR(\\(p\\)) and \\(A\\) is the parameter matrix of \\(VAR(1)\\) representation of VAR(\\(p\\)). \\(A^i\\) is the matrix \\(A\\) raised to the power of \\(i\\). \\[\n\\underset{Np \\times Np}{A} =\n\\begin{pmatrix} A_1 & A_2 & \\cdots & A_{p-1} & A_p \\\\\nI_N & 0_{N \\times N} & \\cdots & 0_{N \\times N} & 0_{N \\times N}  \\\\\n0 & I_N  & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & I_N & 0 \\end{pmatrix}\n\\qquad\n\\underset{\\text{N} \\times \\text{Np}}{J} = [I_N \\quad 0_{N \\times N(p-1)}]\n\\] \\[p(A|Y, X, \\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\] \\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\] as before. In this approach, we are incorporating into our prediction the uncertainties in the estimates of the parameters by integrating over \\(A\\) and \\(\\Sigma\\).\n\n\n3.1.5 Test for Granger Causality\nGranger causality testing, introduced in Granger (1969), is a way to measure whether the past and current values of one time series contains useful information for predicting future values of another. The idea behind granger causality testing is that if a time series does contain predictive information about another, incorporating it into the model should reduce the error variance and improve the precision of the forecasts. The classical approach for testing causality is the Wald test, but it fails to quantify the degree of evidence in the data in favour of or against the causality hypothesis. The Bayesian approach to testing Granger causality based on Baye‚Äôs factor, on the other hand, has the ability to quantify the evidence for and against the hypothesis with a single number. The Baye‚Äôs factor is defined as:\n\\[B_H = \\frac{p[H_0|Y]}{p[H_1|Y]}\\] In the context of testing for VAR(p) models, we have the marginal likelihood under the assumption of no Granger causality is:\n\\[p_0(Y) = \\int p_1(Y| A_{ij} = 0, A_{-(ij)})dA_{-(ij)} = p_1(Y|A_{ij}=0)\\] using Baye‚Äôs rule, we have: \\[p_0(Y) = \\frac{p_1(A_{ij} = 0|Y) \\times p_1(Y)}{p_1(A_{ij} = 0)}\\] We divide the above equation by \\(p(y_t|H_1)\\) to get Baye‚Äôs factor:\n\\[B_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij=0}|Y, \\Sigma)d \\Sigma}{\\int p(A_{ij}=0, \\Sigma)d\\Sigma}\\]\nand the integration is performed using numerical integration methods. The second part is also known as the Savage-Dickey density ratio estimator, which is the ratio of the posterior over the prior density evaluated at \\(A_{ij} = 0\\). Indicative values for interpreting Bayes factors is provided below:\nTo test for Granger causality, we use the equation: \\[B_H = \\frac{p_1(A_{ij} = 0|Y)}{p(A_{ij}=0)} \\] A value much greater than 1 would indicate that the posterior distribution has a higher probability density at \\(A_{ij} = 0\\) than the prior, hence the data provides evidence for the null hypothesis that \\(A_{ij} = 0\\). Conversely, a value much smaller than 1 would suggest that the data provides evidence against the null hypothesis."
  },
  {
    "objectID": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations",
    "text": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations\nIncorporating MA(1) Gaussian innovations in a large BVARs model can lead to a significant enhancement over traditional BVAR models, especially when forecasting macroeconomic variables, for several reasons. By allowing for serial correlation in the innovations term, we will be able to capture the momentum or persistence in economic variables that is often observed in real-world data. Recognizing that shocks may have a lasting impact over several periods can enhance the model‚Äôs ability to predict future values by considering the path-dependent nature of the economy. In addition, our BVAR model with common stochastic volatility is a natural extension to the standard BVAR model and formulated as below:\n\n3.2.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(\\Omega\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\nwith MA(1) innovations, we have, for i = 1,‚Ä¶.,N and t = 1,‚Ä¶,T, \\[e_{t, i} = \\eta_{t, i} + \\psi \\eta_{t-1, i}\\] where \\(|\\psi|&lt;1\\) and \\(\\eta_{t,i} \\sim \\mathcal{N}(0,1)\\)\nIn matrix notation, we have:\n\\[e_i = H_{\\psi} \\eta_i\\]\nwhere\n\\[\ne_i = \\begin{bmatrix}\ne_{1, i} \\\\\ne_{2, i} \\\\\n\\vdots \\\\\ne_{T, i}\n\\end{bmatrix} \\qquad\n\\eta_i = \\begin{bmatrix}\n\\eta_{1, i} \\\\\n\\eta_{2, i} \\\\\n\\vdots \\\\\n\\eta_{T, i}\n\\end{bmatrix} \\qquad\nH_{\\psi} = \\left(\n\\begin{array}{cccc}\n1 & 0  & \\cdots & 0 \\\\\n\\psi & 1  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\psi & 1\n\\end{array}\n\\right) \\qquad\nO_{\\psi} = diag(1+\\psi^2, 1, ..., 1)\n\\]\nHence, we have\n\\[E \\sim \\mathcal{N}(0, H_{\\psi}O_{\\psi}H_{\\psi}^T) \\qquad \\qquad \\Omega = H_{\\psi}O_{\\psi}H_{\\psi}^T\\] Note that the covariance matrix \\(\\Omega\\) depends on \\(\\psi\\) only.\n\n\n3.2.2 Prior Specification\nWe consider a prior independent distributions for \\((A, \\Sigma, \\Omega)\\), specifically, we have: \\[P(A, \\Sigma, \\Omega) = P(A, \\Sigma) \\times P(\\Omega)\\]\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of A and \\(\\Sigma\\) and before, \\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_0, V_A, S_0, \\nu_0)\\]\nWe apply a truncated normal prior on \\(\\psi\\), \\[\\psi \\sim \\mathcal{N}(\\psi_0, V_{\\psi})\\mathbb{1}_{\\{|\\psi|&lt;1\\}}\\]\nFor estimation purposes, we initialize with the following setting:\n\n\\(e_{i1} \\sim \\mathcal{N}(0, 1+\\psi^2)\\)\n\\(\\psi_0 = 0\\) and \\(V_{\\psi} = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1, 1)\n\n\n\n3.2.3 Posterior Distributions\n\nThe posterior distribution of Y specified above has the form:\n\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\n\\[= (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}(1+\\psi^2)^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(H_{\\psi}O_{\\psi}H_{\\psi}^T)^{-1}(Y-XA))\\]\n\nThe joint posterior distribution of A and \\(\\Sigma\\) can be derived as follows:\n\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\] 3. The posterior distribution for the parameter \\(\\psi\\) can be obtained as follows: \\[P(\\psi|Y, A, \\Sigma) = \\frac{P(\\psi, Y, A, \\Sigma)}{P(Y, A, \\Sigma)} \\propto P(\\psi, Y, A, \\Sigma) = P(Y|A, \\Sigma, \\psi) \\times P(A,\\Sigma, \\psi) \\]\n\\[= P(Y|A, \\Sigma, \\psi) \\times P(A, \\Sigma) \\times P(\\psi) \\propto P(Y|A, \\Sigma, \\psi) \\times P(\\psi)\\]\nwe can sample from the posterior distribution of \\(\\psi\\) using an independence-chain Metropolis-Hastings algorithm.\n\n\n3.2.4 Estimation Procedure\nWe obtain posterior estimates of \\(A, \\Sigma, \\psi\\) using a Gibbs sampler, specifically, we initialize \\(\\psi^{(0)}\\) and for s = 1,‚Ä¶,S1+S2, we sequentially sample:\n\n\\(\\Sigma^{(s)} | Y, X, \\psi^{(s-1)} \\sim \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\\(A^{(s)} | Y, X, \\Sigma^{(s)}, \\psi^{(s-1)} \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma^{(s)}, \\bar{V})\\)\n\\(\\psi^{(s)} | Y, X, A^{(s)}, \\Sigma^{(s)} \\propto P(Y|A^{(s)}, \\Sigma^{(s)}, \\psi^{s-1})\\times p(\\psi)\\)\n\nThe Metropolis-Hastings Algorithm for Sampling \\(\\psi\\) is given as follows:\nInitialization:\n\nChoose an initial value \\(\\psi^{(0)}\\) within the bounds \\((-1, 1)\\).\n\nProposal Distribution:\n\nSelect the proposal distribution \\(q(\\psi' | \\psi^{(t)}) \\sim N(\\psi^{(t)}, \\tau^2)\\). \\(\\tau^2\\) is a tuning parameter that controls the step size.\n\nSampling Loop:\nFor t = 1,‚Ä¶,\\(T_1+T_2\\)\n\nGenerate a candidate \\(\\psi'^{(t)}\\) from \\(q(\\psi' | \\psi^{(t-1)})\\).\nCheck if \\(\\psi'\\) is within the bounds \\((-1, 1)\\). If not, reject \\(\\psi'\\) (set \\(\\alpha = 0\\)).\nCompute the acceptance ratio \\(\\alpha\\): \\[ \\alpha(\\psi^{(t-1)}, \\psi') = \\min\\left(1, \\frac{p(Y | A, \\Sigma, \\psi') p(\\psi') q(\\psi^{(t-1)} | \\psi')}{p(Y | A, \\Sigma, \\psi^{(t-1)}) p(\\psi^{(t-1)}) q(\\psi' | \\psi^{(t-1)})}\\right)\\]\n\nDecide to accept or reject:\n\nGenerate a random number \\(u\\) from \\(U[0,1]\\).\nIf \\(u \\leq \\alpha\\), accept \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi'\\).\nOtherwise, reject \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi^{(t-1)}\\).\n\nBurn in period\n\nDiscard the first \\(T_1\\) samples to allow the algorithm to converge to the true distribution\n\nObtain one sample of \\(\\psi\\)\n\nRandomly draw a \\(\\psi^*\\) from the \\(T_2\\) draws and set \\(\\psi^{(s)} = \\psi^*\\)\n\nWhen combined with Gibbs sampling for the estimation of \\(A\\) and \\(\\Sigma\\), the Metropolis-Hastings step can be embedded into the Gibbs sampler, a method known as Metropolis-within-Gibbs sampling. In this approach, one sample from the Metropolis-Hastings step is generated per Gibbs iteration. Detailed proofs and explanations on the convergence properties and efficiency of the Metropolis-within-Gibbs sampling method are provided in Chib and Greenberg (1995).\nWe monitor the acceptance rate and adjust \\(\\tau^2\\) as necessary to achieve an optimal rate of about 20-40%.\nWe note that since \\(\\Omega\\) is a band matrix, which means we do not need compute \\(\\Omega^{-1}\\). Instead, we obtain the Cholesky decomposition \\(C_{\\Omega}\\) of \\(\\Omega\\), which has a time complexity of \\(O(T)\\) instead of \\(O(T^3)\\). Terms involving \\(\\Omega^{-1}\\) such as \\(X^T\\Omega^{-1}X\\) can be obtained by: \\[X^T\\Omega^{-1}X = X^T(C_{\\Omega}^{-1})^{T}C_{\\Omega}^{-1}X=(C_{\\Omega}^{-1}X)^T(C_{\\Omega}^{-1}X) = \\tilde{X}^T\\tilde{X}\\]\nTo make forecasts of the \\(Y\\), we make the following observations: One-period ahead forecast \\[y_{t+1} = \\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1} +  e_{t+1}\\] \\[E(y_{t+1}) = E(\\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1} +  e_{t+1}) = \\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1}\\] The one-period ahead forecast error is: \\[e_{t+1|t} = y_{t+1} - y_{t+1|t} = e_{t+1}\\] The one period ahead forecast variance is: \\[\\mathbb{V}ar(e_{t+1}|t) = \\mathbb{E}[\\mathbb{E}_t(e_{t+1}e_{t+1}^T)] = \\Sigma\\] Two-period ahead forecast \\[y_{t+2} = \\mu_0 + A_1y_{t+1} + \\cdots + A_py_{t-p+2} +  e_{t+2}\\] \\[E(y_{t+2}) = E(\\mu_0 + A_1y_{t+1} + \\cdots + A_py_{t-p+2} +  e_{t+2}) = \\mu_0 + A_1y_{t+1|t} + \\cdots + A_py_{t-p+2}\\] The two-period ahead forecast error is: \\[e_{t+2|t} = y_{t+2}-y_{t+2|t} = e_{t+2} + A_1(y_{t+1} - y_{t+1|t}) = e_{t+2} + A_1e_{t+1}\\] \\[Var(e_{t+2|t}) = E[e_{t+2|t}e_{t+2|t}^T] = E((e_{t+2} + A_1e_{t+1})(e_{t+2} + A_1e_{t+1})^T) = E((e_{t+2}e_{t+2}^T + A_1e_{t+1}e_{t+2}^T+e_{t+2}e_{t+1}^TA_1^T+A_1e_{t+1}e_{t+1}^TA_1^T)\\] \\[ = \\Sigma + A_i\\Psi+ \\Psi A_1^T + A_1\\Sigma A_1^T\\] since in the MA(1) innovations setup, we have:\n\\[\\mathbb{V}ar(e_{t+2} e_{t+1}^T) = E \\left[\n\\begin{pmatrix}\ne_{t+2, 1} \\\\\ne_{t+2, 2} \\\\\n\\vdots \\\\\ne_{t+2, N}\n\\end{pmatrix}\n\\begin{pmatrix}\ne_{t+1, 1} & e_{t+1, 2} & \\cdots & e_{t+1, N}\n\\end{pmatrix}\n\\right] \\] \\[=\nE \\left[\n\\begin{pmatrix}\ne_{t+2, 1}e_{t+1, 1} & e_{t+2, 1}e_{t+1, 2} & \\cdots & e_{t+2, 1}e_{t+1, N}\\\\\ne_{t+2, 2}e_{t+1, 1} & e_{t+2, 2}e_{t+1, 2} & \\cdots & e_{t+2, 2}e_{t+1, N}\\\\\n\\vdots               &    \\vdots            & \\vdots &    \\vdots          \\\\\ne_{t+2, N}e_{t+2, 1} & e_{t+2, N}e_{t+2, 2} & \\vdots &    e_{t+2,N}e_{t+1,N}\n\\end{pmatrix}\n\\right]\\] \\[ = E \\left[\n\\begin{pmatrix}\n(\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t,N}+\\psi\\eta_{t,N})) \\\\\n(\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t,N}+\\psi\\eta_{t,N})) \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n(\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t,N}+\\psi\\eta_{t,N}))\\\n\\end{pmatrix}\n\\right]\n\\] \\[= \\begin{pmatrix}\n\\psi\\mathbb{E}[\\eta_{t+1,1}^2] & 0 & \\cdots & 0\\\\\n0 & \\psi\\mathbb{E}[\\eta_{t+1,2}^2] & \\cdots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\cdots & \\psi\\mathbb{E}[\\eta_{t+1,N}^2]\n\\end{pmatrix} = \\begin{pmatrix}\n\\psi & 0 & \\cdots & 0\\\\\n0 & \\psi & \\cdots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\cdots & \\psi\n\\end{pmatrix} = \\Psi\\]\nsince \\(\\eta_{t,i} \\overset{iid}{\\sim}\\mathcal{N}(0,1)\\)"
  },
  {
    "objectID": "index.html#model-3-large-bvar-model-with-common-stochastic-volatility",
    "href": "index.html#model-3-large-bvar-model-with-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.3 Model 3: Large BVAR model with common Stochastic Volatility",
    "text": "3.3 Model 3: Large BVAR model with common Stochastic Volatility\nThe variances of economic shocks are rarely constant over time. Volatility tends to cluster during periods of economic crisis and becomes more tranquil during stable times. Thus, treating the innovations as having constant variance is an unrealistic assumption in practice. To remedy the situation, we can incorporate stochastic volatility into the model, thereby allowing the model to adapt to changing volatility in the data. This typically lead to improvement in model performance, especially in the presence of financial market instability or shifts in economic policy. A BVAR model with common stochastic volatility is s specified as follows:\n\\[ Y = XA+E\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\n\\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\] where\n\\[e_t \\sim \\mathcal{N}(0, e^{h_t}\\Sigma)\\] and \\(h_t\\) follows an AR(1) process. \\[h_t = \\rho h_{t-1} + u_t^h\\] \\[u_t^h \\sim N(0, \\sigma^2_h)\\]"
  },
  {
    "objectID": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "href": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility",
    "text": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility\n\n3.4.1 Model Specification\n\\[ Y = XA+E\\]\nwhere\n\\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nthe same as before. But, instead of the column specific matrix as the identity matrix, we specify the column specific matrix as a diagonal matrix \\(\\Omega\\). Specifically, we have:\n\\[\\epsilon_t = u_t + \\psi_1 u_{t-1}\\]\n\\[u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\]\n\\[h_t = \\rho h_{t-1} + u_t^h \\quad \\text{ follows an Autoregressive process of lag 1 AR(1), and}\\]\n\\[u_t^h \\sim N(0,\\sigma_h^2)\\]\n\\[\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\]\nIn this specification, each element of \\(e_t\\) may have distinct variances, and the variances of all innovations can be scaled by a common factor. This approach is economically intuitive, as the volatility of macroeconomic variables often exhibit co-movement. It is important to emphasize that each component of \\(e_t\\) must adhere to the same univariate time series model.\n\n\n3.4.2 Prior Specification\nHere, we consider a priori independent distributions for \\((A, \\Sigma, \\Omega)\\), namely:\n\\[p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\] Given this structure, we can sample from the posterior distribution by sequentially sampling from:\n\n\\(P(A, \\Sigma | Y, X, \\Omega)\\)\n\\(P(\\Omega | Y, X, A, \\Sigma)\\)\n\nThe prior distribution of \\((A,\\Sigma)\\) follow the same normal inverse Wishart prior distribution as outlined in model one. But the variance matrix \\(V_A\\) for A is different:\n\\[V_A = diag(v_{A,ii})\\]\n\\[v_{A,ii} = \\begin{cases}\n\\kappa_1(\\frac{l^2}{\\hat{s}_r}) & \\text{for a coefficient associated to lag l of variable r} \\\\\n\\kappa_2& \\text{for an intercept}\n\\end{cases}\\]\nwhere \\(\\hat{s}_r\\) is the sample variance of an AR(4) model for the variable r.\n\\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\]\nand,\nFor the moving average coefficients, we adopt an uninformative truncated normal prior for the MA coefficient \\(\\psi\\):\n\\[\n\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\n\\]\nand we set \\(\\psi_0 = 0\\) and \\(V_\\psi = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1,1). Further more, we assume independent priors for \\(\\sigma^2_h\\) and \\(\\rho\\):\n\\[\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\]\n\\[\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\]\nWe set the hyperparameters \\(\\nu_{h_0} = 5\\), \\(s_{h_0} = 0.04\\), \\(\\rho_0 = 0.9\\) and \\(V_\\rho = 0.04\\) so that the prior mean of \\(\\sigma_h^2\\) is 0.01 and \\(\\rho\\) is centered at 0.9.\n\n\n3.4.3 Posterior Distribution\nThe posterior distribution specified above has the following form:\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.4.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\\(p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\\(\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\)\n\\(\\epsilon_t = u_t + \\psi_1 u_{t-1}\\)\n\\(\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\n\\(u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\)\n\\(h_t = \\rho h_{t-1} + u_t^h\\)\n\\(\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\n\\(u_t^h \\sim N(0,\\sigma_h^2)\\)\n\\(\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\)\n\nTo sample \\(S_1+S_2\\) draws of \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\)\n\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\sigma^{2,(s)}_h\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{IG}(v_{h_0}, s_{h_0})\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\rho^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\nFor each \\(\\sigma^{2,(s)}_h\\), sample \\(\\left\\{u_t^{h,(s)}\\right\\}_{t=1}^T\\) from \\(N(0,\\sigma_h^{2,(s)})\\)\nFor t = 1,‚Ä¶,T and s = 1,‚Ä¶, \\(S_1+S_2\\), compute \\(h_t^{(s)} = \\rho h_{t-1}^{(s)} + u_t^{h,(s)}\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{u_t^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(u_t \\sim \\mathcal{N}(0,e^{h_t^{(s)}} \\Sigma)\\) for t = 1,‚Ä¶,T\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\psi^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\nfor each t = 1,‚Ä¶,T and s = 1,‚Ä¶\\(S_1+S_2\\), compute \\(\\epsilon_t^{(s)} = u_t^{(s)} + \\psi^{(s)}u_{t-1}^{(s)}\\)\n\nAfter we have obtained \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\), we can use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution \\(p(A \\mid Y, X, \\Sigma, \\Omega)\\):\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Omega^{(s)}, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}, \\Omega^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nSampling from the joint predictive density is the same as before."
  },
  {
    "objectID": "index.html#standard-bayesian-var",
    "href": "index.html#standard-bayesian-var",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.1 Standard Bayesian VAR",
    "text": "4.1 Standard Bayesian VAR\n\n4.1.1 Model Building Code and Validation\nWe verify that our model can replicate the true parameter of the data generate process by: 1. Generate artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. That is,\n\\[\\mathbf{y_t} = \\begin{pmatrix} y_t,1 \\\\ y_t,2\\end{pmatrix} = \\mathbf{y_{t-1}} + \\mathbf{\\epsilon_t} = \\begin{pmatrix}y_{t-1,1}\\\\y_{t-1, 2}\\end{pmatrix} + \\begin{pmatrix}\\epsilon_{t,1}\\\\ \\epsilon_{t, 2}\\end{pmatrix}\\] and\n\\[\\mathbf{\\epsilon} \\sim iid \\mathcal{N}(\\mathbf{0}, \\mathbf{I_2} )\\]\n\n\n\n\n\n\n\n\n\nThen we estimate a model with a constant term and 1 lag using the simulated data, show that the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the posterior mean of the constant term is close to a vector of zeros.\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n       [,1]  [,2]\n[1,]  0.068 0.165\n[2,]  1.000 0.001\n[3,] -0.003 0.995\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n      [,1]  [,2]\n[1,] 0.074 0.071\n[2,] 0.003 0.002\n[3,] 0.002 0.002\n\n\n[1] \"Posterior mean of covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 1.051 0.067\n[2,] 0.067 0.949\n\n\n[1] \"Posterior standard deviation of covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 0.048 0.032\n[2,] 0.032 0.042\n\n\n\n\n4.1.2 Empirical Result\n1. Fitted Model Parameters \n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] -0.899  0.116 -1.487 -0.685 -0.092  0.305 -0.003 -0.070  5.525  0.136\n [2,]  0.421  0.049 -0.191 -0.034 -0.004  0.071  0.000 -0.002  0.275 -0.006\n [3,]  0.098  0.315  0.157 -0.061 -0.001  0.066  0.000  0.004  0.251 -0.007\n [4,]  0.067 -0.126  0.639  0.032 -0.007  0.016  0.002  0.004  0.510  0.016\n [5,] -0.010  0.055 -0.145  0.518  0.002 -0.046 -0.004 -0.009  0.012 -0.002\n [6,] -0.006 -0.002 -0.004 -0.007  0.998  0.005  0.000 -0.001  0.150  0.003\n [7,]  0.116 -0.112  0.029  0.025  0.007  0.881 -0.003 -0.009 -0.307 -0.005\n [8,] -0.016  0.033  0.022  0.022 -0.002  0.025  1.001  0.004  0.205  0.000\n [9,]  0.002  0.081 -0.004  0.000  0.003  0.020  0.001  1.001 -0.230 -0.009\n[10,]  0.025 -0.003 -0.011 -0.003 -0.001 -0.008  0.000 -0.004  1.180  0.002\n[11,] -0.010  0.043  0.023  0.018  0.000 -0.022  0.001  0.006  0.182  0.982\n[12,] -0.013 -0.066  0.031  0.020 -0.004  0.005 -0.001  0.003  0.362  0.010\n[13,]  0.049 -0.026 -0.041 -0.047 -0.001 -0.032 -0.001 -0.009  0.131  0.003\n[14,]  0.021 -0.103  0.001  0.052  0.004 -0.055 -0.003 -0.007 -0.296  0.000\n[15,] -0.019  0.083 -0.074  0.002 -0.003 -0.003  0.000 -0.004  0.228  0.005\n[16,]  0.003  0.026  0.022  0.015 -0.003  0.010  0.000  0.002  0.347  0.005\n[17,] -0.001  0.005 -0.003 -0.001  0.000  0.000  0.000  0.000  0.028  0.001\n[18,] -0.050  0.020  0.019 -0.003 -0.004  0.028  0.001  0.006  0.302  0.000\n[19,] -0.006  0.008  0.007  0.005 -0.001  0.007  0.000  0.001  0.054  0.000\n[20,] -0.003  0.011  0.003  0.004  0.000  0.009  0.000  0.001  0.002 -0.001\n[21,] -0.022 -0.011  0.010  0.002  0.003  0.014  0.000  0.004 -0.448 -0.002\n[22,] -0.001  0.005  0.010  0.001  0.000 -0.006  0.000  0.001  0.042 -0.008\n[23,] -0.006 -0.009  0.006  0.003  0.000  0.001  0.000  0.002  0.052  0.002\n[24,] -0.021  0.007  0.017  0.013 -0.001  0.025  0.000  0.001  0.187  0.001\n[25,] -0.005  0.027 -0.076 -0.029  0.000 -0.023  0.000 -0.001 -0.155  0.000\n[26,] -0.009  0.017  0.046  0.014  0.000  0.004  0.000 -0.001  0.087 -0.002\n[27,] -0.010 -0.020  0.010  0.005 -0.001  0.001  0.000 -0.001  0.049  0.002\n[28,] -0.001 -0.001 -0.001  0.000  0.000  0.001  0.000  0.000  0.009  0.000\n[29,]  0.009  0.047  0.009 -0.002  0.000  0.005  0.000  0.001 -0.060  0.001\n[30,] -0.002  0.005  0.002  0.002  0.000  0.003  0.000  0.000  0.016  0.000\n[31,]  0.001  0.006  0.002  0.001  0.000  0.002  0.000  0.000 -0.013  0.000\n[32,]  0.006  0.016  0.012  0.001 -0.003 -0.014  0.000 -0.001  0.398  0.001\n[33,]  0.002  0.004  0.006  0.003  0.000 -0.002  0.000  0.000  0.019 -0.003\n[34,] -0.002  0.000  0.001  0.001  0.000  0.000  0.000  0.001  0.016  0.000\n[35,]  0.021 -0.037 -0.002 -0.005 -0.001 -0.013  0.000 -0.002  0.070  0.000\n[36,] -0.012  0.132 -0.004  0.002  0.000  0.007  0.000  0.000  0.000 -0.001\n[37,]  0.013 -0.039  0.020  0.007 -0.001  0.001  0.000 -0.001  0.090  0.000\n[38,]  0.008  0.000 -0.004  0.006  0.000  0.004  0.000  0.001 -0.026  0.000\n[39,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.009  0.000\n[40,] -0.011 -0.007 -0.001  0.006 -0.001  0.019  0.000  0.001  0.051  0.001\n[41,] -0.001  0.002  0.001  0.002  0.000  0.002  0.000  0.000  0.012  0.000\n[42,] -0.001  0.001  0.000  0.001  0.000  0.003  0.000  0.000  0.007  0.000\n[43,]  0.001 -0.021 -0.006  0.000  0.001  0.013  0.000  0.002 -0.147  0.001\n[44,]  0.000  0.002  0.002  0.002  0.000 -0.001  0.000  0.000  0.005 -0.002\n[45,] -0.001  0.002  0.001  0.001  0.000  0.000  0.000  0.000  0.003  0.000\n[46,] -0.014  0.009 -0.004 -0.001  0.000  0.006  0.000  0.001  0.039  0.000\n[47,]  0.004 -0.029  0.015 -0.005  0.000  0.003  0.000  0.000  0.069  0.001\n[48,]  0.002 -0.016  0.002  0.008 -0.001  0.001  0.000  0.001  0.064  0.001\n[49,] -0.003  0.003 -0.002  0.004  0.000 -0.002  0.000  0.001  0.027  0.000\n[50,]  0.000 -0.001  0.000  0.000  0.000  0.000  0.000  0.000  0.006  0.000\n[51,]  0.006 -0.010  0.000  0.001  0.000 -0.008  0.000 -0.001 -0.018  0.000\n[52,] -0.001  0.001  0.001  0.001  0.000  0.001  0.000  0.000  0.005  0.000\n[53,]  0.000  0.000  0.000  0.000  0.000  0.001  0.000  0.000  0.001  0.000\n[54,] -0.004  0.007 -0.002  0.001  0.001 -0.006  0.000 -0.001 -0.097 -0.001\n[55,]  0.000  0.001  0.002  0.001  0.000 -0.001  0.000  0.000 -0.003 -0.002\n[56,]  0.000  0.001  0.001  0.001  0.000  0.000  0.000  0.000  0.002  0.000\n       [,11]\n [1,]  0.154\n [2,]  0.016\n [3,] -0.003\n [4,]  0.004\n [5,]  0.009\n [6,]  0.002\n [7,] -0.004\n [8,] -0.002\n [9,] -0.001\n[10,]  0.003\n[11,] -0.009\n[12,]  0.996\n[13,]  0.007\n[14,]  0.001\n[15,]  0.007\n[16,]  0.000\n[17,]  0.001\n[18,] -0.002\n[19,] -0.001\n[20,]  0.001\n[21,] -0.003\n[22,] -0.002\n[23,] -0.002\n[24,]  0.001\n[25,]  0.002\n[26,]  0.002\n[27,]  0.001\n[28,]  0.000\n[29,] -0.001\n[30,]  0.000\n[31,]  0.000\n[32,]  0.000\n[33,] -0.001\n[34,] -0.001\n[35,]  0.001\n[36,] -0.001\n[37,]  0.000\n[38,] -0.002\n[39,]  0.000\n[40,]  0.000\n[41,]  0.000\n[42,]  0.000\n[43,]  0.000\n[44,]  0.000\n[45,] -0.001\n[46,]  0.000\n[47,]  0.000\n[48,] -0.001\n[49,] -0.001\n[50,]  0.000\n[51,]  0.000\n[52,]  0.000\n[53,]  0.000\n[54,]  0.001\n[55,]  0.000\n[56,]  0.000\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]   [,9] [,10] [,11]\n [1,] 2.914 4.910 3.068 3.203 0.507 1.487 0.467 0.541 22.079 0.668 0.548\n [2,] 0.086 0.145 0.091 0.093 0.015 0.044 0.014 0.016  0.651 0.020 0.016\n [3,] 0.052 0.087 0.054 0.057 0.009 0.026 0.008 0.010  0.393 0.012 0.010\n [4,] 0.077 0.130 0.082 0.084 0.014 0.039 0.012 0.014  0.585 0.018 0.014\n [5,] 0.079 0.133 0.083 0.085 0.014 0.040 0.013 0.015  0.601 0.018 0.015\n [6,] 0.115 0.193 0.122 0.126 0.020 0.059 0.018 0.021  0.873 0.026 0.022\n [7,] 0.089 0.149 0.093 0.096 0.015 0.045 0.014 0.016  0.673 0.020 0.017\n [8,] 0.114 0.193 0.121 0.125 0.020 0.058 0.018 0.021  0.864 0.026 0.021\n [9,] 0.111 0.186 0.116 0.121 0.019 0.056 0.018 0.020  0.842 0.025 0.021\n[10,] 0.012 0.020 0.013 0.013 0.002 0.006 0.002 0.002  0.091 0.003 0.002\n[11,] 0.099 0.167 0.103 0.108 0.017 0.050 0.016 0.018  0.751 0.023 0.018\n[12,] 0.115 0.192 0.120 0.124 0.020 0.058 0.018 0.021  0.871 0.026 0.021\n[13,] 0.053 0.090 0.056 0.058 0.009 0.027 0.008 0.010  0.407 0.012 0.010\n[14,] 0.040 0.068 0.042 0.044 0.007 0.020 0.006 0.007  0.306 0.009 0.007\n[15,] 0.051 0.086 0.054 0.055 0.009 0.026 0.008 0.009  0.387 0.012 0.009\n[16,] 0.051 0.086 0.054 0.056 0.009 0.026 0.008 0.009  0.392 0.012 0.010\n[17,] 0.059 0.100 0.062 0.064 0.010 0.030 0.009 0.011  0.448 0.014 0.011\n[18,] 0.054 0.092 0.057 0.059 0.009 0.027 0.009 0.010  0.413 0.012 0.010\n[19,] 0.059 0.099 0.062 0.064 0.010 0.030 0.009 0.011  0.449 0.014 0.011\n[20,] 0.059 0.099 0.061 0.064 0.010 0.030 0.009 0.011  0.444 0.013 0.011\n[21,] 0.018 0.030 0.018 0.019 0.003 0.009 0.003 0.003  0.133 0.004 0.003\n[22,] 0.057 0.096 0.060 0.062 0.010 0.029 0.009 0.010  0.435 0.013 0.011\n[23,] 0.059 0.099 0.062 0.064 0.010 0.030 0.009 0.011  0.448 0.014 0.011\n[24,] 0.038 0.064 0.040 0.041 0.007 0.019 0.006 0.007  0.287 0.009 0.007\n[25,] 0.031 0.053 0.033 0.034 0.006 0.016 0.005 0.006  0.238 0.007 0.006\n[26,] 0.037 0.062 0.039 0.040 0.006 0.019 0.006 0.007  0.278 0.008 0.007\n[27,] 0.037 0.062 0.039 0.040 0.006 0.019 0.006 0.007  0.280 0.008 0.007\n[28,] 0.040 0.067 0.042 0.043 0.007 0.020 0.006 0.007  0.299 0.009 0.007\n[29,] 0.038 0.064 0.040 0.042 0.007 0.019 0.006 0.007  0.289 0.009 0.007\n[30,] 0.040 0.067 0.042 0.043 0.007 0.020 0.006 0.007  0.301 0.009 0.007\n[31,] 0.040 0.067 0.041 0.043 0.007 0.020 0.006 0.007  0.299 0.009 0.007\n[32,] 0.016 0.028 0.017 0.018 0.003 0.008 0.003 0.003  0.124 0.004 0.003\n[33,] 0.039 0.065 0.041 0.042 0.007 0.020 0.006 0.007  0.295 0.009 0.007\n[34,] 0.039 0.066 0.041 0.043 0.007 0.020 0.006 0.007  0.299 0.009 0.007\n[35,] 0.029 0.049 0.030 0.031 0.005 0.015 0.005 0.005  0.218 0.007 0.005\n[36,] 0.026 0.043 0.027 0.028 0.004 0.013 0.004 0.005  0.194 0.006 0.005\n[37,] 0.028 0.048 0.030 0.031 0.005 0.014 0.004 0.005  0.214 0.006 0.005\n[38,] 0.029 0.049 0.030 0.031 0.005 0.014 0.005 0.005  0.217 0.007 0.005\n[39,] 0.030 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.226 0.007 0.006\n[40,] 0.029 0.049 0.031 0.032 0.005 0.015 0.005 0.005  0.220 0.007 0.005\n[41,] 0.030 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.227 0.007 0.006\n[42,] 0.030 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.226 0.007 0.006\n[43,] 0.015 0.026 0.016 0.017 0.003 0.008 0.002 0.003  0.116 0.004 0.003\n[44,] 0.029 0.050 0.031 0.032 0.005 0.015 0.005 0.005  0.223 0.007 0.005\n[45,] 0.030 0.051 0.031 0.032 0.005 0.015 0.005 0.005  0.226 0.007 0.006\n[46,] 0.023 0.039 0.025 0.025 0.004 0.012 0.004 0.004  0.178 0.005 0.004\n[47,] 0.022 0.037 0.023 0.024 0.004 0.011 0.003 0.004  0.166 0.005 0.004\n[48,] 0.023 0.039 0.024 0.025 0.004 0.012 0.004 0.004  0.176 0.005 0.004\n[49,] 0.023 0.039 0.024 0.025 0.004 0.012 0.004 0.004  0.176 0.005 0.004\n[50,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004\n[51,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.178 0.005 0.004\n[52,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004\n[53,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004\n[54,] 0.011 0.018 0.011 0.012 0.002 0.005 0.002 0.002  0.081 0.002 0.002\n[55,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.179 0.005 0.004\n[56,] 0.024 0.040 0.025 0.026 0.004 0.012 0.004 0.004  0.181 0.005 0.004\n\n\n[1] \"Posterior mean of covariance matrix:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\n [1,]  0.355 -0.051  0.086  0.038  0.001 -0.058 0.000  0.001 -0.039  0.005\n [2,] -0.051  1.011  0.031  0.093  0.008  0.021 0.004  0.012 -0.647 -0.007\n [3,]  0.086  0.031  0.393  0.169 -0.001 -0.002 0.003  0.012  0.433 -0.010\n [4,]  0.038  0.093  0.169  0.421  0.000  0.016 0.003  0.012  0.178 -0.005\n [5,]  0.001  0.008 -0.001  0.000  0.011 -0.003 0.000  0.000 -0.189 -0.002\n [6,] -0.058  0.021 -0.002  0.016 -0.003  0.092 0.002  0.007  0.208  0.000\n [7,]  0.000  0.004  0.003  0.003  0.000  0.002 0.009  0.000  0.012  0.000\n [8,]  0.001  0.012  0.012  0.012  0.000  0.007 0.000  0.012  0.004 -0.001\n [9,] -0.039 -0.647  0.433  0.178 -0.189  0.208 0.012  0.004 20.485  0.134\n[10,]  0.005 -0.007 -0.010 -0.005 -0.002  0.000 0.000 -0.001  0.134  0.019\n[11,] -0.008 -0.012 -0.014 -0.009  0.000  0.000 0.000 -0.003  0.045  0.001\n       [,11]\n [1,] -0.008\n [2,] -0.012\n [3,] -0.014\n [4,] -0.009\n [5,]  0.000\n [6,]  0.000\n [7,]  0.000\n [8,] -0.003\n [9,]  0.045\n[10,]  0.001\n[11,]  0.012\n\n\n[1] \"Posterior standard deviation of covariance matrix:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.048 0.056 0.036 0.037 0.006 0.018 0.005 0.006 0.255 0.008 0.006\n [2,] 0.056 0.134 0.060 0.062 0.010 0.029 0.009 0.010 0.433 0.013 0.011\n [3,] 0.036 0.060 0.052 0.042 0.006 0.018 0.006 0.007 0.270 0.008 0.007\n [4,] 0.037 0.062 0.042 0.056 0.006 0.018 0.006 0.007 0.277 0.008 0.007\n [5,] 0.006 0.010 0.006 0.006 0.001 0.003 0.001 0.001 0.048 0.001 0.001\n [6,] 0.018 0.029 0.018 0.018 0.003 0.012 0.003 0.003 0.130 0.004 0.003\n [7,] 0.005 0.009 0.006 0.006 0.001 0.003 0.001 0.001 0.040 0.001 0.001\n [8,] 0.006 0.010 0.007 0.007 0.001 0.003 0.001 0.002 0.047 0.001 0.001\n [9,] 0.255 0.433 0.270 0.277 0.048 0.130 0.040 0.047 2.746 0.059 0.048\n[10,] 0.008 0.013 0.008 0.008 0.001 0.004 0.001 0.001 0.059 0.002 0.001\n[11,] 0.006 0.011 0.007 0.007 0.001 0.003 0.001 0.001 0.048 0.001 0.002\n\n\n2. Prediction Plots\n\n\n[1] \"Running prediction for 50000 iterations took: 131.527000 seconds\"\n\n\n\n\n\n\n\n\n\n\n\n3. Test for Granger Causality We first perform Granger Causality test for each country, we test whether the macroeconomic factor of one country has some predictive power on the macroeconomic factors of another. Specifically, we test the following hypothesis:\n\n$H_0: $ The consumer price index, interest rate, foreign direct investment, balance of payments, gross domestic product, exchange rate against the dollar of country A has no impact on these variables in the country B$\n\nThe Baye‚Äôs factor in this case is defined as:\n\\[B_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij} = 0|Y, X,\\Sigma)d \\Sigma}{\\int p_1(A_{ij}=0, \\Sigma)d\\Sigma}\\] \\[p_1(A_{ij} = 0|Y, X,\\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A_{ij}}, \\Sigma_{ij}, \\bar{V}_{ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_{0, ij}, V_{A, ij}, S_{0, ij}, \\nu_{0, ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) = p(A_{ij}|\\Sigma)\\times p(\\Sigma)\\] \\[A_{ij}|\\Sigma \\sim \\mathcal{MN}_{K \\times N} (A_{0,ij}, \\Sigma, V_{A, ij}) \\] \\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\] and we test the bi-lateral relationship between each of these countries.\n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 671.543000 seconds\"\n\n\n             CN           US           JP          AU\nCN -318685.2027     1070.657     1051.494    1112.069\nUS    1079.5498 -1618774.333     1348.506    1358.446\nJP     932.3779     1159.568 -1602272.595    1284.957\nAU    1150.6094     1301.027     1326.103 -945759.463\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\nTesting our null hypothesis using the Savage-Dickey ratio indicates that with p=5 lags, no country Granger-causes the economic indicators of another country. However, the lags of each country do Granger-cause their own economic indicators. We will test p = 4 lags next.\n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 352.230000 seconds\"\n\n\n             CN           US           JP          AU\nCN -272960.5910     1042.926     1056.942    1099.940\nUS    1188.0282 -1638597.076     1458.310    1478.405\nJP     918.1063     1287.356 -1616055.722    1359.913\nAU    1316.9405     1333.768     1459.371 -966899.803\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\nTesting our null hypothesis using the Savage-Dickey ratio indicates that with p=4 lags, no country Granger-causes the economic indicators of another country. However, the lags of each country do Granger-cause their own economic indicators. We will test the model for pre-covid and post_covid period next.\n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 671.645000 seconds\"\n\n\n             CN           US           JP          AU\nCN -314680.5342     1057.100     1049.716    1110.877\nUS    1052.6074 -1619446.107     1365.653    1418.698\nJP     950.7971     1143.669 -1609776.548    1277.014\nAU    1197.0614     1295.412     1365.178 -944295.423\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\nTesting our null hypothesis using the Savage-Dickey ratio indicates that with p=4 lags, no country Granger-causes the economic indicators of another country in the pre-Covid period from 1994 Q1 to 2019 Q4. Next, we test the hypothesis on data from Q1 2004, two years after China joined the World Trade Organization.\n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 672.771000 seconds\"\n\n\n             CN           US           JP          AU\nCN -319453.0737     1113.804     1047.330    1094.916\nUS    1095.5090 -1613266.393     1357.081    1372.285\nJP     945.0979     1137.518 -1602505.247    1253.436\nAU    1180.1254     1271.541     1346.508 -961943.542"
  },
  {
    "objectID": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.2 Large BVAR model with MA(1) Gaussian Innovations",
    "text": "4.2 Large BVAR model with MA(1) Gaussian Innovations\n\n4.2.1 Model Building Code and Validation\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.1785\n\n\n[1] \"Fit MA BVAR on 1000 bi-variate random walk for 2000 iterations took: 3887.012000 seconds\"\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n       [,1]  [,2]\n[1,]  0.068 0.175\n[2,]  0.999 0.002\n[3,] -0.003 0.995\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n      [,1]  [,2]\n[1,] 0.073 0.074\n[2,] 0.003 0.003\n[3,] 0.002 0.002\n\n\n[1] \"Posterior mean of row specific covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 1.150 0.063\n[2,] 0.063 1.048\n\n\n[1] \"Posterior standard deviation of row specific covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 0.053 0.035\n[2,] 0.035 0.045\n\n\n[1] \"Posterior mean of column specific covariance matrix (first 10 rows and columns):\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,]  1.001 -0.028  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [2,] -0.028  1.001 -0.028  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [3,]  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000  0.000  0.000  0.000\n [4,]  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000  0.000  0.000\n [5,]  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000  0.000\n [6,]  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000  0.000\n [7,]  0.000  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000  0.000\n [8,]  0.000  0.000  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028  0.000\n [9,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.028  1.001 -0.028\n[10,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.028  1.001\n\n\n[1] \"Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.001 0.022 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [2,] 0.022 0.001 0.022 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [3,] 0.000 0.022 0.001 0.022 0.000 0.000 0.000 0.000 0.000 0.000\n [4,] 0.000 0.000 0.022 0.001 0.022 0.000 0.000 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.022 0.001 0.022 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.000 0.022 0.001 0.022 0.000 0.000 0.000\n [7,] 0.000 0.000 0.000 0.000 0.000 0.022 0.001 0.022 0.000 0.000\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.022 0.001 0.022 0.000\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.022 0.001 0.022\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.022 0.001\n\n\n\n\n4.2.1 Empirical Results\n1. Fitted Model Parameters\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2496\n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 3887.012000 seconds\"\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\n [1,]  0.104 -0.248  0.043 -0.127 -0.059  0.075 0.003  0.023  5.413 -0.204\n [2,]  0.992  0.000 -0.008 -0.002  0.000  0.000 0.000  0.000  0.020  0.000\n [3,]  0.008  0.964  0.008 -0.005  0.000  0.003 0.000  0.000  0.020 -0.001\n [4,]  0.002 -0.006  0.988 -0.001  0.000 -0.001 0.000  0.000  0.039  0.000\n [5,]  0.001 -0.002 -0.006  0.990  0.000 -0.001 0.000  0.000  0.034  0.000\n [6,]  0.000  0.001  0.000  0.000  1.000  0.000 0.000  0.000  0.001  0.000\n [7,]  0.002  0.001  0.000  0.002  0.000  0.998 0.000  0.000 -0.004  0.002\n [8,]  0.000  0.001  0.000  0.001  0.000  0.000 1.000  0.000  0.003  0.000\n [9,]  0.000  0.002  0.000  0.001  0.000  0.000 0.000  1.000 -0.004  0.000\n[10,]  0.003 -0.001 -0.006 -0.002 -0.001 -0.001 0.000 -0.001  1.146  0.002\n[11,]  0.000 -0.003  0.000 -0.001  0.000  0.000 0.000  0.001  0.005  0.998\n[12,]  0.000  0.000  0.001  0.000  0.000  0.000 0.000  0.000  0.004  0.000\n[13,]  0.000  0.000 -0.001 -0.001  0.000  0.000 0.000  0.000  0.007  0.000\n[14,]  0.001 -0.007 -0.002 -0.001  0.000 -0.001 0.000  0.000  0.000  0.000\n[15,] -0.001  0.002 -0.003 -0.001  0.000  0.000 0.000  0.000  0.009  0.000\n[16,]  0.000  0.000 -0.001 -0.001  0.000  0.000 0.000  0.000  0.009  0.000\n[17,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[18,] -0.001  0.002  0.000  0.001  0.000  0.000 0.000  0.000  0.002  0.000\n[19,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[20,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[21,] -0.003  0.002  0.003  0.001  0.001  0.001 0.000  0.001 -0.092  0.000\n[22,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001 -0.001\n[23,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[24,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.004  0.000\n[25,] -0.001  0.000 -0.002 -0.001  0.000 -0.001 0.000  0.000  0.000  0.000\n[26,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.005  0.000\n[27,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.003  0.000\n[28,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[29,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000\n[30,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[31,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[32,]  0.000  0.000  0.003  0.001  0.000  0.000 0.000  0.000 -0.037  0.000\n[33,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[34,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[35,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[36,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.003  0.000\n[37,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.003  0.000\n[38,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[39,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[40,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[41,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[42,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[43,]  0.000 -0.001  0.000  0.001  0.000  0.000 0.000  0.000 -0.034  0.000\n[44,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[45,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[46,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[47,]  0.000 -0.002  0.001  0.000  0.000  0.000 0.000  0.000  0.004  0.000\n[48,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[49,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[50,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[51,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[52,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[53,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[54,]  0.000  0.001  0.000  0.001  0.000  0.000 0.000  0.000 -0.030  0.000\n[55,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[56,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n       [,11]\n [1,] -0.012\n [2,]  0.001\n [3,]  0.000\n [4,]  0.000\n [5,]  0.000\n [6,]  0.000\n [7,]  0.000\n [8,]  0.000\n [9,]  0.000\n[10,]  0.001\n[11,] -0.001\n[12,]  1.000\n[13,]  0.000\n[14,]  0.000\n[15,]  0.000\n[16,]  0.000\n[17,]  0.000\n[18,]  0.000\n[19,]  0.000\n[20,]  0.000\n[21,] -0.001\n[22,]  0.000\n[23,]  0.000\n[24,]  0.000\n[25,]  0.000\n[26,]  0.000\n[27,]  0.000\n[28,]  0.000\n[29,]  0.000\n[30,]  0.000\n[31,]  0.000\n[32,]  0.000\n[33,]  0.000\n[34,]  0.000\n[35,]  0.000\n[36,]  0.000\n[37,]  0.000\n[38,]  0.000\n[39,]  0.000\n[40,]  0.000\n[41,]  0.000\n[42,]  0.000\n[43,]  0.000\n[44,]  0.000\n[45,]  0.000\n[46,]  0.000\n[47,]  0.000\n[48,]  0.000\n[49,]  0.000\n[50,]  0.000\n[51,]  0.000\n[52,]  0.000\n[53,]  0.000\n[54,]  0.000\n[55,]  0.000\n[56,]  0.000\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.563 1.211 0.651 0.609 0.082 0.266 0.080 0.083 4.126 0.120 0.097\n [2,] 0.014 0.029 0.016 0.015 0.002 0.006 0.002 0.002 0.099 0.003 0.002\n [3,] 0.013 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.096 0.003 0.002\n [4,] 0.014 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.098 0.003 0.002\n [5,] 0.014 0.029 0.016 0.015 0.002 0.006 0.002 0.002 0.099 0.003 0.002\n [6,] 0.014 0.028 0.015 0.015 0.002 0.006 0.002 0.002 0.101 0.003 0.002\n [7,] 0.013 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.095 0.003 0.002\n [8,] 0.014 0.029 0.015 0.015 0.002 0.006 0.002 0.002 0.100 0.003 0.002\n [9,] 0.014 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.098 0.003 0.002\n[10,] 0.006 0.012 0.007 0.006 0.001 0.003 0.001 0.001 0.047 0.001 0.001\n[11,] 0.013 0.028 0.015 0.014 0.002 0.006 0.002 0.002 0.098 0.003 0.002\n[12,] 0.014 0.029 0.015 0.015 0.002 0.007 0.002 0.002 0.100 0.003 0.002\n[13,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[14,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[15,] 0.007 0.015 0.008 0.007 0.001 0.003 0.001 0.001 0.051 0.001 0.001\n[16,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[17,] 0.007 0.015 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[18,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[19,] 0.007 0.015 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[20,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[21,] 0.006 0.012 0.007 0.006 0.001 0.003 0.001 0.001 0.044 0.001 0.001\n[22,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.051 0.001 0.001\n[23,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n[24,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[25,] 0.004 0.009 0.005 0.005 0.001 0.002 0.001 0.001 0.033 0.001 0.001\n[26,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[27,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[28,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[29,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.033 0.001 0.001\n[30,] 0.005 0.009 0.005 0.005 0.001 0.002 0.001 0.001 0.033 0.001 0.001\n[31,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[32,] 0.004 0.009 0.005 0.004 0.001 0.002 0.001 0.001 0.030 0.001 0.001\n[33,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[34,] 0.005 0.010 0.005 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[35,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001\n[36,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001\n[37,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001\n[38,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001\n[39,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001\n[40,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001\n[41,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001\n[42,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001\n[43,] 0.003 0.007 0.004 0.003 0.000 0.001 0.000 0.000 0.023 0.001 0.001\n[44,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001\n[45,] 0.003 0.007 0.004 0.004 0.000 0.002 0.000 0.000 0.025 0.001 0.001\n[46,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[47,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[48,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[49,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[50,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[51,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[52,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[53,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[54,] 0.002 0.005 0.003 0.003 0.000 0.001 0.000 0.000 0.018 0.001 0.000\n[55,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n[56,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.020 0.001 0.000\n\n\n[1] \"Posterior mean of row specific covariance matrix:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\n [1,]  0.376 -0.159  0.154  0.049  0.001 -0.036 0.001  0.005  0.142  0.003\n [2,] -0.159  1.502 -0.073  0.099  0.006  0.003 0.005  0.017 -0.727 -0.001\n [3,]  0.154 -0.073  0.571  0.192  0.001  0.026 0.004  0.018  0.243 -0.017\n [4,]  0.049  0.099  0.192  0.421  0.001  0.014 0.004  0.012 -0.023 -0.006\n [5,]  0.001  0.006  0.001  0.001  0.011 -0.002 0.000  0.001 -0.263 -0.002\n [6,] -0.036  0.003  0.026  0.014 -0.002  0.086 0.002  0.007  0.133 -0.002\n [7,]  0.001  0.005  0.004  0.004  0.000  0.002 0.009  0.000  0.017  0.000\n [8,]  0.005  0.017  0.018  0.012  0.001  0.007 0.000  0.014 -0.054 -0.002\n [9,]  0.142 -0.727  0.243 -0.023 -0.263  0.133 0.017 -0.054 30.696  0.198\n[10,]  0.003 -0.001 -0.017 -0.006 -0.002 -0.002 0.000 -0.002  0.198  0.026\n[11,] -0.012 -0.014 -0.024 -0.011 -0.001 -0.002 0.000 -0.005  0.115  0.002\n       [,11]\n [1,] -0.012\n [2,] -0.014\n [3,] -0.024\n [4,] -0.011\n [5,] -0.001\n [6,] -0.002\n [7,]  0.000\n [8,] -0.005\n [9,]  0.115\n[10,]  0.002\n[11,]  0.015\n\n\n[1] \"Posterior standard deviation of row specific covariance matrix:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.052 0.073 0.046 0.038 0.006 0.017 0.005 0.007 0.320 0.009 0.007\n [2,] 0.073 0.206 0.089 0.076 0.012 0.034 0.011 0.013 0.649 0.019 0.015\n [3,] 0.046 0.089 0.078 0.049 0.008 0.021 0.007 0.009 0.401 0.011 0.009\n [4,] 0.038 0.076 0.049 0.057 0.007 0.018 0.006 0.007 0.339 0.010 0.008\n [5,] 0.006 0.012 0.008 0.007 0.002 0.003 0.001 0.001 0.061 0.002 0.001\n [6,] 0.017 0.034 0.021 0.018 0.003 0.012 0.003 0.003 0.156 0.004 0.003\n [7,] 0.005 0.011 0.007 0.006 0.001 0.003 0.001 0.001 0.049 0.001 0.001\n [8,] 0.007 0.013 0.009 0.007 0.001 0.003 0.001 0.002 0.061 0.002 0.001\n [9,] 0.320 0.649 0.401 0.339 0.061 0.156 0.049 0.061 4.236 0.086 0.066\n[10,] 0.009 0.019 0.011 0.010 0.002 0.004 0.001 0.002 0.086 0.004 0.002\n[11,] 0.007 0.015 0.009 0.008 0.001 0.003 0.001 0.001 0.066 0.002 0.002\n\n\n[1] \"Posterior mean of column specific covariance matrix (first 10 rows and columns):\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,]  1.192 -0.521  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [2,] -0.521  1.230 -0.436  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [3,]  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000  0.000  0.000  0.000\n [4,]  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000  0.000  0.000\n [5,]  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000  0.000\n [6,]  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000  0.000\n [7,]  0.000  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000  0.000\n [8,]  0.000  0.000  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436  0.000\n [9,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.436  1.192 -0.436\n[10,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.436  1.192\n\n\n[1] \"Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.038 0.070 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [2,] 0.070 0.053 0.045 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [3,] 0.000 0.045 0.038 0.045 0.000 0.000 0.000 0.000 0.000 0.000\n [4,] 0.000 0.000 0.045 0.038 0.045 0.000 0.000 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.045 0.038 0.045 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.000 0.045 0.038 0.045 0.000 0.000 0.000\n [7,] 0.000 0.000 0.000 0.000 0.000 0.045 0.038 0.045 0.000 0.000\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.045 0.038 0.045 0.000\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.045 0.038 0.045\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.045 0.038\n\n\n2. Prediction Plots \n\n\n[1] \"Predict 8 periods ahead MA BVAR on data for 10000 iterations took: 285.658000 seconds\"\n\n\n\n\n\n\n\n\n\n\n\n2. Forecasts Model Parameters\n3. Test for Granger Causality\n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 674.808000 seconds\"\n\n\n             CN           US           JP           AU\nCN -102804.7576     376.2762     327.0783     379.1168\nUS     375.6391 -528073.5117     352.3693     379.6552\nJP     378.5526     378.8469 -522652.2917     374.8028\nAU     376.2018     382.5976     323.0241 -302290.8092"
  },
  {
    "objectID": "index.html#large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "href": "index.html#large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.4 Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility",
    "text": "4.4 Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility\n\n4.4.1 Model Building Code and Validation\n\n\n4.4.2 Empirical Result\n```"
  }
]