[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "",
    "text": "Objective: Construct a Bayesian Vector Autoregression model to forecast major macroeconomic indicators for the United States, Australia, Japan, and China to facilitate an investigation into the prospective interdependencies between the economies of these nations.\nQuestion: This research project will examine how trade relationships, investment flows, monetary policy environments, and economic performances within the United States, Australia, Japan and China mutually influence each other, and assess the implications of these interactions for predicting future values of these economic indicators.\nMotivation: Since the onset of the COVID-19 pandemic, the global economic landscape has witnessed a series of unprecedented shifts in key macroeconomic indicators, spurred by governments‚Äô adoption of varied expansionary monetary policies. Initially, to buffer their economies, many nations implemented expansive monetary strategies, later swiftly transitioning to interest rate hikes in a bid to manage surging inflation rates‚Äîa scenario not seen in decades. The pandemic‚Äôs disruption to trade further exacerbated inflationary pressures for some economies, highlighting the intricate interdependencies among major economies with significant trade and financial ties. This period recorded stark contrast in inflation levels, with unprecedented highs in the US and Australia and notably low inflation in China and Japan. Amidst this turmoil, a divergence in economic paths also became apparent, the United States and Australia have witness robust economic rebounds, whereas China and Japan saw more tepid recoveries. This research aims to dissect the nuanced web of economic interdependencies between the United States, Australia, Japan, and China, analyzing how their trade relationships, investment flows, and monetary policy environments have mutually influenced their economic performances. Additionally, it seeks to understand the ramifications of these dynamics for the predictive accuracy of future economic indicators, offering insights into the evolving global economic order."
  },
  {
    "objectID": "index.html#data-plots",
    "href": "index.html#data-plots",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Data Plots",
    "text": "Data Plots\nOriginal Variables"
  },
  {
    "objectID": "index.html#stationarity-check",
    "href": "index.html#stationarity-check",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Stationarity Check",
    "text": "Stationarity Check\nStationary Tests\nThe Augmented Dickey-Fuller Test is used in this section to test the null hypothesis that a unit root is present in the time series and the time series is non-stationary.\n\n\n\nADF Test Results for CPI Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.258408\n4\n0.08079872\nJP_CPI\n\n\n-4.299916\n4\n0.01\nCN_CPI\n\n\n-3.55799\n4\n0.0394958\nUS_CPI\n\n\n-3.333834\n4\n0.06822768\nAU_CPI\n\n\n\n\n\nThe lag order chosen in the ADF test is 4, which is appropriate given our data is of quarterly frequency. The result of the ADF test on the CPI data shows that we do not have enough evidence to reject the null hypothesis that the CPI series is unit root non-stationary at 1% significance level, except for China.\n\n\n\nADF Test Results for Foreign Direct Investment (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-2.26286\n4\n0.4686993\nJP_FDI\n\n\n-2.7189\n4\n0.285367\nCN_FDI\n\n\n-3.130793\n4\n0.1197816\nUS_FDI\n\n\n-1.607673\n4\n0.7320914\nAU_FDI\n\n\n\n\n\nThe ADF results shows that we do not have enough evidence to reject the null hypothesis that the foreign direct investment data is not unit-root stationary for Australia, the United States and Japan at 1% significance level and we do have enough evidence to reject the null hypothesis for China at 1% significance level.\n\n\n\nADF Test Results for Exchange Rate (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-1.791111\n4\n0.6635047\nJP_XCH\n\n\n-1.565957\n4\n0.7575635\nCN_XCH\n\n\n-2.56175\n4\n0.3415666\nAU_XCH\n\n\n\n\n\nThe result of the ADF test on the exchange rate data shows that we do not have enough evidence to reject the null hypothesis that the exchange rate against the dollar series is unit root non-stationary at 1% significance level.\n\n\n\nADF Test Results for Balance of Payments (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-2.914308\n4\n0.2025054\nJP_BOP\n\n\n-2.50488\n4\n0.3695164\nCN_BOP\n\n\n-2.533578\n4\n0.3578101\nUS_BOP\n\n\n-2.058287\n4\n0.5516879\nAU_BOP\n\n\n\n\n\nThe result of the ADF test on the balance of payments data shows that we do not have enough evidence to reject the null hypothesis that the balance of payments series is unit root non-stationary at 1% significance level for all countries.\n\n\n\nADF Test Results for GDP (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.499383\n4\n0.04535076\nJP_GDP\n\n\n-2.472818\n4\n0.3802514\nCN_GDP\n\n\n0.7030938\n4\n0.99\nUS_GDP\n\n\n-2.030622\n4\n0.5638687\nAU_GDP\n\n\n\n\n\nThe result of the ADF test on the GDP data shows that we do not have enough evidence to reject the null hypothesis that the GDP series is unit root non-stationary at 5% significance level."
  },
  {
    "objectID": "index.html#model-1-standard-bvarp-model",
    "href": "index.html#model-1-standard-bvarp-model",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.1 Model 1: Standard BVAR(p) Model",
    "text": "3.1 Model 1: Standard BVAR(p) Model\n\n3.1.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, I_T)\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(I_T\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\\(x_{t}^T = \\left( \\begin{array}{cccc}1  & y_{t-1} & y_{t-2} & \\cdots & y_{t-p} \\end{array} \\right)\\)\n\nThe Bayesian Vector Autoregression model as formulated above provides a robust framework for investigating the relationships among selected economic indicators across different nations. By employing this model, this research aims to quantitatively measure the influence of one country‚Äôs economic indicators on another, such as how lagged changes in China‚Äôs inflation rate, may influence the GDP growth rate of the United States and vice versa. The BVAR model, with its estimation of coefficients across various lags, offers a deep understanding of both immediate and more delayed economic interactions, which is crucial to analyzing the cyclical nature of trade relationships, investment flows, monetary policy environments, and economic performances and the transmission of these metrics across borders.\nThe strength of this BVAR model lies in its ability to incorporate prior economic knowledge and beliefs into the estimation process. By setting prior distributions for the matrix of coefficients A and the covariance matrix \\(\\Sigma\\), the model can be tailors to reflect established economic theories regarding international economic linkages and the time it takes for policy changes in one country to affect another. By calibrating the prior variances, particularly for the autoregressive coefficients, we can integrate prior knowledge or hypotheses, such as the presence of unit roots or the diminishing influence of distant lags on current values, into the analysis. When interpreting the estimation output, attention will be given to the posterior means and variances of the coefficients, which represent the model‚Äôs ‚Äúlearnt‚Äù understanding of the underlying economic structure. The analysis will be supplemented by forecast error variance decompositions to better understand the proportion of the movements in economic indicators that can be accounted for by their own shocks versus shocks to other variables.\nThe economic context underscoring this analysis is the increased globalization over the past decade, marking an era where economies are more intertwined than ever through trade, capital flows, and policy decisions. This period has witnessed not only the strengthening of global economic ties but also recent calls from political leaders advocating for a reduction in globalization. These contrasting dynamics highlight the complexity of the current global economic landscape, where the push for deeper integration coexists with growing sentiments for retrenchment. This dual trend sets the stage for our investigation, providing a rich context to explore how economic variables across nations influence each other amidst fluctuating levels of global interconnectedness. In this environment, understanding the cross-country spillover effects is vital for policymakers and businesses alike, as decisions made in one country can have far-reaching implications. By addressing these aspects, this research will contribute to the discourse on economic policy formulation, risk assessment, and strategic planning.\n\n\n3.1.2 Prior Settings\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of coefficient matrices A and the row-specific variance matrix \\(\\Sigma\\), and a Minnesota prior on the coefficients A. Specifically, we have:\n\\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0) \\]\n\\[p(\\Sigma) \\propto |\\Sigma|^{-\\frac{\\nu_0+N+1}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\]\n\\[A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\]\n\\[p(A|\\Sigma) \\propto |\\Sigma|^{-\\frac{K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\n\\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\]\n\\[p(A,\\Sigma) \\propto |\\Sigma|^{-\\frac{K+\\nu_0+N+1}{2}} \\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\nwhere\n\n\\[V_A = diag(\\kappa_2 \\quad \\kappa_1(\\mathbf{p} \\otimes I_N^T))\\]\n\\[\\mathbf{p} = [1 \\quad 2 \\quad ... \\quad p]\\]\n\\[I_N = [1 \\quad 1 \\quad ... \\quad 1] \\in \\mathbb{R}^N\\]\n\\[\\kappa_1 \\text{ is the overall shrinkage level for autoregressive slopes}\\]\n\\[\\kappa_2 \\text{ is the overall shrinkage lvel for the constant term }\\]\n\nAdditionally, we adopt commonly used values for the hyperparameters as established in the literature.\n\\[A_0 = 0\\]\n\\[v_0 = N+3\\]\n\\[S_0 = I_N\\]\n\\[\\kappa_1 = 0.2^2 \\quad \\kappa_2 = 10^2\\]\nThe hyperparameters \\(\\kappa_1\\) and \\(\\kappa_2\\) are specified in a way such that the coefficient associated with a lag l variable is shrunk more heavily to - as lag length increases whereas the intercepts are not shrunk to 0.\n\n\n3.1.3 Posterior Distributions\nThe posterior distribution specified above has the form\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{TN}{2}}|\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y)  = \\frac{p(A,\\Sigma,Y)}{p(Y)}\\propto p(A, \\Sigma, Y) \\propto p(Y|A,\\Sigma)\\times p(A,\\Sigma) =  p(Y|A,\\Sigma) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\]\n\\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^TY-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\] Hence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^TX + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^TY + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^TY + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.1.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\nwe use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution:\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nThe draws from joint predictive density can then be obtained using the following algorithm:\n\nSample S draws \\(\\left\\{ A^{(s)}, \\Sigma^{(s)} \\right\\}_{s=1}^{S}\\) from \\(p(A,\\Sigma|Y, X)\\)\nSample S draws \\(\\left\\{ Y_{t+h}^{(s)} \\right\\}_{s=1}^S\\) from \\(Y_{t+h}^{(s)} \\sim \\mathcal{N}_{hN}(Y_{t+h|t}(A^{(s)}), \\mathbb{V}ar[Y_{t+h|t}|A^{(s)}, \\Sigma^{(s)}])\\)\n\nwhere \\[Y_{t+h} = \\begin{pmatrix}\n    Y_{t+1} \\\\\n    Y_{t+2} \\\\\n    \\vdots    \\\\\n    Y_{t+h}\n\\end{pmatrix}\n\\]\nTo derive the posterior predictive density for \\(Y_{t+h}\\), we first note that:\n\\[p(Y_{t+h}|Y_{t}) = \\int \\int p(Y_{t+h}|Y_{t},A, \\Sigma)\\times p(A,\\Sigma|Y,X)dAd\\Sigma\\] where\n\\[p(y_{t+h}|Y, X, A, \\Sigma, 1) \\sim \\mathcal{N}_{hN} (Y_{t+h|t}(A), Var(Y_{t+h}|A, \\Sigma))\\]\n\\[Y_{t+h|t}(A) =\\begin{pmatrix}\n    Y_{t+1|t} \\\\\n    Y_{t+2|t} \\\\\n    \\vdots    \\\\\n    Y_{t+h|t}\n\\end{pmatrix} = \\begin{pmatrix}  \n\\mu_0 + A_1 Y_t + \\cdots + A_pY_{t-p+1|t} \\\\\n\\mu_0 + A_1 Y_{t+1|t} + \\cdots + A_p Y_{t-p+2|t}\\\\\n\\vdots \\\\\n\\mu_0 + A_1 Y_{t+1|t} + \\cdots + A_p Y_{t-p+h|t}\n\\end{pmatrix}\\]\n\\[\\mathbb{V}ar(Y_{t+h|t}|A, \\Sigma) = \\begin{pmatrix}\n    \\Sigma & \\Sigma \\phi_1^T & \\cdots & \\Sigma \\phi_{h-1}^T \\\\\n    \\phi_1\\Sigma &\\Sigma + \\phi_1 \\Sigma \\phi_1^T  & \\cdots & \\Sigma \\phi_1^T + \\phi_1 \\Sigma \\phi_2^T+ \\cdots + \\phi_{h-2}\\Sigma \\phi_{h-1}^T + \\phi_{h-1} \\Sigma \\phi_{h}^T \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{h-1}\\Sigma\\ & \\phi_1\\Sigma  + \\phi_2 \\Sigma \\Phi_1^T + \\cdots + \\phi_{h} \\Sigma \\phi_{h-1}^T & \\cdots & \\Sigma + \\phi_1 \\Sigma \\phi_1^T + \\cdots+\\phi_{h-1}\\Sigma\\phi_{h-1}^T+\\phi_h\\Sigma\\phi_h^T\n\\end{pmatrix}\n\\] where \\(\\phi_i = J A^i J'\\) are the parameters of the VMA(\\(\\infty\\)) representation of VAR(\\(p\\)) and \\(A\\) is the parameter matrix of VAR(1) representation of VAR(\\(p\\)).\n\\[p(A|Y, X, \\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\] In this approach, we are incorporating into our prediction the uncertainties in the estimates of the parameters by integrating over A and \\(\\Sigma\\), the is solved using numerical integration.\n\n\n3.1.5 Test for Granger Causality\nGranger causality testing, introduced in Chan (1969), is a way to measure whether the past and current values of one time series contains useful information for predicting future values of another. The idea behind granger causality testing is that if a time series does contain predictive information about another, incorporating it into the model should reduce the error variance and improve the precision of the forecasts. The classical approach for testing causality is the Wald test, but it fails to quantify the degree of evidence in the data in favour of or against the causality hypothesis. The Bayesian approach to testing Granger causality based on Baye‚Äôs factor, on the other hand, has the ability to quantify the evidence for and against the hypothesis with a single number. The Baye‚Äôs factor is defined as:\n\\[B_H = \\frac{p[H_0|Y]}{p[H_1|Y]}\\] In the context of testing for VAR(p) models, we have the marginal likelihood under the assumption of no Granger causality is:\n\\[p_0(Y) = \\int p_1(Y| A_{ij} = 0, A_{-(ij)})dA_{-(ij)} = p_1(Y|A_{ij}=0)\\] using Baye‚Äôs rule, we have: \\[p_0(Y) = \\frac{p_1(A_{ij} = 0|Y) \\times p_1(Y)}{p_1(A_{ij} = 0)}\\] We divide the above equation by \\(p(y_t|H_1)\\) to get Baye‚Äôs factor:\n\\[B_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij=0}|Y, \\Sigma)d \\Sigma}{\\int p(A_{ij}=0, \\Sigma)d\\Sigma}\\]\nand the integration is performed using numerical integration methods. The second part is also known as the Savage-Dickey density ratio estimator, which is the ratio of the posterior over the prior density evaluated at \\(A_{ij} = 0\\). Indicative values for interpreting Bayes factors is provided below:\nTo test for Granger causality, we use the equation: \\[B_H = \\frac{p_1(A_{ij=0}|Y)}{p(A_{ij}=0)} \\] and the integral is evaluated using numerical integration.\nDescriptive labels for certain Bayes factors.\n\n\n\nLabel\n\\(B_H\\)\n\\(p(H_1 \\mid X)\\)\n\n\n\n\nData strongly support \\(H_1\\)\n10\n91%\n\n\nData weakly support \\(H_1\\)\n3\n75%\n\n\nData provide ambiguous information\n1\n50%\n\n\nData weakly support \\(H_0\\)\n1/3\n25%\n\n\nData strongly support \\(H_0\\)\n1/10\n9%"
  },
  {
    "objectID": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations",
    "text": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations\nIncorporating MA(1) Gaussian innovations in a large BVARs model lead to a significant enhancement over traditional BVAR models, especially when forecasting macroeconomic variables for several reasons. Firstly, the variances of economic shocks is rarely constant over time. For example, volatility tends to cluster during periods of economic crisis and is more tranquil during stable times. Incorporating stochastic volatility allows the model to adapt to changing volatility in the data, improving forecasting performance especially in the presence of financial market instability or economic policy shifts. In addition, by allowing for serial correlation in the innovations term, we will be able to capture the momentum or persistence in economic variables that is often observed in real-world data. Recognizing that shocks may have a lasting impact over several periods can enhance the model‚Äôs ability to predict future values by considering the path-dependent nature of the economy. Our BVAR model with common stochastic volatility is a natural extension to the standard BVAR model and formulated as below:\n\n3.2.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(I_T\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\nwith MA(1) innovations, we have, for i = 1,‚Ä¶.,N and t = 1,‚Ä¶,T, \\[e_{it} = \\eta_{it} + \\psi \\eta_{i,t-1}\\] where \\(|\\psi|&lt;1\\) and \\(\\eta_{it} \\sim \\mathcal{N}(0,1)\\)\nIn matrix notation, we have:\n\\[e_i = H_{\\psi} \\eta_i\\]\nwhere\n\\[\ne_i = \\begin{bmatrix}\ne_{i1} \\\\\ne_{i2} \\\\\n\\vdots \\\\\ne_{iT}\n\\end{bmatrix} \\qquad\n\\eta_i = \\begin{bmatrix}\n\\eta_{i1} \\\\\n\\eta_{i2} \\\\\n\\vdots \\\\\n\\eta_{iT}\n\\end{bmatrix} \\qquad\nH_{\\psi} = \\left(\n\\begin{array}{cccc}\n1 & 0  & \\cdots & 0 \\\\\n\\psi & 1  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\psi & 1\n\\end{array}\n\\right) \\qquad\nO_{\\psi} = diag(1+\\psi^2, 1, ..., 1)\n\\]\nHence, we have\n\\[e \\sim \\mathcal{N}(0, H_{\\psi}O_{\\psi}H_{\\psi}^T) \\qquad \\qquad \\Omega = H_{\\psi}O_{\\psi}H_{\\psi}^T\\] Note that the covariance matrix \\(\\Omega\\) depends on \\(\\psi\\) only.\n\n\n3.2.2 Prior Specification\nWe consider a prior independent distributions for \\((A, \\Sigma, \\Omega)\\), specifically, we have: \\[P(A, \\Sigma, \\Omega) = P(A, \\Sigma) \\times P(\\Omega)\\]\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of A and \\(\\Sigma\\) and before, \\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_0, V_A, S_0, \\nu_0)\\]\nWe apply a truncated normal prior on \\(\\psi\\), \\[\\psi \\sim \\mathcal{N}(\\psi_0, V_{\\psi})\\mathbb{1}_{\\{|\\psi|&lt;1\\}}\\]\nFor estimation purposes, we initialize with the following setting:\n\n\\(e_{i1} \\sim \\mathcal{N}(0, 1+\\psi^2)\\)\n\\(\\psi_0 = 0\\) and \\(V_{\\psi} = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1, 1)\n\n\n\n3.2.3 Posterior Distributions\n\nThe posterior distribution of Y specified above has the form:\n\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\n\\[= (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}(1+\\psi^2)^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(H_{\\psi}O_{\\psi}H_{\\psi}^T)^{-1}(Y-XA))\\]\n\nThe joint posterior distribution of A and \\(\\Sigma\\) can be derived as follows:\n\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\] 3. The posterior distribution for the parameter \\(\\psi\\) can be obtained as follows: \\[P(\\psi|Y, A, \\Sigma) = \\frac{P(\\psi, Y, A, \\Sigma)}{P(Y, A, \\Sigma)} \\propto P(\\psi, Y, A, \\Sigma) = P(Y|A, \\Sigma, \\psi) \\times P(A,\\Sigma, \\psi) \\]\n\\[= P(Y|A, \\Sigma, \\psi) \\times P(A, \\Sigma) \\times P(\\psi) \\propto P(Y|A, \\Sigma, \\psi) \\times P(\\psi)\\]\nwe can sample from the posterior distribution of \\(\\psi\\) using an independence-chain Metropolis-Hastings algorithm.\n\n\n3.2.4 Estimation Procedure\nWe obtain posterior estimates of \\(A, \\Sigma, \\psi\\) using a Gibbs sampler, specifically, we initialize \\(\\psi^{(0)}\\) and for s = 1,‚Ä¶,S1+S2, we sequentially sample:\n\n\\(\\Sigma^{(s)} | Y, X, \\psi^{(s-1)} \\sim \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\\(A^{(s)} | Y, X, \\Sigma^{(s)}, \\psi^{(s-1)} \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma^{(s)}, \\bar{V})\\)\n\\(\\psi^{(s)} | Y, X, A^{(s)}, \\Sigma^{(s)} \\propto P(Y|A^{(s)}, \\Sigma^{(s)}, \\psi^{s-1})\\times p(\\psi)\\)\n\nThe Metropolis-Hastings Algorithm for Sampling \\(\\psi\\) is given as follows:\nInitialization:\n\nChoose an initial value \\(\\psi^{(0)}\\) within the bounds \\((-1, 1)\\).\n\nProposal Distribution:\n\nSelect the proposal distribution \\(q(\\psi' | \\psi^{(t)}) \\sim N(\\psi^{(t)}, \\tau^2)\\). \\(\\tau^2\\) is a tuning parameter that controls the step size.\n\nSampling Loop:\nFor t = 1,‚Ä¶,\\(T_1+T_2\\)\n\nGenerate a candidate \\(\\psi'^{(t)}\\) from \\(q(\\psi' | \\psi^{(t-1)})\\).\nCheck if \\(\\psi'\\) is within the bounds \\((-1, 1)\\). If not, reject \\(\\psi'\\) (set \\(\\alpha = 0\\)).\nCompute the acceptance ratio \\(\\alpha\\): \\[ \\alpha(\\psi^{(t-1)}, \\psi') = \\min\\left(1, \\frac{p(Y | A, \\Sigma, \\psi') p(\\psi') q(\\psi^{(t-1)} | \\psi')}{p(Y | A, \\Sigma, \\psi^{(t-1)}) p(\\psi^{(t-1)}) q(\\psi' | \\psi^{(t-1)})}\\right)\\]\n\nDecide to accept or reject:\n\nGenerate a random number \\(u\\) from \\(U[0,1]\\).\nIf \\(u \\leq \\alpha\\), accept \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi'\\).\nOtherwise, reject \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi^{(t-1)}\\).\n\nBurn in period\n\nDiscard the first \\(T_1\\) samples to allow the algorithm to converge to the true distribution\n\nObtain one sample of \\(\\psi\\)\n\nRandomly draw a \\(\\psi^*\\) from the \\(T_2\\) draws and set \\(\\psi^{(s)} = \\psi^*\\)\n\nWe monitor the acceptance rate and adjust \\(\\tau^2\\) as necessary to achieve an optimal rate of about 20-40%.\nWe note that since \\(\\Omega\\) is a band matrix, we do not need compute \\(\\Omega^{-1}\\), instead, we obtain the Cholesky decomposition \\(C_{\\Omega}\\) of \\(\\Omega\\), which has a time complexity of \\(O(T)\\). Terms involving \\(\\Omega^{-1}\\) such as \\(X^T\\Omega^{-1}X\\) can be obtained by: \\[X^T\\Omega^{-1}X = X^T(C_{\\Omega}^{-1})^{T}C_{\\Omega}^{-1}X=(C_{\\Omega}^{-1}X)^T(C_{\\Omega}^{-1}X) = \\tilde{X}^T\\tilde{X}\\]"
  },
  {
    "objectID": "index.html#model-3-large-bvar-model-with-ommon-stochastic-volatility",
    "href": "index.html#model-3-large-bvar-model-with-ommon-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.3 Model 3: Large BVAR model with ommon Stochastic Volatility",
    "text": "3.3 Model 3: Large BVAR model with ommon Stochastic Volatility"
  },
  {
    "objectID": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "href": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility",
    "text": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility\n\n3.4.1 Model Specification\n\\[ Y = XA+E\\]\nwhere\n\\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\] where\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\)\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\nthe same as before. But, instead of the column specific matrix as the identity matrix, we specify the column specific matrix as a diagonal matrix \\(\\Omega\\). Specifically, we have:\n\\[\\epsilon_t = u_t + \\psi_1 u_{t-1}\\]\n\\[u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\]\n\\[h_t = \\rho h_{t-1} + u_t^h \\quad \\text{ follows an Autoregressive process of lag 1 AR(1), and}\\]\n\\[u_t^h \\sim N(0,\\sigma_h^2)\\]\n\\[\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\]\nIn this specification, each element of \\(e_t\\) may have distinct variances, and the variances of all innovations can be scaled by a common factor. This approach is economically intuitive, as the volatility of macroeconomic variables often exhibit co-movement. It is important to emphasize that each component of \\(e_t\\) must adhere to the same univariate time series model.\n\n\n3.4.2 Prior Specification\nHere, we consider a priori independent distributions for \\((A, \\Sigma, \\Omega)\\), namely:\n\\[p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\] Given this structure, we can sample from the posterior distribution by sequentially sampling from:\n\n\\(P(A, \\Sigma | Y, X, \\Omega)\\)\n\\(P(\\Omega | Y, X, A, \\Sigma)\\)\n\nThe prior distribution of \\((A,\\Sigma)\\) follow the same normal inverse Wishart prior distribution as outlined in model one. But the variance matrix \\(V_A\\) for A is different:\n\\[V_A = diag(v_{A,ii})\\]\n\\[v_{A,ii} = \\begin{cases}\n\\kappa_1(\\frac{l^2}{\\hat{s}_r}) & \\text{for a coefficient associated to lag l of variable r} \\\\\n\\kappa_2& \\text{for an intercept}\n\\end{cases}\\]\nwhere \\(\\hat{s}_r\\) is the sample variance of an AR(4) model for the variable r.\n\\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\]\nand,\nFor the moving average coefficients, we adopt an uninformative truncated normal prior for the MA coefficient \\(\\psi\\):\n\\[\n\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\n\\]\nand we set \\(\\psi_0 = 0\\) and \\(V_\\psi = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1,1). Further more, we assume independent priors for \\(\\sigma^2_h\\) and \\(\\rho\\):\n\\[\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\]\n\\[\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\]\nWe set the hyperparameters \\(\\nu_{h_0} = 5\\), \\(s_{h_0} = 0.04\\), \\(\\rho_0 = 0.9\\) and \\(V_\\rho = 0.04\\) so that the prior mean of \\(\\sigma_h^2\\) is 0.01 and \\(\\rho\\) is centered at 0.9.\n\n\n3.4.3 Posterior Distribution\nThe posterior distribution specified above has the following form:\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.4.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\\(p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\\(\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\)\n\\(\\epsilon_t = u_t + \\psi_1 u_{t-1}\\)\n\\(\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\n\\(u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\)\n\\(h_t = \\rho h_{t-1} + u_t^h\\)\n\\(\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\n\\(u_t^h \\sim N(0,\\sigma_h^2)\\)\n\\(\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\)\n\nTo sample \\(S_1+S_2\\) draws of \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\)\n\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\sigma^{2,(s)}_h\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{IG}(v_{h_0}, s_{h_0})\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\rho^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\nFor each \\(\\sigma^{2,(s)}_h\\), sample \\(\\left\\{u_t^{h,(s)}\\right\\}_{t=1}^T\\) from \\(N(0,\\sigma_h^{2,(s)})\\)\nFor t = 1,‚Ä¶,T and s = 1,‚Ä¶, \\(S_1+S_2\\), compute \\(h_t^{(s)} = \\rho h_{t-1}^{(s)} + u_t^{h,(s)}\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{u_t^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(u_t \\sim \\mathcal{N}(0,e^{h_t^{(s)}} \\Sigma)\\) for t = 1,‚Ä¶,T\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\psi^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\nfor each t = 1,‚Ä¶,T and s = 1,‚Ä¶\\(S_1+S_2\\), compute \\(\\epsilon_t^{(s)} = u_t^{(s)} + \\psi^{(s)}u_{t-1}^{(s)}\\)\n\nAfter we have obtained \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\), we can use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution \\(p(A \\mid Y, X, \\Sigma, \\Omega)\\):\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Omega^{(s)}, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}, \\Omega^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nSampling from the joint predictive density is the same as before."
  },
  {
    "objectID": "index.html#standard-bayesian-var",
    "href": "index.html#standard-bayesian-var",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.1 Standard Bayesian VAR",
    "text": "4.1 Standard Bayesian VAR\n\n4.1.1 Model Building Code and Validation\nWe verify that our model can replicate the true parameter of the data generate process by: 1. Generate artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. That is,\n\\[\\mathbf{y_t} = \\begin{pmatrix} y_t,1 \\\\ y_t,2\\end{pmatrix} = \\mathbf{y_{t-1}} + \\mathbf{\\epsilon_t} = \\begin{pmatrix}y_{t-1,1}\\\\y_{t-1, 2}\\end{pmatrix} + \\begin{pmatrix}\\epsilon_{t,1}\\\\ \\epsilon_{t, 2}\\end{pmatrix}\\] and\n\\[\\mathbf{\\epsilon} \\sim iid \\mathcal{N}(\\mathbf{0}, \\mathbf{I_2} )\\]\n\n\n\n\n\n\n\n\n\nThen we estimate a model with a constant term and 1 lag using the simulated data, show that the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the posterior mean of the constant term is close to a vector of zeros.\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n       [,1]  [,2]\n[1,]  0.068 0.165\n[2,]  1.000 0.001\n[3,] -0.003 0.995\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n      [,1]  [,2]\n[1,] 0.074 0.071\n[2,] 0.003 0.002\n[3,] 0.002 0.002\n\n\n[1] \"Posterior mean of covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 1.051 0.067\n[2,] 0.067 0.949\n\n\n[1] \"Posterior standard deviation of covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 0.048 0.032\n[2,] 0.032 0.042\n\n\n\n\n4.1.2 Empirical Result\n1. Fitted Model Parameters \n2. Prediction Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Test for Granger Causality We first perform Granger Causality test for each country, we test whether the macroeconomic factor of one country has some predictive power on the macroeconomic factors of another. Specifically, we test the following hypothesis:\n\n$H_0: $ The consumer price index, interest rate, foreign direct investment, balance of payments, gross domestic product, exchange rate against the dollar of country A has no impact on these variables in the country B$\n\nThe Baye‚Äôs factor in this case is defined as:\n\\[B_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij} = 0|Y, X,\\Sigma)d \\Sigma}{\\int p_1(A_{ij}=0, \\Sigma)d\\Sigma}\\] \\[p_1(A_{ij} = 0|Y, X,\\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A_{ij}}, \\Sigma_{ij}, \\bar{V}_{ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_{0, ij}, V_{A, ij}, S_{0, ij}, \\nu_{0, ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) = p(A_{ij}|\\Sigma)\\times p(\\Sigma)\\] \\[A_{ij}|\\Sigma \\sim \\mathcal{MN}_{K \\times N} (A_{0,ij}, \\Sigma, V_{A, ij}) \\] \\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\] and we test the bi-lateral relationship between each of these countries.\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\n-\nInf\n5.466142e+215\nInf\n\n\nUS\nInf\n-\nInf\nInf\n\n\nJapan\nInf\nInf\n-\nInf\n\n\nAustralia\nInf\n1.697063e-58\nInf\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\n-\nStrong Evidence for Granger Causality\nSStrong Evidence for Granger Causality\nStrong Evidence for Granger Causality\n\n\nUS\nStrong Evidence for Granger Causality\n-\nStrong Evidence for Granger Causality\nStrong Evidence for Granger Causality\n\n\nJapan\nStrong Evidence for Granger Causality\nStrong Evidence for Granger Causality\n-\nStrong Evidence for Granger Causality\n\n\nAustralia\nStrong Evidence for Granger Causality\nStrong Evidence for Granger Causality\nStrong Evidence against Granger Causality\n-"
  },
  {
    "objectID": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.2 Large BVAR model with MA(1) Gaussian Innovations",
    "text": "4.2 Large BVAR model with MA(1) Gaussian Innovations\n\n4.2.1 Model Building Code and Validation\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2375\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n       [,1]  [,2]\n[1,]  0.059 0.165\n[2,]  0.999 0.001\n[3,] -0.002 0.995\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n      [,1]  [,2]\n[1,] 0.082 0.069\n[2,] 0.003 0.002\n[3,] 0.003 0.002\n\n\n[1] \"Posterior mean of row specific covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 1.162 0.066\n[2,] 0.066 1.047\n\n\n[1] \"Posterior standard deviation of row specific covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 0.047 0.032\n[2,] 0.032 0.046\n\n\n[1] \"Posterior mean of column specific covariance matrix (first 10 rows and columns):\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,]  1.002 -0.035  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [2,] -0.035  1.002 -0.035  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [3,]  0.000 -0.035  1.002 -0.035  0.000  0.000  0.000  0.000  0.000  0.000\n [4,]  0.000  0.000 -0.035  1.002 -0.035  0.000  0.000  0.000  0.000  0.000\n [5,]  0.000  0.000  0.000 -0.035  1.002 -0.035  0.000  0.000  0.000  0.000\n [6,]  0.000  0.000  0.000  0.000 -0.035  1.002 -0.035  0.000  0.000  0.000\n [7,]  0.000  0.000  0.000  0.000  0.000 -0.035  1.002 -0.035  0.000  0.000\n [8,]  0.000  0.000  0.000  0.000  0.000  0.000 -0.035  1.002 -0.035  0.000\n [9,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.035  1.002 -0.035\n[10,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.035  1.002\n\n\n[1] \"Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.002 0.026 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [2,] 0.026 0.002 0.026 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [3,] 0.000 0.026 0.002 0.026 0.000 0.000 0.000 0.000 0.000 0.000\n [4,] 0.000 0.000 0.026 0.002 0.026 0.000 0.000 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.026 0.002 0.026 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.000 0.026 0.002 0.026 0.000 0.000 0.000\n [7,] 0.000 0.000 0.000 0.000 0.000 0.026 0.002 0.026 0.000 0.000\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.026 0.002 0.026 0.000\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.026 0.002 0.026\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.026 0.002\n\n\n\n\n4.2.1 Empirical Results\n1. Fitted Model Parameters\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.28\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\n [1,]  0.113 -0.246  0.042 -0.185 -0.063  0.068 0.001  0.017  5.657 -0.204\n [2,]  0.992  0.001 -0.009 -0.001  0.000  0.000 0.000  0.000 -0.001  0.000\n [3,]  0.008  0.963  0.010 -0.006  0.000  0.003 0.000  0.000  0.009 -0.001\n [4,]  0.000 -0.004  0.986 -0.002  0.000  0.000 0.000  0.000  0.042  0.000\n [5,]  0.000 -0.001 -0.006  0.992  0.000  0.000 0.000  0.000  0.036  0.000\n [6,]  0.000 -0.001 -0.001  0.000  1.000  0.000 0.000  0.000 -0.001  0.000\n [7,]  0.001  0.002  0.001  0.003  0.000  0.998 0.000  0.000 -0.002  0.001\n [8,] -0.001 -0.001 -0.001  0.000  0.000  0.000 1.000  0.000  0.002  0.000\n [9,]  0.000  0.002  0.000  0.001  0.000  0.000 0.000  1.000 -0.010  0.001\n[10,]  0.003 -0.002 -0.006 -0.002 -0.001 -0.001 0.000 -0.001  1.140  0.002\n[11,]  0.000 -0.002  0.000 -0.001  0.000  0.000 0.000  0.001  0.013  0.998\n[12,]  0.001  0.002  0.001  0.000  0.000  0.000 0.000  0.000  0.005  0.000\n[13,]  0.001 -0.001 -0.001 -0.001  0.000  0.000 0.000  0.000  0.006  0.000\n[14,]  0.000 -0.008 -0.003  0.000  0.000 -0.001 0.000  0.000  0.000  0.000\n[15,] -0.001  0.004 -0.004 -0.002  0.000  0.000 0.000  0.000  0.008  0.000\n[16,]  0.000  0.000 -0.001 -0.001  0.000  0.000 0.000  0.000  0.014  0.000\n[17,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.002  0.000\n[18,] -0.001  0.002  0.000  0.001  0.000  0.000 0.000  0.000 -0.001  0.000\n[19,]  0.001  0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.002  0.000\n[20,]  0.000  0.002  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[21,] -0.004  0.003  0.003  0.000  0.001  0.001 0.000  0.001 -0.087  0.000\n[22,]  0.000  0.002  0.001  0.001  0.000  0.000 0.000  0.000 -0.004 -0.001\n[23,]  0.000  0.000  0.000  0.001  0.000  0.000 0.000  0.000  0.003  0.000\n[24,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.004  0.000\n[25,]  0.000  0.000 -0.002 -0.001  0.000 -0.001 0.000  0.000  0.003  0.000\n[26,]  0.000  0.000  0.001  0.001  0.000  0.000 0.000  0.000  0.007  0.000\n[27,]  0.000  0.000  0.000 -0.001  0.000  0.000 0.000  0.000  0.001  0.000\n[28,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[29,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[30,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.004  0.000\n[31,]  0.000  0.001  0.000  0.001  0.000  0.000 0.000  0.000 -0.002  0.000\n[32,]  0.000  0.001  0.002  0.001  0.000  0.000 0.000  0.000 -0.035  0.000\n[33,] -0.001  0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000\n[34,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[35,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[36,]  0.000  0.000  0.000 -0.001  0.000  0.000 0.000  0.000  0.009  0.000\n[37,]  0.000 -0.001  0.001  0.000  0.000  0.000 0.000  0.000  0.005  0.000\n[38,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000\n[39,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.003  0.000\n[40,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[41,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[42,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[43,]  0.000 -0.001  0.001  0.001  0.000  0.000 0.000  0.000 -0.032  0.000\n[44,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[45,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[46,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[47,]  0.000 -0.002  0.000  0.000  0.000  0.000 0.000  0.000  0.007  0.000\n[48,]  0.000  0.000 -0.001  0.000  0.000  0.000 0.000  0.000  0.002  0.000\n[49,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000\n[50,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.002  0.000\n[51,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[52,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[53,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.002  0.000\n[54,]  0.000  0.001  0.000  0.001  0.000  0.000 0.000  0.000 -0.033  0.000\n[55,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000\n[56,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.002  0.000\n       [,11]\n [1,]  0.001\n [2,]  0.000\n [3,]  0.000\n [4,]  0.000\n [5,]  0.000\n [6,]  0.000\n [7,]  0.000\n [8,]  0.000\n [9,]  0.000\n[10,]  0.001\n[11,] -0.001\n[12,]  1.000\n[13,]  0.000\n[14,]  0.000\n[15,]  0.000\n[16,]  0.000\n[17,]  0.000\n[18,]  0.000\n[19,]  0.000\n[20,]  0.000\n[21,] -0.001\n[22,]  0.000\n[23,]  0.000\n[24,]  0.000\n[25,]  0.000\n[26,]  0.000\n[27,]  0.000\n[28,]  0.000\n[29,]  0.000\n[30,]  0.000\n[31,]  0.000\n[32,]  0.000\n[33,]  0.000\n[34,]  0.000\n[35,]  0.000\n[36,]  0.000\n[37,]  0.000\n[38,]  0.000\n[39,]  0.000\n[40,]  0.000\n[41,]  0.000\n[42,]  0.000\n[43,]  0.000\n[44,]  0.000\n[45,]  0.000\n[46,]  0.000\n[47,]  0.000\n[48,]  0.000\n[49,]  0.000\n[50,]  0.000\n[51,]  0.000\n[52,]  0.000\n[53,]  0.000\n[54,]  0.000\n[55,]  0.000\n[56,]  0.000\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.582 1.063 0.659 0.692 0.080 0.305 0.082 0.095 4.499 0.118 0.098\n [2,] 0.015 0.026 0.015 0.015 0.002 0.008 0.002 0.002 0.102 0.003 0.002\n [3,] 0.013 0.023 0.017 0.015 0.002 0.007 0.002 0.002 0.095 0.003 0.002\n [4,] 0.013 0.025 0.015 0.014 0.002 0.007 0.002 0.002 0.099 0.003 0.002\n [5,] 0.014 0.025 0.016 0.015 0.002 0.007 0.002 0.002 0.104 0.003 0.002\n [6,] 0.015 0.026 0.016 0.015 0.002 0.007 0.002 0.002 0.109 0.003 0.002\n [7,] 0.014 0.023 0.014 0.013 0.002 0.007 0.002 0.002 0.096 0.003 0.002\n [8,] 0.015 0.026 0.015 0.015 0.002 0.008 0.002 0.002 0.103 0.003 0.002\n [9,] 0.013 0.023 0.015 0.015 0.002 0.007 0.002 0.002 0.102 0.003 0.002\n[10,] 0.007 0.011 0.007 0.006 0.001 0.003 0.001 0.001 0.048 0.001 0.001\n[11,] 0.013 0.023 0.017 0.014 0.002 0.007 0.002 0.002 0.101 0.003 0.002\n[12,] 0.016 0.024 0.016 0.015 0.002 0.008 0.002 0.002 0.099 0.003 0.002\n[13,] 0.007 0.013 0.008 0.008 0.001 0.004 0.001 0.001 0.052 0.001 0.001\n[14,] 0.007 0.013 0.007 0.007 0.001 0.004 0.001 0.001 0.052 0.002 0.001\n[15,] 0.007 0.012 0.008 0.007 0.001 0.004 0.001 0.001 0.053 0.002 0.001\n[16,] 0.007 0.012 0.008 0.007 0.001 0.004 0.001 0.001 0.052 0.002 0.001\n[17,] 0.007 0.012 0.008 0.007 0.001 0.004 0.001 0.001 0.055 0.002 0.001\n[18,] 0.007 0.013 0.008 0.007 0.001 0.004 0.001 0.001 0.050 0.002 0.001\n[19,] 0.007 0.012 0.008 0.007 0.001 0.004 0.001 0.001 0.053 0.002 0.001\n[20,] 0.008 0.013 0.008 0.007 0.001 0.004 0.001 0.001 0.052 0.002 0.001\n[21,] 0.006 0.010 0.007 0.006 0.001 0.003 0.001 0.001 0.043 0.001 0.001\n[22,] 0.007 0.012 0.007 0.007 0.001 0.004 0.001 0.001 0.049 0.001 0.001\n[23,] 0.007 0.012 0.007 0.007 0.001 0.004 0.001 0.001 0.051 0.001 0.001\n[24,] 0.005 0.008 0.005 0.005 0.001 0.002 0.001 0.001 0.035 0.001 0.001\n[25,] 0.005 0.009 0.005 0.005 0.001 0.003 0.001 0.001 0.034 0.001 0.001\n[26,] 0.005 0.009 0.005 0.005 0.001 0.002 0.001 0.001 0.035 0.001 0.001\n[27,] 0.005 0.009 0.005 0.005 0.001 0.003 0.001 0.001 0.034 0.001 0.001\n[28,] 0.005 0.009 0.005 0.005 0.001 0.003 0.001 0.001 0.034 0.001 0.001\n[29,] 0.005 0.008 0.005 0.004 0.001 0.002 0.001 0.001 0.032 0.001 0.001\n[30,] 0.005 0.008 0.005 0.005 0.001 0.002 0.001 0.001 0.030 0.001 0.001\n[31,] 0.005 0.008 0.005 0.005 0.001 0.002 0.001 0.001 0.032 0.001 0.001\n[32,] 0.005 0.008 0.004 0.004 0.001 0.002 0.001 0.001 0.032 0.001 0.001\n[33,] 0.005 0.009 0.005 0.005 0.001 0.002 0.001 0.001 0.035 0.001 0.001\n[34,] 0.005 0.008 0.005 0.005 0.001 0.002 0.001 0.001 0.032 0.001 0.001\n[35,] 0.004 0.006 0.004 0.004 0.000 0.002 0.000 0.001 0.026 0.001 0.001\n[36,] 0.003 0.006 0.004 0.004 0.001 0.002 0.000 0.001 0.024 0.001 0.001\n[37,] 0.004 0.006 0.004 0.003 0.001 0.002 0.001 0.001 0.025 0.001 0.001\n[38,] 0.003 0.006 0.004 0.004 0.001 0.002 0.001 0.001 0.026 0.001 0.001\n[39,] 0.003 0.006 0.004 0.004 0.001 0.002 0.001 0.001 0.024 0.001 0.001\n[40,] 0.003 0.006 0.004 0.004 0.001 0.002 0.000 0.001 0.026 0.001 0.000\n[41,] 0.003 0.006 0.004 0.004 0.001 0.002 0.000 0.001 0.026 0.001 0.001\n[42,] 0.004 0.006 0.004 0.004 0.000 0.002 0.000 0.001 0.026 0.001 0.001\n[43,] 0.003 0.005 0.004 0.003 0.000 0.002 0.000 0.001 0.022 0.001 0.001\n[44,] 0.004 0.006 0.004 0.003 0.001 0.002 0.000 0.001 0.027 0.001 0.001\n[45,] 0.004 0.006 0.004 0.003 0.001 0.002 0.000 0.001 0.025 0.001 0.001\n[46,] 0.003 0.005 0.003 0.003 0.000 0.001 0.000 0.000 0.022 0.001 0.000\n[47,] 0.003 0.005 0.003 0.003 0.000 0.001 0.000 0.001 0.020 0.001 0.000\n[48,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.018 0.001 0.000\n[49,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.021 0.001 0.000\n[50,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.021 0.001 0.000\n[51,] 0.003 0.005 0.003 0.003 0.000 0.001 0.000 0.000 0.019 0.001 0.000\n[52,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.021 0.001 0.000\n[53,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.020 0.001 0.000\n[54,] 0.003 0.005 0.003 0.003 0.000 0.001 0.000 0.000 0.017 0.001 0.000\n[55,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.021 0.001 0.000\n[56,] 0.003 0.005 0.003 0.003 0.000 0.001 0.000 0.000 0.021 0.001 0.000\n\n\n[1] \"Posterior mean of row specific covariance matrix:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\n [1,]  0.379 -0.152  0.159  0.051  0.001 -0.036 0.001  0.004  0.126  0.003\n [2,] -0.152  1.533 -0.065  0.101  0.006  0.002 0.006  0.018 -0.744  0.002\n [3,]  0.159 -0.065  0.572  0.194  0.001  0.026 0.005  0.018  0.252 -0.016\n [4,]  0.051  0.101  0.194  0.424  0.001  0.017 0.005  0.011 -0.016 -0.006\n [5,]  0.001  0.006  0.001  0.001  0.011 -0.002 0.000  0.000 -0.263 -0.002\n [6,] -0.036  0.002  0.026  0.017 -0.002  0.086 0.002  0.007  0.129 -0.002\n [7,]  0.001  0.006  0.005  0.005  0.000  0.002 0.009  0.000  0.017  0.000\n [8,]  0.004  0.018  0.018  0.011  0.000  0.007 0.000  0.014 -0.053 -0.002\n [9,]  0.126 -0.744  0.252 -0.016 -0.263  0.129 0.017 -0.053 30.694  0.202\n[10,]  0.003  0.002 -0.016 -0.006 -0.002 -0.002 0.000 -0.002  0.202  0.026\n[11,] -0.013 -0.015 -0.024 -0.011 -0.001 -0.002 0.000 -0.005  0.111  0.002\n       [,11]\n [1,] -0.013\n [2,] -0.015\n [3,] -0.024\n [4,] -0.011\n [5,] -0.001\n [6,] -0.002\n [7,]  0.000\n [8,] -0.005\n [9,]  0.111\n[10,]  0.002\n[11,]  0.015\n\n\n[1] \"Posterior standard deviation of row specific covariance matrix:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.048 0.072 0.045 0.037 0.006 0.017 0.006 0.006 0.322 0.009 0.007\n [2,] 0.072 0.210 0.084 0.071 0.013 0.035 0.011 0.014 0.655 0.019 0.014\n [3,] 0.045 0.084 0.068 0.050 0.008 0.022 0.007 0.008 0.388 0.011 0.009\n [4,] 0.037 0.071 0.050 0.058 0.006 0.018 0.005 0.007 0.347 0.009 0.008\n [5,] 0.006 0.013 0.008 0.006 0.001 0.003 0.001 0.001 0.059 0.002 0.001\n [6,] 0.017 0.035 0.022 0.018 0.003 0.012 0.003 0.003 0.151 0.005 0.003\n [7,] 0.006 0.011 0.007 0.005 0.001 0.003 0.001 0.001 0.050 0.002 0.001\n [8,] 0.006 0.014 0.008 0.007 0.001 0.003 0.001 0.002 0.058 0.002 0.001\n [9,] 0.322 0.655 0.388 0.347 0.059 0.151 0.050 0.058 4.260 0.092 0.061\n[10,] 0.009 0.019 0.011 0.009 0.002 0.005 0.002 0.002 0.092 0.003 0.002\n[11,] 0.007 0.014 0.009 0.008 0.001 0.003 0.001 0.001 0.061 0.002 0.002\n\n\n[1] \"Posterior mean of column specific covariance matrix (first 10 rows and columns):\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,]  1.187 -0.513  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [2,] -0.513  1.224 -0.431  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [3,]  0.000 -0.431  1.187 -0.431  0.000  0.000  0.000  0.000  0.000  0.000\n [4,]  0.000  0.000 -0.431  1.187 -0.431  0.000  0.000  0.000  0.000  0.000\n [5,]  0.000  0.000  0.000 -0.431  1.187 -0.431  0.000  0.000  0.000  0.000\n [6,]  0.000  0.000  0.000  0.000 -0.431  1.187 -0.431  0.000  0.000  0.000\n [7,]  0.000  0.000  0.000  0.000  0.000 -0.431  1.187 -0.431  0.000  0.000\n [8,]  0.000  0.000  0.000  0.000  0.000  0.000 -0.431  1.187 -0.431  0.000\n [9,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.431  1.187 -0.431\n[10,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.431  1.187\n\n\n[1] \"Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.033 0.060 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [2,] 0.060 0.045 0.039 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [3,] 0.000 0.039 0.033 0.039 0.000 0.000 0.000 0.000 0.000 0.000\n [4,] 0.000 0.000 0.039 0.033 0.039 0.000 0.000 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.039 0.033 0.039 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.000 0.039 0.033 0.039 0.000 0.000 0.000\n [7,] 0.000 0.000 0.000 0.000 0.000 0.039 0.033 0.039 0.000 0.000\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.039 0.033 0.039 0.000\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.039 0.033 0.039\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.039 0.033\n\n\n2. Prediction Plots \n2. Forecasts Model Parameters\n3. Test for Granger Causality\n4. Test Time Change \n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.24\n\n\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2133333\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\n [1,]  0.031 -0.196  0.026 -0.176 -0.063  0.084 0.005  0.017  5.891 -0.211\n [2,]  0.992  0.001 -0.010 -0.004  0.000  0.000 0.000  0.000  0.007  0.000\n [3,]  0.005  0.965  0.009 -0.004  0.000  0.002 0.000  0.000  0.020 -0.001\n [4,]  0.001 -0.009  0.988 -0.003  0.000  0.000 0.000  0.000  0.041  0.000\n [5,]  0.000 -0.003 -0.007  0.991  0.000 -0.001 0.000  0.000  0.038  0.000\n [6,]  0.002  0.000 -0.001  0.001  1.000  0.000 0.000  0.000 -0.003  0.000\n [7,]  0.000  0.001 -0.001  0.002  0.000  0.998 0.000 -0.001 -0.009  0.001\n [8,]  0.002  0.000  0.000  0.001  0.000  0.000 1.000  0.000 -0.002  0.000\n [9,]  0.002 -0.002  0.002  0.000  0.000  0.000 0.000  1.000  0.003  0.001\n[10,]  0.003 -0.001 -0.006 -0.001 -0.001  0.000 0.000 -0.001  1.143  0.002\n[11,]  0.003 -0.001  0.002 -0.002  0.000  0.000 0.000  0.001 -0.004  0.999\n[12,]  0.000 -0.001  0.001  0.001  0.000  0.000 0.000  0.000  0.005  0.000\n[13,]  0.000  0.001 -0.001 -0.001  0.000  0.000 0.000  0.000  0.004  0.000\n[14,]  0.001 -0.008 -0.002 -0.002  0.000 -0.001 0.000  0.000  0.001  0.000\n[15,] -0.001  0.001 -0.003 -0.001  0.000  0.000 0.000  0.000  0.005  0.000\n[16,]  0.000 -0.002  0.000 -0.001  0.000  0.000 0.000  0.000  0.010  0.000\n[17,] -0.001  0.003  0.000  0.001  0.000  0.000 0.000  0.000 -0.002  0.000\n[18,] -0.001  0.003 -0.001  0.000  0.000  0.000 0.000  0.000 -0.002  0.000\n[19,]  0.001  0.001  0.000  0.001  0.000  0.000 0.000  0.000 -0.001  0.000\n[20,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.002  0.000\n[21,] -0.003  0.001  0.002 -0.001  0.001  0.000 0.000  0.000 -0.093  0.000\n[22,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[23,] -0.001 -0.001  0.001  0.000  0.000  0.000 0.000  0.000 -0.003  0.000\n[24,] -0.001 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.005  0.000\n[25,] -0.001  0.001 -0.003 -0.001  0.000  0.000 0.000  0.000  0.000  0.000\n[26,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.005  0.000\n[27,] -0.001  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.005  0.000\n[28,]  0.000  0.000  0.001  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[29,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[30,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.005  0.000\n[31,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.002  0.000\n[32,] -0.001  0.000  0.003  0.001  0.000  0.000 0.000  0.000 -0.039  0.000\n[33,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.005  0.000\n[34,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[35,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[36,]  0.000  0.001  0.000 -0.001  0.000  0.000 0.000  0.000  0.003  0.000\n[37,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[38,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[39,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.002  0.000\n[40,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.002  0.000\n[41,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[42,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[43,]  0.000  0.000  0.000  0.001  0.000  0.000 0.000  0.000 -0.033  0.000\n[44,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[45,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.005  0.000\n[46,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[47,]  0.000 -0.001  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[48,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.002  0.000\n[49,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.001  0.000\n[50,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.000  0.000\n[51,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.002  0.000\n[52,]  0.000  0.001  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000\n[53,]  0.000  0.000  0.000 -0.001  0.000  0.000 0.000  0.000  0.001  0.000\n[54,]  0.000  0.000  0.000  0.001  0.000  0.000 0.000  0.000 -0.028  0.000\n[55,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000 -0.001  0.000\n[56,]  0.000  0.000  0.000  0.000  0.000  0.000 0.000  0.000  0.002  0.000\n       [,11]\n [1,] -0.005\n [2,]  0.001\n [3,] -0.001\n [4,]  0.001\n [5,]  0.001\n [6,]  0.000\n [7,]  0.000\n [8,]  0.000\n [9,]  0.000\n[10,]  0.001\n[11,] -0.001\n[12,]  1.000\n[13,]  0.000\n[14,]  0.000\n[15,]  0.000\n[16,]  0.000\n[17,]  0.000\n[18,]  0.000\n[19,]  0.000\n[20,]  0.000\n[21,] -0.001\n[22,]  0.000\n[23,]  0.000\n[24,]  0.000\n[25,]  0.000\n[26,]  0.000\n[27,]  0.000\n[28,]  0.000\n[29,]  0.000\n[30,]  0.000\n[31,]  0.000\n[32,]  0.000\n[33,]  0.000\n[34,]  0.000\n[35,]  0.000\n[36,]  0.000\n[37,]  0.000\n[38,]  0.000\n[39,]  0.000\n[40,]  0.000\n[41,]  0.000\n[42,]  0.000\n[43,]  0.000\n[44,]  0.000\n[45,]  0.000\n[46,]  0.000\n[47,]  0.000\n[48,]  0.000\n[49,]  0.000\n[50,]  0.000\n[51,]  0.000\n[52,]  0.000\n[53,]  0.000\n[54,]  0.000\n[55,]  0.000\n[56,]  0.000\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.617 1.173 0.709 0.676 0.077 0.324 0.087 0.105 4.017 0.113 0.107\n [2,] 0.017 0.028 0.018 0.016 0.002 0.008 0.002 0.002 0.102 0.003 0.002\n [3,] 0.014 0.026 0.016 0.015 0.002 0.007 0.002 0.002 0.102 0.003 0.003\n [4,] 0.015 0.026 0.018 0.014 0.002 0.008 0.002 0.003 0.100 0.003 0.002\n [5,] 0.015 0.025 0.017 0.015 0.002 0.007 0.002 0.002 0.099 0.003 0.003\n [6,] 0.015 0.026 0.018 0.018 0.002 0.007 0.002 0.002 0.094 0.003 0.002\n [7,] 0.014 0.024 0.017 0.015 0.002 0.007 0.002 0.002 0.094 0.003 0.002\n [8,] 0.015 0.027 0.016 0.016 0.002 0.007 0.002 0.002 0.106 0.003 0.002\n [9,] 0.016 0.028 0.017 0.015 0.002 0.008 0.002 0.002 0.094 0.003 0.002\n[10,] 0.007 0.012 0.008 0.007 0.001 0.003 0.001 0.001 0.045 0.001 0.001\n[11,] 0.015 0.026 0.016 0.017 0.002 0.007 0.002 0.002 0.095 0.003 0.003\n[12,] 0.015 0.028 0.016 0.016 0.002 0.008 0.002 0.002 0.099 0.003 0.002\n[13,] 0.008 0.013 0.009 0.008 0.001 0.004 0.001 0.001 0.047 0.002 0.001\n[14,] 0.008 0.013 0.008 0.008 0.001 0.004 0.001 0.001 0.051 0.001 0.001\n[15,] 0.008 0.014 0.009 0.008 0.001 0.004 0.001 0.001 0.048 0.002 0.001\n[16,] 0.007 0.013 0.009 0.009 0.001 0.004 0.001 0.001 0.053 0.002 0.001\n[17,] 0.007 0.013 0.008 0.008 0.001 0.004 0.001 0.001 0.050 0.002 0.001\n[18,] 0.008 0.014 0.009 0.008 0.001 0.004 0.001 0.001 0.047 0.001 0.001\n[19,] 0.008 0.013 0.008 0.008 0.001 0.004 0.001 0.001 0.049 0.001 0.001\n[20,] 0.007 0.014 0.008 0.008 0.001 0.004 0.001 0.001 0.049 0.002 0.001\n[21,] 0.007 0.013 0.007 0.007 0.001 0.004 0.001 0.001 0.040 0.001 0.001\n[22,] 0.008 0.014 0.009 0.008 0.001 0.004 0.001 0.001 0.044 0.001 0.001\n[23,] 0.007 0.014 0.009 0.008 0.001 0.004 0.001 0.001 0.050 0.001 0.001\n[24,] 0.005 0.010 0.006 0.006 0.001 0.003 0.001 0.001 0.034 0.001 0.001\n[25,] 0.005 0.009 0.006 0.006 0.001 0.003 0.001 0.001 0.032 0.001 0.001\n[26,] 0.005 0.009 0.006 0.005 0.001 0.003 0.001 0.001 0.029 0.001 0.001\n[27,] 0.005 0.009 0.006 0.005 0.001 0.003 0.001 0.001 0.035 0.001 0.001\n[28,] 0.005 0.009 0.006 0.006 0.001 0.003 0.001 0.001 0.031 0.001 0.001\n[29,] 0.005 0.009 0.006 0.005 0.001 0.003 0.001 0.001 0.036 0.001 0.001\n[30,] 0.005 0.009 0.006 0.005 0.001 0.003 0.001 0.001 0.031 0.001 0.001\n[31,] 0.006 0.009 0.006 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[32,] 0.005 0.008 0.005 0.004 0.001 0.002 0.001 0.001 0.029 0.001 0.001\n[33,] 0.005 0.010 0.006 0.005 0.001 0.002 0.001 0.001 0.034 0.001 0.001\n[34,] 0.005 0.009 0.006 0.006 0.001 0.003 0.001 0.001 0.031 0.001 0.001\n[35,] 0.004 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001\n[36,] 0.004 0.006 0.004 0.004 0.001 0.002 0.000 0.001 0.023 0.001 0.001\n[37,] 0.004 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.022 0.001 0.001\n[38,] 0.004 0.007 0.004 0.004 0.001 0.002 0.000 0.001 0.023 0.001 0.001\n[39,] 0.003 0.007 0.004 0.004 0.001 0.002 0.000 0.001 0.026 0.001 0.001\n[40,] 0.004 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001\n[41,] 0.004 0.008 0.004 0.004 0.000 0.002 0.000 0.001 0.025 0.001 0.001\n[42,] 0.003 0.007 0.004 0.004 0.001 0.002 0.000 0.001 0.023 0.001 0.001\n[43,] 0.003 0.006 0.004 0.003 0.000 0.002 0.000 0.001 0.021 0.001 0.001\n[44,] 0.004 0.007 0.004 0.004 0.000 0.002 0.000 0.001 0.024 0.001 0.001\n[45,] 0.004 0.006 0.005 0.004 0.001 0.002 0.000 0.001 0.024 0.001 0.001\n[46,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.020 0.001 0.000\n[47,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.001 0.019 0.001 0.000\n[48,] 0.003 0.006 0.003 0.003 0.000 0.002 0.000 0.001 0.019 0.001 0.001\n[49,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.019 0.001 0.000\n[50,] 0.003 0.005 0.004 0.003 0.000 0.002 0.000 0.000 0.017 0.001 0.000\n[51,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.020 0.001 0.000\n[52,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.020 0.001 0.001\n[53,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.000 0.019 0.001 0.001\n[54,] 0.003 0.005 0.003 0.003 0.000 0.001 0.000 0.000 0.018 0.000 0.000\n[55,] 0.003 0.005 0.003 0.003 0.000 0.002 0.000 0.001 0.020 0.001 0.000\n[56,] 0.003 0.006 0.003 0.003 0.000 0.001 0.000 0.000 0.019 0.001 0.000\n\n\n[1] \"Posterior mean of row specific covariance matrix:\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\n [1,]  0.382 -0.160  0.155  0.051  0.000 -0.035 0.001  0.004  0.140  0.004\n [2,] -0.160  1.509 -0.076  0.098  0.007  0.003 0.004  0.016 -0.785 -0.002\n [3,]  0.155 -0.076  0.574  0.192  0.001  0.029 0.004  0.017  0.253 -0.016\n [4,]  0.051  0.098  0.192  0.425  0.001  0.013 0.003  0.011 -0.003 -0.006\n [5,]  0.000  0.007  0.001  0.001  0.011 -0.002 0.000  0.000 -0.261 -0.002\n [6,] -0.035  0.003  0.029  0.013 -0.002  0.086 0.002  0.007  0.121 -0.002\n [7,]  0.001  0.004  0.004  0.003  0.000  0.002 0.009  0.001  0.019  0.000\n [8,]  0.004  0.016  0.017  0.011  0.000  0.007 0.001  0.013 -0.049 -0.002\n [9,]  0.140 -0.785  0.253 -0.003 -0.261  0.121 0.019 -0.049 30.011  0.200\n[10,]  0.004 -0.002 -0.016 -0.006 -0.002 -0.002 0.000 -0.002  0.200  0.025\n[11,] -0.012 -0.012 -0.024 -0.010 -0.001 -0.002 0.000 -0.005  0.107  0.002\n       [,11]\n [1,] -0.012\n [2,] -0.012\n [3,] -0.024\n [4,] -0.010\n [5,] -0.001\n [6,] -0.002\n [7,]  0.000\n [8,] -0.005\n [9,]  0.107\n[10,]  0.002\n[11,]  0.015\n\n\n[1] \"Posterior standard deviation of row specific covariance matrix:\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]\n [1,] 0.049 0.075 0.049 0.037 0.006 0.018 0.005 0.007 0.331 0.009 0.008\n [2,] 0.075 0.210 0.093 0.077 0.012 0.035 0.011 0.014 0.604 0.018 0.013\n [3,] 0.049 0.093 0.072 0.049 0.008 0.022 0.006 0.008 0.388 0.011 0.009\n [4,] 0.037 0.077 0.049 0.057 0.006 0.018 0.006 0.007 0.343 0.009 0.008\n [5,] 0.006 0.012 0.008 0.006 0.002 0.003 0.001 0.001 0.056 0.002 0.001\n [6,] 0.018 0.035 0.022 0.018 0.003 0.012 0.003 0.003 0.152 0.004 0.003\n [7,] 0.005 0.011 0.006 0.006 0.001 0.003 0.001 0.001 0.050 0.001 0.001\n [8,] 0.007 0.014 0.008 0.007 0.001 0.003 0.001 0.002 0.059 0.002 0.001\n [9,] 0.331 0.604 0.388 0.343 0.056 0.152 0.050 0.059 3.872 0.085 0.063\n[10,] 0.009 0.018 0.011 0.009 0.002 0.004 0.001 0.002 0.085 0.003 0.002\n[11,] 0.008 0.013 0.009 0.008 0.001 0.003 0.001 0.001 0.063 0.002 0.002\n\n\n[1] \"Posterior mean of column specific covariance matrix (first 10 rows and columns):\"\n\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,]  1.177 -0.494  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [2,] -0.494  1.210 -0.418  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [3,]  0.000 -0.418  1.177 -0.418  0.000  0.000  0.000  0.000  0.000  0.000\n [4,]  0.000  0.000 -0.418  1.177 -0.418  0.000  0.000  0.000  0.000  0.000\n [5,]  0.000  0.000  0.000 -0.418  1.177 -0.418  0.000  0.000  0.000  0.000\n [6,]  0.000  0.000  0.000  0.000 -0.418  1.177 -0.418  0.000  0.000  0.000\n [7,]  0.000  0.000  0.000  0.000  0.000 -0.418  1.177 -0.418  0.000  0.000\n [8,]  0.000  0.000  0.000  0.000  0.000  0.000 -0.418  1.177 -0.418  0.000\n [9,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.418  1.177 -0.418\n[10,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.418  1.177\n\n\n[1] \"Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):\"\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.039 0.071 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [2,] 0.071 0.054 0.046 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n [3,] 0.000 0.046 0.039 0.046 0.000 0.000 0.000 0.000 0.000 0.000\n [4,] 0.000 0.000 0.046 0.039 0.046 0.000 0.000 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.046 0.039 0.046 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.000 0.046 0.039 0.046 0.000 0.000 0.000\n [7,] 0.000 0.000 0.000 0.000 0.000 0.046 0.039 0.046 0.000 0.000\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.046 0.039 0.046 0.000\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.046 0.039 0.046\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.046 0.039\n\n\n\n\n4.4.2 Empirical Result\n```"
  }
]