[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "",
    "text": "Objective: Construct a Bayesian Vector Autoregression model to forecast major macroeconomic indicators for the United States, Australia, Japan, and China to facilitate an investigation into the prospective interdependencies between the economies of these nations.\nQuestion: This research project will examine how trade relationships, investment flows, monetary policy environments, and economic performances within the United States, Australia, Japan and China mutually influence each other, and assess the implications of these interactions for predicting future values of these economic indicators.\nMotivation: Since the onset of the COVID-19 pandemic, the global economic landscape has witnessed a series of unprecedented shifts in key macroeconomic indicators, spurred by governments‚Äô adoption of varied expansionary monetary policies. Initially, to buffer their economies, many nations implemented expansive monetary strategies, later swiftly transitioning to interest rate hikes in a bid to manage surging inflation rates‚Äîa scenario not seen in decades. The pandemic‚Äôs disruption to trade further exacerbated inflationary pressures for some economies, highlighting the intricate interdependencies among major economies with significant trade and financial ties. This period recorded stark contrast in inflation levels, with unprecedented highs in the US and Australia and notably low inflation in China and Japan. Amidst this turmoil, a divergence in economic paths also became apparent, thei9 United States and Australia have witness robust economic rebounds, whereas China and Japan saw more tepid recoveries. This research aims to dissect the nuanced web of economic interdependencies between the United States, Australia, Japan, and China, analyzing how their trade relationships, investment flows, and monetary policy environments have mutually influenced their economic performances. Additionally, it seeks to understand the ramifications of these dynamics for the predictive accuracy of future economic indicators, offering insights into the evolving global economic order."
  },
  {
    "objectID": "index.html#data-plots",
    "href": "index.html#data-plots",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Data Plots",
    "text": "Data Plots"
  },
  {
    "objectID": "index.html#stationarity-check",
    "href": "index.html#stationarity-check",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "Stationarity Check",
    "text": "Stationarity Check\nStationary Tests\nThis section employs the Augmented Dickey-Fuller test to examine the null hypothesis that a unit root is present in these time series. A lag order of 4 was chosen for the ADF test given our data‚Äôs quarterly frequency. The results indicate that most of the macroeconomic indicators possess a unit root and are not stationary, with a few exceptions. Specifically, we have enough evidence to reject the null hypothesis for the CPI series in China and the United States at a 5% significance level. Interestingly, Japan‚Äôs GDP series is found to be unit-root stationary at this significance level. This implies that Japan‚Äôs GDP exhibits a consistent mean and variance over time, contrasting with the general trend observed in other national GDP series. Japan‚Äôs GDP growth has been known to be stagnant for quite some years and it‚Äôs macroeconomic indicators may respond differently to economic shocks compared to its non-stationary counterparts\n\n\n\nADF Test Results for CPI Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.258408\n4\n0.08079872\nJP_CPI\n\n\n-4.299916\n4\n0.01\nCN_CPI\n\n\n-3.55799\n4\n0.0394958\nUS_CPI\n\n\n-3.333834\n4\n0.06822768\nAU_CPI\n\n\n\n\n\n\n\n\nADF Test Results for Exchange Rate (% change) Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-1.791111\n4\n0.6635047\nJP_XCH\n\n\n-1.565957\n4\n0.7575635\nCN_XCH\n\n\n-2.56175\n4\n0.3415666\nAU_XCH\n\n\n\n\n\n\n\n\nADF Test Results for seasonally adjusted Data by Country\n\n\nDickey-Fuller Statistic\nLag Order\nP-value\nCountry\n\n\n\n\n-3.499383\n4\n0.04535076\nJP_GDP\n\n\n-2.472818\n4\n0.3802514\nCN_GDP\n\n\n0.7030938\n4\n0.99\nUS_GDP\n\n\n-2.030497\n4\n0.5639204\nAU_GDP\n\n\n\n\n\nThe ACF plots of seasonally adjusted GDP data shows a strong autocorrelation at smaller lags and the autocorrelation decreases sharply as the lags increase. It is worth noting that the autocorrelations decreases much faster for Japan than for the other countries. Similar phenomenon is observed with exchange rates against the dollar for all three countries. It is worth noting that the autocorrelations for the Chinese series a more persistent than the others, possibly due to the fact that China has a managed floating exchange rate system, where the Chinese central banks sets a target exchange rate. The CPI data for all four countries shows generally low autocorrelation across lags, particularly beyond the first few periods. This suggests that CPI measurements are relatively independent from quarter to quarter. The alternating signs in the autocorrelation function (ACF) plot for Japan and US‚Äôs CPI data suggest possible seasonal effects, cyclical patterns or due to data collection methods."
  },
  {
    "objectID": "index.html#model-1-standard-bvarp-model",
    "href": "index.html#model-1-standard-bvarp-model",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.1 Model 1: Standard BVAR(p) Model",
    "text": "3.1 Model 1: Standard BVAR(p) Model\n\n3.1.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, I_T)\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(I_T\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\\(x_{t}^T = \\left( \\begin{array}{cccc}1  & y_{t-1} & y_{t-2} & \\cdots & y_{t-p} \\end{array} \\right)\\)\n\nIn our specific application, the Y matrix is formulated as follows:\n\\[\nY = \\begin{pmatrix}\n    \\text{CPI}_{\\text{CN}, p+1} & \\text{XCH}_{\\text{CN}, p+1} & \\log(\\text{GDP})_{\\text{CN}, p+1} & \\text{CPI}_{\\text{US}, p+1} & \\log(\\text{GDP})_{\\text{US}, p+1} & \\text{CPI}_{\\text{JP}, p+1} & \\text{XCH}_{\\text{JP}, p+1} & \\log(\\text{GDP})_{\\text{JP}, p+1} & \\text{CPI}_{\\text{AU}, p+1} & \\text{XCH}_{\\text{AU}, p+1} & \\log(\\text{GDP})_{\\text{AU}, p+1} \\\\\n    \\text{CPI}_{\\text{CN}, p+2} & \\text{XCH}_{\\text{CN}, p+2} & \\log(\\text{GDP})_{\\text{CN}, p+2} & \\text{CPI}_{\\text{US}, p+2} & \\log(\\text{GDP})_{\\text{US}, p+2} & \\text{CPI}_{\\text{JP}, p+2} & \\text{XCH}_{\\text{JP}, p+2} & \\log(\\text{GDP})_{\\text{JP}, p+2} & \\text{CPI}_{\\text{AU}, p+2} & \\text{XCH}_{\\text{AU}, p+2} & \\log(\\text{GDP})_{\\text{AU}, p+2} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n    \\text{CPI}_{\\text{CN}, T} & \\text{XCH}_{\\text{CN}, T} & \\log(\\text{GDP})_{\\text{CN}, T} & \\text{CPI}_{\\text{US}, T} & \\log(\\text{GDP})_{\\text{US}, T} & \\text{CPI}_{\\text{JP}, T} & \\text{XCH}_{\\text{JP}, T} & \\log(\\text{GDP})_{\\text{JP}, T} & \\text{CPI}_{\\text{AU}, T} & \\text{XCH}_{\\text{AU}, T} & \\log(\\text{GDP})_{\\text{AU}, T}\n\\end{pmatrix}\n\\]\nThe Bayesian Vector Autoregression model as formulated above provides a robust framework for investigating the relationships among selected economic indicators across different nations. By employing this model, this research aims to quantitatively measure the influence of one country‚Äôs economic indicators on another, such as how lagged changes in China‚Äôs i consumer price index may influence the GDP growth rate of the United States and vice versa. The BVAR model, with its estimation of coefficients across various lags, offers a deep understanding of both immediate and more delayed economic interactions, which is crucial to analyzing the cyclical nature of trade relationships, investment flows, monetary policy environments, and economic performances and the transmission of these metrics across borders.\nThe strength of this BVAR model lies in its ability to incorporate prior economic knowledge and beliefs into the estimation process. By setting prior distributions for the matrix of coefficients A and the covariance matrix \\(\\Sigma\\), the model can be tailors to reflect established economic theories regarding international economic linkages and the time it takes for policy changes in one country to affect another. By calibrating the prior variances, particularly for the autoregressive coefficients, we can integrate prior knowledge or hypotheses, such as the presence of unit roots or the diminishing influence of distant lags on current values, into the analysis. When interpreting the estimation output, attention will be given to the posterior means and variances of the coefficients, which represent the model‚Äôs ‚Äúlearnt‚Äù understanding of the underlying economic structure. The analysis will be supplemented by forecast error variance decompositions to better understand the proportion of the movements in economic indicators that can be accounted for by their own shocks versus shocks to other variables.\nThe economic context underscoring this analysis is the increased globalization over the past decade, marking an era where economies are more intertwined than ever through trade, capital flows, and policy decisions. This period has witnessed not only the strengthening of global economic ties but also recent calls from political leaders advocating for a reduction in globalization. These contrasting dynamics highlight the complexity of the current global economic landscape, where the push for deeper integration coexists with growing sentiments for retrenchment. This dual trend sets the stage for our investigation, providing a rich context to explore how economic variables across nations influence each other amidst fluctuating levels of global interconnectedness. In this environment, understanding the cross-country spillover effects is vital for policymakers and businesses alike, as decisions made in one country can have far-reaching implications. By addressing these aspects, this research will contribute to the discourse on economic policy formulation, risk assessment, and strategic planning.\n\n\n3.1.2 Prior Settings\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of coefficient matrices A and the row-specific variance matrix \\(\\Sigma\\), and a Minnesota prior on the coefficients A. Specifically, we have:\n\\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0) \\]\n\\[p(\\Sigma) \\propto |\\Sigma|^{-\\frac{\\nu_0+N+1}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\]\n\\[A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\]\n\\[p(A|\\Sigma) \\propto |\\Sigma|^{-\\frac{K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\n\\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\]\n\\[p(A,\\Sigma) \\propto |\\Sigma|^{-\\frac{K+\\nu_0+N+1}{2}} \\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_0))\\times exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^T(A-A_0)))\\]\nwhere\n\\[V_A = diag(\\kappa_2 \\quad \\kappa_1(\\mathbf{p} \\otimes I_N^T))\\]\n\\[\\mathbf{p} = [1 \\quad 2 \\quad ... \\quad p]\\] \\[I_N = [1 \\quad 1 \\quad ... \\quad 1] \\in \\mathbb{R}^N\\] \\[\\kappa_1 \\text{ is the overall shrinkage level for autoregressive slopes}\\] \\[\\kappa_2 \\text{ is the overall shrinkage lvel for the constant term }\\]\nAdditionally, we adopt commonly used values for the hyperparameters as established in the literature.\n\\[A_0 = 0\\]\n\\[v_0 = N+3\\]\n\\[S_0 = I_N\\]\n\\[\\kappa_1 = 0.2^2 \\quad \\kappa_2 = 10^2\\]\nThe hyperparameters \\(\\kappa_1\\) and \\(\\kappa_2\\) are specified in a way such that the coefficient associated with a lag l variable is shrunk more heavily as lag length increases whereas the intercepts are not shrunk to 0.\n\n\n3.1.3 Posterior Distributions\nThe posterior distribution specified above has the form\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{TN}{2}}|\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y)  = \\frac{p(A,\\Sigma,Y)}{p(Y)}\\propto p(A, \\Sigma, Y) \\propto p(Y|A,\\Sigma)\\times p(A,\\Sigma) =  p(Y|A,\\Sigma) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(Y-XA))) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\]\n\\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^TY-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\] Hence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^TX + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^TY + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^TY + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.1.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\nwe use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution:\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nThe draws from joint predictive density can then be obtained using the following algorithm:\n\nSample S draws \\(\\left\\{ A^{(s)}, \\Sigma^{(s)} \\right\\}_{s=1}^{S}\\) from \\(p(A,\\Sigma|Y, X)\\)\nSample S draws \\(\\left\\{ Y_{t+h}^{(s)} \\right\\}_{s=1}^S\\) from \\(Y_{t+h}^{(s)} \\sim \\mathcal{N}_{hN}(Y_{t+h|t}(A^{(s)}), \\mathbb{V}ar[Y_{t+h|t}|A^{(s)}, \\Sigma^{(s)}])\\)\n\nwhere\n\\[\\underset{\\text{hN} \\times1}{Y_{t+h}} = \\begin{pmatrix}\n    Y_{t+1} \\\\\n    Y_{t+2} \\\\\n    \\vdots    \\\\\n    Y_{t+h}\n\\end{pmatrix}\\]\nTo derive the posterior predictive density for \\(Y_{t+h}\\), we first note that:\n\\[p(Y_{t+h}|Y_{t}) = \\int \\int p(Y_{t+h}|Y_{t},A, \\Sigma)\\times p(A,\\Sigma|Y,X)dAd\\Sigma\\] where\n\\[p(y_{t+h}|Y, X, A, \\Sigma, I_T) \\sim \\mathcal{N}_{hN} (Y_{t+h|t}(A), Var(Y_{t+h}|A, \\Sigma))\\]\n\\[Y_{t+h|t}(A) =\\begin{pmatrix}\n    Y_{t+1|t} \\\\\n    Y_{t+2|t} \\\\\n    \\vdots    \\\\\n    Y_{t+h|t}\n\\end{pmatrix} = \\begin{pmatrix}  \n\\mu_0 + A_1 Y_t + \\cdots + A_pY_{t-p+1|t} \\\\\n\\mu_0 + A_1 Y_{t+1|t} + \\cdots + A_p Y_{t-p+2|t}\\\\\n\\vdots \\\\\n\\mu_0 + A_1 Y_{t+h-1|t} + \\cdots + A_p Y_{t+h-p|t}\n\\end{pmatrix}\\]\n\\[\\underset{\\text{(hN-1)} \\times \\text{(hN-1)}}{\\mathbb{V}ar(Y_{t+h|t}|A, \\Sigma)} = \\begin{pmatrix}\n    \\Sigma & \\Sigma \\phi_1^T & \\cdots & \\Sigma \\phi_{h-1}^T \\\\\n    \\phi_1\\Sigma &\\Sigma + \\phi_1 \\Sigma \\phi_1^T  & \\cdots & \\Sigma \\phi_1^T + \\phi_1 \\Sigma \\phi_2^T+ \\cdots + \\phi_{h-2}\\Sigma \\phi_{h-1}^T + \\phi_{h-1} \\Sigma \\phi_{h}^T \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{h-1}\\Sigma\\ & \\phi_1\\Sigma  + \\phi_2 \\Sigma \\Phi_1^T + \\cdots + \\phi_{h} \\Sigma \\phi_{h-1}^T & \\cdots & \\Sigma + \\phi_1 \\Sigma \\phi_1^T + \\cdots+\\phi_{h-2}\\Sigma\\phi_{h-2}^T+\\phi_{h-1}\\Sigma\\phi_{h-1}^T\n\\end{pmatrix}\n\\]\nwhere \\(\\phi_i = J A^i J'\\) are the parameters of the VMA(\\(\\infty\\)) representation of VAR(\\(p\\)) and \\(A\\) is the parameter matrix of \\(VAR(1)\\) representation of VAR(\\(p\\)). \\(A^i\\) is the matrix \\(A\\) raised to the power of \\(i\\). \\[\n\\underset{Np \\times Np}{A} =\n\\begin{pmatrix} A_1 & A_2 & \\cdots & A_{p-1} & A_p \\\\\nI_N & 0_{N \\times N} & \\cdots & 0_{N \\times N} & 0_{N \\times N}  \\\\\n0 & I_N  & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & I_N & 0 \\end{pmatrix}\n\\qquad\n\\underset{\\text{N} \\times \\text{Np}}{J} = [I_N \\quad 0_{N \\times N(p-1)}]\n\\] \\[p(A|Y, X, \\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\] \\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\] as before. In this approach, we are incorporating into our prediction the uncertainties in the estimates of the parameters by integrating over \\(A\\) and \\(\\Sigma\\).\n\n\n3.1.5 Test for Granger Causality\nGranger causality testing, introduced in Granger (1969), is a way to measure whether the past and current values of one time series contains useful information for predicting future values of another. The idea behind granger causality testing is that if a time series does contain predictive information about another, incorporating it into the model should reduce the error variance and improve the precision of the forecasts. The classical approach for testing causality is the Wald test, but it fails to quantify the degree of evidence in the data in favour of or against the causality hypothesis. The Bayesian approach to testing Granger causality based on Baye‚Äôs factor, on the other hand, has the ability to quantify the evidence for and against the hypothesis with a single number. The Baye‚Äôs factor is defined as:\n\\[B_H = \\frac{p[H_0|Y]}{p[H_1|Y]}\\] In the context of testing for VAR(p) models, we have the marginal likelihood under the assumption of no Granger causality is:\n\\[p_0(Y) = \\int p_1(Y| A_{ij} = 0, A_{-(ij)})dA_{-(ij)} = p_1(Y|A_{ij}=0)\\] using Baye‚Äôs rule, we have: \\[p_0(Y) = \\frac{p_1(A_{ij} = 0|Y) \\times p_1(Y)}{p_1(A_{ij} = 0)}\\] We divide the above equation by \\(p(y_t|H_1)\\) to get Baye‚Äôs factor:\n\\[B_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij=0}|Y, \\Sigma)d \\Sigma}{\\int p(A_{ij}=0, \\Sigma)d\\Sigma}\\]\nand the integration is performed using numerical integration methods. The second part is also known as the Savage-Dickey density ratio estimator, which is the ratio of the posterior over the prior density evaluated at \\(A_{ij} = 0\\). Indicative values for interpreting Bayes factors is provided below:\nTo test for Granger causality, we use the equation: \\[B_H = \\frac{p_1(A_{ij} = 0|Y)}{p(A_{ij}=0)} \\] A value much greater than 1 would indicate that the posterior distribution has a higher probability density at \\(A_{ij} = 0\\) than the prior, hence the data provides evidence for the null hypothesis that \\(A_{ij} = 0\\). Conversely, a value much smaller than 1 would suggest that the data provides evidence against the null hypothesis."
  },
  {
    "objectID": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#model-2-large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations",
    "text": "3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations\nIncorporating MA(1) Gaussian innovations in a large BVARs model can lead to a significant enhancement over traditional BVAR models, especially when forecasting macroeconomic variables, for several reasons. By allowing for serial correlation in the innovations term, we will be able to capture the momentum or persistence in economic variables that is often observed in real-world data. Recognizing that shocks may have a lasting impact over several periods can enhance the model‚Äôs ability to predict future values by considering the path-dependent nature of the economy. In addition, our BVAR model with common stochastic volatility is a natural extension to the standard BVAR model and formulated as below:\n\n3.2.1 Model Specification\n\\[ Y = XA+E\\] \\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nwhere\n\n\\(T\\) is the number of time periods under consideration\n\\(N\\) is the number of variables, in our case, N = 20\n\\(P\\) is the number of lags\n\\(Y\\) is a \\(T \\times N\\) matrix of variables of response variables we aim to model.\n\\(A\\) is a \\(K \\times N\\) matrix of coefficients, \\(K = (1+ùëù\\times N)\\).\n\\(E\\) is a \\(T \\times N\\) matrix of the error terms\n\\(X\\) is a \\(T \\times (1+ùëù\\times N)\\) matrix of covariates\n\\(\\Sigma\\) is a \\(N \\times N\\) matrix representing the row-specific covariance matrix\n\\(\\Omega\\) is a \\(T \\times T\\) identity matrix representing the column specific covariance matrix\n\\(E|X\\) follows a matrix-variate normal distribution with mean \\(0_{T \\times N}\\), row specific covariance matrix \\(\\Sigma_{N \\times N}\\) and column specific covariance matrix \\(I_T\\)\n\nwith MA(1) innovations, we have, for i = 1,‚Ä¶.,N and t = 1,‚Ä¶,T, \\[e_{t, i} = \\eta_{t, i} + \\psi \\eta_{t-1, i}\\] where \\(|\\psi|&lt;1\\) and \\(\\eta_{t,i} \\sim \\mathcal{N}(0,1)\\)\nIn matrix notation, we have:\n\\[e_i = H_{\\psi} \\eta_i\\]\nwhere\n\\[\ne_i = \\begin{bmatrix}\ne_{1, i} \\\\\ne_{2, i} \\\\\n\\vdots \\\\\ne_{T, i}\n\\end{bmatrix} \\qquad\n\\eta_i = \\begin{bmatrix}\n\\eta_{1, i} \\\\\n\\eta_{2, i} \\\\\n\\vdots \\\\\n\\eta_{T, i}\n\\end{bmatrix} \\qquad\nH_{\\psi} = \\left(\n\\begin{array}{cccc}\n1 & 0  & \\cdots & 0 \\\\\n\\psi & 1  & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\psi & 1\n\\end{array}\n\\right) \\qquad\nO_{\\psi} = diag(1+\\psi^2, 1, ..., 1)\n\\]\nHence, we have\n\\[E \\sim \\mathcal{N}(0, H_{\\psi}O_{\\psi}H_{\\psi}^T) \\qquad \\qquad \\Omega = H_{\\psi}O_{\\psi}H_{\\psi}^T\\] Note that the covariance matrix \\(\\Omega\\) depends on \\(\\psi\\) only.\n\n\n3.2.2 Prior Specification\nWe consider a prior independent distributions for \\((A, \\Sigma, \\Omega)\\), specifically, we have: \\[P(A, \\Sigma, \\Omega) = P(A, \\Sigma) \\times P(\\Omega)\\]\nWe will employ a Normal-Inverse Wishart distribution for the joint distribution of A and \\(\\Sigma\\) and before, \\[(A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_0, V_A, S_0, \\nu_0)\\]\nWe apply a truncated normal prior on \\(\\psi\\), \\[\\psi \\sim \\mathcal{N}(\\psi_0, V_{\\psi})\\mathbb{1}_{\\{|\\psi|&lt;1\\}}\\]\nFor estimation purposes, we initialize with the following setting:\n\n\\(e_{i1} \\sim \\mathcal{N}(0, 1+\\psi^2)\\)\n\\(\\psi_0 = 0\\) and \\(V_{\\psi} = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1, 1)\n\n\n\n3.2.3 Posterior Distributions\n\nThe posterior distribution of Y specified above has the form:\n\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\n\\[= (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}(1+\\psi^2)^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T(H_{\\psi}O_{\\psi}H_{\\psi}^T)^{-1}(Y-XA))\\]\n\nThe joint posterior distribution of A and \\(\\Sigma\\) can be derived as follows:\n\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\] 3. The posterior distribution for the parameter \\(\\psi\\) can be obtained as follows: \\[P(\\psi|Y, A, \\Sigma) = \\frac{P(\\psi, Y, A, \\Sigma)}{P(Y, A, \\Sigma)} \\propto P(\\psi, Y, A, \\Sigma) = P(Y|A, \\Sigma, \\psi) \\times P(A,\\Sigma, \\psi) \\]\n\\[= P(Y|A, \\Sigma, \\psi) \\times P(A, \\Sigma) \\times P(\\psi) \\propto P(Y|A, \\Sigma, \\psi) \\times P(\\psi)\\]\nwe can sample from the posterior distribution of \\(\\psi\\) using an independence-chain Metropolis-Hastings algorithm.\n\n\n3.2.4 Estimation Procedure\nWe obtain posterior estimates of \\(A, \\Sigma, \\psi\\) using a Gibbs sampler, specifically, we initialize \\(\\psi^{(0)}\\) and for s = 1,‚Ä¶,S1+S2, we sequentially sample:\n\n\\(\\Sigma^{(s)} | Y, X, \\psi^{(s-1)} \\sim \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\\(A^{(s)} | Y, X, \\Sigma^{(s)}, \\psi^{(s-1)} \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma^{(s)}, \\bar{V})\\)\n\\(\\psi^{(s)} | Y, X, A^{(s)}, \\Sigma^{(s)} \\propto P(Y|A^{(s)}, \\Sigma^{(s)}, \\psi^{s-1})\\times p(\\psi)\\)\n\nThe Metropolis-Hastings Algorithm for Sampling \\(\\psi\\) is given as follows:\nInitialization:\n\nChoose an initial value \\(\\psi^{(0)}\\) within the bounds \\((-1, 1)\\).\n\nProposal Distribution:\n\nSelect the proposal distribution \\(q(\\psi' | \\psi^{(t)}) \\sim N(\\psi^{(t)}, \\tau^2)\\). \\(\\tau^2\\) is a tuning parameter that controls the step size.\n\nSampling Loop:\nFor t = 1,‚Ä¶,\\(T_1+T_2\\)\n\nGenerate a candidate \\(\\psi'^{(t)}\\) from \\(q(\\psi' | \\psi^{(t-1)})\\).\nCheck if \\(\\psi'\\) is within the bounds \\((-1, 1)\\). If not, reject \\(\\psi'\\) (set \\(\\alpha = 0\\)).\nCompute the acceptance ratio \\(\\alpha\\): \\[ \\alpha(\\psi^{(t-1)}, \\psi') = \\min\\left(1, \\frac{p(Y | A, \\Sigma, \\psi') p(\\psi') q(\\psi^{(t-1)} | \\psi')}{p(Y | A, \\Sigma, \\psi^{(t-1)}) p(\\psi^{(t-1)}) q(\\psi' | \\psi^{(t-1)})}\\right)\\]\n\nDecide to accept or reject:\n\nGenerate a random number \\(u\\) from \\(U[0,1]\\).\nIf \\(u \\leq \\alpha\\), accept \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi'\\).\nOtherwise, reject \\(\\psi'\\) and set \\(\\psi^{(t)} = \\psi^{(t-1)}\\).\n\nBurn in period\n\nDiscard the first \\(T_1\\) samples to allow the algorithm to converge to the true distribution\n\nObtain one sample of \\(\\psi\\)\n\nRandomly draw a \\(\\psi^*\\) from the \\(T_2\\) draws and set \\(\\psi^{(s)} = \\psi^*\\)\n\nWhen combined with Gibbs sampling for the estimation of \\(A\\) and \\(\\Sigma\\), the Metropolis-Hastings step can be embedded into the Gibbs sampler, a method known as Metropolis-within-Gibbs sampling. In this approach, one sample from the Metropolis-Hastings step is generated per Gibbs iteration. Detailed proofs and explanations on the convergence properties and efficiency of the Metropolis-within-Gibbs sampling method are provided in Chib and Greenberg (1995).\nWe monitor the acceptance rate and adjust \\(\\tau^2\\) as necessary to achieve an optimal rate of about 20-40%.\nWe note that since \\(\\Omega\\) is a band matrix, which means we do not need compute \\(\\Omega^{-1}\\). Instead, we obtain the Cholesky decomposition \\(C_{\\Omega}\\) of \\(\\Omega\\), which has a time complexity of \\(O(T)\\) instead of \\(O(T^3)\\). Terms involving \\(\\Omega^{-1}\\) such as \\(X^T\\Omega^{-1}X\\) can be obtained by: \\[X^T\\Omega^{-1}X = X^T(C_{\\Omega}^{-1})^{T}C_{\\Omega}^{-1}X=(C_{\\Omega}^{-1}X)^T(C_{\\Omega}^{-1}X) = \\tilde{X}^T\\tilde{X}\\]\nTo make forecasts of the \\(Y\\), we make the following observations: One-period ahead forecast \\[y_{t+1} = \\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1} +  e_{t+1}\\] \\[E(y_{t+1}) = E(\\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1} +  e_{t+1}) = \\mu_0 + A_1y_t + \\cdots + A_py_{t-p+1}\\] The one-period ahead forecast error is: \\[e_{t+1|t} = y_{t+1} - y_{t+1|t} = e_{t+1}\\] The one period ahead forecast variance is: \\[\\mathbb{V}ar(e_{t+1}|t) = \\mathbb{E}[\\mathbb{E}_t(e_{t+1}e_{t+1}^T)] = \\Sigma\\] Two-period ahead forecast \\[y_{t+2} = \\mu_0 + A_1y_{t+1} + \\cdots + A_py_{t-p+2} +  e_{t+2}\\] \\[E(y_{t+2}) = E(\\mu_0 + A_1y_{t+1} + \\cdots + A_py_{t-p+2} +  e_{t+2}) = \\mu_0 + A_1y_{t+1|t} + \\cdots + A_py_{t-p+2}\\] The two-period ahead forecast error is: \\[e_{t+2|t} = y_{t+2}-y_{t+2|t} = e_{t+2} + A_1(y_{t+1} - y_{t+1|t}) = e_{t+2} + A_1e_{t+1}\\] \\[Var(e_{t+2|t}) = E[e_{t+2|t}e_{t+2|t}^T] = E((e_{t+2} + A_1e_{t+1})(e_{t+2} + A_1e_{t+1})^T) = E((e_{t+2}e_{t+2}^T + A_1e_{t+1}e_{t+2}^T+e_{t+2}e_{t+1}^TA_1^T+A_1e_{t+1}e_{t+1}^TA_1^T)\\] \\[ = \\Sigma + A_i\\Psi+ \\Psi A_1^T + A_1\\Sigma A_1^T\\] since in the MA(1) innovations setup, we have:\n\\[\\mathbb{V}ar(e_{t+2} e_{t+1}^T) = E \\left[\n\\begin{pmatrix}\ne_{t+2, 1} \\\\\ne_{t+2, 2} \\\\\n\\vdots \\\\\ne_{t+2, N}\n\\end{pmatrix}\n\\begin{pmatrix}\ne_{t+1, 1} & e_{t+1, 2} & \\cdots & e_{t+1, N}\n\\end{pmatrix}\n\\right] \\] \\[=\nE \\left[\n\\begin{pmatrix}\ne_{t+2, 1}e_{t+1, 1} & e_{t+2, 1}e_{t+1, 2} & \\cdots & e_{t+2, 1}e_{t+1, N}\\\\\ne_{t+2, 2}e_{t+1, 1} & e_{t+2, 2}e_{t+1, 2} & \\cdots & e_{t+2, 2}e_{t+1, N}\\\\\n\\vdots               &    \\vdots            & \\vdots &    \\vdots          \\\\\ne_{t+2, N}e_{t+2, 1} & e_{t+2, N}e_{t+2, 2} & \\vdots &    e_{t+2,N}e_{t+1,N}\n\\end{pmatrix}\n\\right]\\] \\[ = E \\left[\n\\begin{pmatrix}\n(\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, 1} + \\psi \\eta_{t+1, 1})(\\eta_{t,N}+\\psi\\eta_{t,N})) \\\\\n(\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, 2} + \\psi \\eta_{t+1, 2})(\\eta_{t,N}+\\psi\\eta_{t,N})) \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n(\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t+1, 1} + \\psi \\eta_{t, 1}) & (\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t+1, 2}+\\psi \\eta_{t,2}) & \\cdots & (\\eta_{t+2, N} + \\psi \\eta_{t+1, N})(\\eta_{t,N}+\\psi\\eta_{t,N}))\\\n\\end{pmatrix}\n\\right]\n\\] \\[= \\begin{pmatrix}\n\\psi\\mathbb{E}[\\eta_{t+1,1}^2] & 0 & \\cdots & 0\\\\\n0 & \\psi\\mathbb{E}[\\eta_{t+1,2}^2] & \\cdots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\cdots & \\psi\\mathbb{E}[\\eta_{t+1,N}^2]\n\\end{pmatrix} = \\begin{pmatrix}\n\\psi & 0 & \\cdots & 0\\\\\n0 & \\psi & \\cdots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\cdots & \\psi\n\\end{pmatrix} = \\Psi\\]\nsince \\(\\eta_{t,i} \\overset{iid}{\\sim}\\mathcal{N}(0,1)\\)"
  },
  {
    "objectID": "index.html#model-3-large-bvar-model-with-common-stochastic-volatility",
    "href": "index.html#model-3-large-bvar-model-with-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.3 Model 3: Large BVAR model with common Stochastic Volatility",
    "text": "3.3 Model 3: Large BVAR model with common Stochastic Volatility\nThe variances of economic shocks are rarely constant over time. Volatility tends to cluster during periods of economic crisis and becomes more tranquil during stable times. Thus, treating the innovations as having constant variance is an unrealistic assumption in practice. To remedy the situation, we can incorporate stochastic volatility into the model, thereby allowing the model to adapt to changing volatility in the data. This typically lead to improvement in model performance, especially in the presence of financial market instability or shifts in economic policy. A BVAR model with common stochastic volatility is s specified as follows:\n\\[ Y = XA+E\\] \\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\n\\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\] where\n\\[e_t \\sim \\mathcal{N}(0, e^{h_t}\\Sigma)\\] and \\(h_t\\) follows an AR(1) process. \\[h_t = \\rho h_{t-1} + u_t^h\\] \\[u_t^h \\sim N(0, \\sigma^2_h)\\]"
  },
  {
    "objectID": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "href": "index.html#model-4-large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility",
    "text": "3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility\n\n3.4.1 Model Specification\n\\[ Y = XA+E\\]\nwhere\n\\[\nA = \\begin{bmatrix}\n\\mu_0^T \\\\\nA_1^T \\\\\n\\vdots \\\\\nA_p^T\n\\end{bmatrix}\n, \\quad\nY = \\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_T^T\n\\end{bmatrix}\n, \\quad\nx_t = \\begin{bmatrix}\n1 \\\\\ny_{t-1}^T \\\\\n\\vdots \\\\\ny_{t-p}^T\n\\end{bmatrix}\n, \\quad\nX = \\begin{bmatrix}\nx_1^T \\\\\nx_2^T \\\\\n\\vdots \\\\\nx_T^T\n\\end{bmatrix}\n, \\quad\nE = \\begin{bmatrix}\ne_1^T \\\\\ne_2^T \\\\\n\\vdots \\\\\ne_T^T\n\\end{bmatrix}\n\\]\nthe same as before. But, instead of the column specific matrix as the identity matrix, we specify the column specific matrix as a diagonal matrix \\(\\Omega\\). Specifically, we have:\n\\[\\epsilon_t = u_t + \\psi_1 u_{t-1}\\]\n\\[u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\]\n\\[h_t = \\rho h_{t-1} + u_t^h \\quad \\text{ follows an Autoregressive process of lag 1 AR(1), and}\\]\n\\[u_t^h \\sim N(0,\\sigma_h^2)\\]\n\\[\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\]\nIn this specification, each element of \\(e_t\\) may have distinct variances, and the variances of all innovations can be scaled by a common factor. This approach is economically intuitive, as the volatility of macroeconomic variables often exhibit co-movement. It is important to emphasize that each component of \\(e_t\\) must adhere to the same univariate time series model.\n\n\n3.4.2 Prior Specification\nHere, we consider a priori independent distributions for \\((A, \\Sigma, \\Omega)\\), namely:\n\\[p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\] Given this structure, we can sample from the posterior distribution by sequentially sampling from:\n\n\\(P(A, \\Sigma | Y, X, \\Omega)\\)\n\\(P(\\Omega | Y, X, A, \\Sigma)\\)\n\nThe prior distribution of \\((A,\\Sigma)\\) follow the same normal inverse Wishart prior distribution as outlined in model one. But the variance matrix \\(V_A\\) for A is different:\n\\[V_A = diag(v_{A,ii})\\]\n\\[v_{A,ii} = \\begin{cases}\n\\kappa_1(\\frac{l^2}{\\hat{s}_r}) & \\text{for a coefficient associated to lag l of variable r} \\\\\n\\kappa_2& \\text{for an intercept}\n\\end{cases}\\]\nwhere \\(\\hat{s}_r\\) is the sample variance of an AR(4) model for the variable r.\n\\[E|X \\sim \\mathcal{MN}_{T \\times N}(0_{T \\times N},\\Sigma_{N \\times N}, \\Omega_{T \\times T})\\]\nand,\nFor the moving average coefficients, we adopt an uninformative truncated normal prior for the MA coefficient \\(\\psi\\):\n\\[\n\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\n\\]\nand we set \\(\\psi_0 = 0\\) and \\(V_\\psi = 1\\) so that the prior centers around 0 with a relatively large variance and has support within (-1,1). Further more, we assume independent priors for \\(\\sigma^2_h\\) and \\(\\rho\\):\n\\[\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\]\n\\[\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\]\nWe set the hyperparameters \\(\\nu_{h_0} = 5\\), \\(s_{h_0} = 0.04\\), \\(\\rho_0 = 0.9\\) and \\(V_\\rho = 0.04\\) so that the prior mean of \\(\\sigma_h^2\\) is 0.01 and \\(\\rho\\) is centered at 0.9.\n\n\n3.4.3 Posterior Distribution\nThe posterior distribution specified above has the following form:\n\\[p(Y|A, \\Sigma) = (2\\pi)^{-\\frac{Tn}{2}}|\\Sigma|^{-\\frac{T}{2}}|\\Omega|^{-\\frac{N}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA))\\]\nand the joint posterior distribution\n\\[p(A, \\Sigma \\mid Y, \\Omega)  = \\frac{p(A,\\Sigma,Y, \\Omega)}{p(Y, \\Omega)}\\] \\[\\propto p(A, \\Sigma, Y, \\Omega) \\propto p(Y \\mid A,\\Sigma, \\Omega)\\times p(A,\\Sigma, \\Omega)\\] \\[=  p(Y \\mid A,\\Sigma, \\Omega) p(A, \\Sigma) p(\\Omega) = p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma) p(\\Omega)\\]\n\\[\\propto p(Y \\mid A,\\Sigma, \\Omega) p(A \\mid \\Sigma) p(\\Sigma)\\]\n\\[\\propto |\\Sigma|^{-\\frac{T}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}(Y-XA)^T\\Omega^{-1}(Y-XA)) \\times\\]\n\\[\\mid \\Sigma \\mid^{-\\frac{\\nu_0+N+K}{2}}exp(-\\frac{1}{2}tr(\\Sigma^{-1}S_o))exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))\\]\n\\[\\propto |\\Sigma|^{-\\frac{T+N+K+\\nu_0+1}{2}} \\times \\exp (-\\frac{1}{2}tr(\\Sigma^{-1}S_0)) \\times \\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\\Omega^{-1}Y-\\bar{A}^T\\bar{V}^{-1}\\bar{A})\\times\\] \\[exp(-\\frac{1}{2}tr(\\Sigma^{-1}(A-\\bar{A})^T\\bar{V}^{-1}(A-\\bar{A})))\\]\nHence,\n\\[p(A, \\Sigma \\mid Y, X) = p(A \\mid Y, X, \\Sigma) p(\\Sigma \\mid Y, X)\\]\n\\[p(A \\mid Y, X, \\Sigma, \\Omega) = \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\]\n\\[p(\\Sigma \\mid Y, X) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\]\nwhere\n\\[\\bar{V} = (X^T\\Omega^{-1}X + V_A^{-1})^{-1}\\]\n\\[\\bar{A} = \\bar{V}(X^T\\Omega^{-1}Y + V_A^{-1}A_0)\\]\n\\[\\bar{\\nu} = T + \\nu_0\\]\n\\[\\bar{S} = S_0 + Y^T\\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \\bar{A}^T\\bar{V}^{-1}\\bar{A}\\]\n\n\n3.4.4 Estimation Procedure\nIn this setting, we have\n\n\\((A,\\Sigma) \\sim \\mathcal{NIW}_{K \\times N} (A_0, V_A, S_0, \\nu_0)\\)\n\\(p(A, \\Sigma, \\Omega) = p(A, \\Sigma) \\times p(\\Omega)\\)\n\nthen, prior draws can be sampled from\n\n\\(\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\)\n\\(A|\\Sigma \\sim \\mathcal{MN}_{K \\times N}(A_0, \\Sigma, V_A)\\)\n\\(\\Omega = \\left(\n\\begin{array}{cccc}\n(1 + \\psi_1^2) e^{h_1} & \\psi_1 e^{h_1} & \\cdots & 0 \\\\\n\\psi_1 e^{h_1} & \\psi_1^2 e^{h_1} + e^{h_2} & \\cdots & \\vdots \\\\\n0 & \\cdots & \\ddots & \\vdots \\\\\n\\vdots & \\cdots & \\psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \\psi_1 e^{h_{T-1}} \\\\\n0 & \\cdots & \\psi_1 e^{h_{T-1}} & \\psi_1^2 e^{h_{T-1}} + e^{h_T}\n\\end{array}\n\\right)\\)\n\\(\\epsilon_t = u_t + \\psi_1 u_{t-1}\\)\n\\(\\psi \\sim \\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\n\\(u_t \\sim \\mathcal{N}(0,e^{h_t} \\Sigma)\\)\n\\(h_t = \\rho h_{t-1} + u_t^h\\)\n\\(\\rho \\sim \\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\n\\(u_t^h \\sim N(0,\\sigma_h^2)\\)\n\\(\\sigma_h^2 \\sim \\mathcal{IG}(\\nu_{h_0}, s_{h_0})\\)\n\nTo sample \\(S_1+S_2\\) draws of \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\)\n\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\sigma^{2,(s)}_h\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{IG}(v_{h_0}, s_{h_0})\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\rho^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\rho_0, V_\\rho) \\mathbb{1}(|\\rho|&lt;1)\\)\nFor each \\(\\sigma^{2,(s)}_h\\), sample \\(\\left\\{u_t^{h,(s)}\\right\\}_{t=1}^T\\) from \\(N(0,\\sigma_h^{2,(s)})\\)\nFor t = 1,‚Ä¶,T and s = 1,‚Ä¶, \\(S_1+S_2\\), compute \\(h_t^{(s)} = \\rho h_{t-1}^{(s)} + u_t^{h,(s)}\\)\nSample \\(S_1+S_2\\) draws of \\(\\left\\{u_t^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(u_t \\sim \\mathcal{N}(0,e^{h_t^{(s)}} \\Sigma)\\) for t = 1,‚Ä¶,T\nSample \\(S_1+S_2\\) draws of \\(\\left\\{\\psi^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\) from \\(\\mathcal{N}(\\psi_0, V_\\psi) \\mathbb{1}(|\\psi|&lt;1)\\)\nfor each t = 1,‚Ä¶,T and s = 1,‚Ä¶\\(S_1+S_2\\), compute \\(\\epsilon_t^{(s)} = u_t^{(s)} + \\psi^{(s)}u_{t-1}^{(s)}\\)\n\nAfter we have obtained \\(\\left\\{\\Omega^{(s)}\\right\\}_{s=1}^{S_1+S_2}\\), we can use the following Gibb‚Äôs Sampler algorithm to sample from the posterior distribution \\(p(A \\mid Y, X, \\Sigma, \\Omega)\\):\n\nInitialize \\(\\Sigma\\) at \\(\\Sigma^0\\)\nFor \\(s = 1,...,S_1+S_2\\)\n\nDraw a sample \\(A^{(s)}\\) from \\(p(A \\mid Y, X, \\Omega^{(s)}, \\Sigma^{(s-1)}) \\sim \\mathcal{MN}_{k \\times N}(\\bar{A}, \\Sigma, \\bar{V})\\)\nDraw a sample \\(\\Sigma^{(s)}\\) from \\(p(\\Sigma \\mid Y, X, A^{(s)}, \\Omega^{(s)}) = \\mathcal{IW}_N(\\bar{S}, \\bar{\\nu})\\)\n\n\nWe discard the first \\(S_1\\) sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain \\(S_2\\) sampled draws from the joint posterior distribution.\n\\[\\left\\{A^{(s)}, \\Sigma^{(s)}\\right\\}_{s=S_1+1}^{S_1+S_2}\\]\nSampling from the joint predictive density is the same as before."
  },
  {
    "objectID": "index.html#standard-bayesian-var",
    "href": "index.html#standard-bayesian-var",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.1 Standard Bayesian VAR",
    "text": "4.1 Standard Bayesian VAR\n\n4.1.1 Model Building Code and Validation\nWe verify that our model can replicate the true parameter of the data generate process by: 1. Generate artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. That is,\n\\[\\mathbf{y_t} = \\begin{pmatrix} y_t,1 \\\\ y_t,2\\end{pmatrix} = \\mathbf{y_{t-1}} + \\mathbf{\\epsilon_t} = \\begin{pmatrix}y_{t-1,1}\\\\y_{t-1, 2}\\end{pmatrix} + \\begin{pmatrix}\\epsilon_{t,1}\\\\ \\epsilon_{t, 2}\\end{pmatrix}\\] and\n\\[\\mathbf{\\epsilon} \\sim iid \\mathcal{N}(\\mathbf{0}, \\mathbf{I_2} )\\]\n\n\n\n\n\n\n\n\n\nThe BVAR_fit function takes in ‚Äòdata‚Äô as an argument, where columns of ‚Äòdata‚Äô represent different variables and rows correspond to observations over time. It also requires an integer p, which specifies the number of lags to include in the BVAR model. The parameters S1 and S2 denote the number of burn-in samples and actual samples, respectively. Kappa1 is the shrinkage level applied to the autoregressive coefficients, while Kappa2 controls the shrinkage for the constant term. The A_prior and V_prior parameters set the prior mean and covariance matrices for the matrix A. Finally, S_prior and nu_prior define the scale and shape parameters, respectively, for the prior distribution of the covariance matrix \\(\\Sigma\\)\nWe use our function to estimate a model with a constant term and lag order 1 using simulated data from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. We show that sampled posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the sampled posterior mean of the constant term is close to a vector of zeros.\n\n\n[1] \"Posterior mean of autoregressive parameter:\"\n\n\n      [,1]  [,2]\n[1,] 0.029 0.019\n[2,] 0.987 0.002\n[3,] 0.006 0.993\n\n\n[1] \"Posterior standard deviation of autoregressive parameter:\"\n\n\n      [,1]  [,2]\n[1,] 0.033 0.033\n[2,] 0.005 0.005\n[3,] 0.006 0.006\n\n\n[1] \"Posterior mean of covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 0.993 0.037\n[2,] 0.037 1.045\n\n\n[1] \"Posterior standard deviation of covariance matrix:\"\n\n\n      [,1]  [,2]\n[1,] 0.045 0.033\n[2,] 0.033 0.047\n\n\n\n\n4.1.2 Empirical Result\n1. Fitted Model Parameters \n2. Prediction Plots\n\n\n[1] \"Running prediction for 50000 iterations took: 150.792000 seconds\"\n\n\n\n\n\n\n\n\n\n\n\n3. Test for Granger Causality We first perform Granger Causality test for each country, we test whether the macroeconomic factor of one country has some predictive power on the macroeconomic factors of another. Specifically, we test the following hypothesis:\n\\(H_0:\\) The consumer price index, interest rate, foreign direct investment, balance of payments, gross domestic product, exchange rate against the dollar of country A has no impact on these variables in the country B$\nThe Baye‚Äôs factor in this case is defined as:\n\\[B_H = \\frac{p_0(Y)}{p_1(Y)} = \\frac{p_1(A_{ij} = 0|Y)}{p_1(A_{ij} = 0)}=\\frac{\\int p_1(A_{ij} = 0|Y, X,\\Sigma)d \\Sigma}{\\int p_1(A_{ij}=0, \\Sigma)d\\Sigma}\\] \\[p_1(A_{ij} = 0|Y, X,\\Sigma) \\sim \\mathcal{MN}_{K \\times N}(\\bar{A_{ij}}, \\Sigma_{ij}, \\bar{V}_{ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) \\sim \\mathcal{NIW}_{K \\times N}(A_{0, ij}, V_{A, ij}, S_{0, ij}, \\nu_{0, ij})\\] \\[p_1(A_{ij} = 0, \\Sigma) = p(A_{ij}|\\Sigma)\\times p(\\Sigma)\\] \\[A_{ij}|\\Sigma \\sim \\mathcal{MN}_{K \\times N} (A_{0,ij}, \\Sigma, V_{A, ij}) \\] \\[\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\\] and we test the bi-lateral relationship between each of these countries.\n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 789.064000 seconds\"\n\n\n             CN           US           JP          AU\nCN -317580.5237     1089.429     1085.807    1092.836\nUS    1101.7079 -1620359.059     1365.100    1387.193\nJP     970.7232     1173.675 -1615075.537    1274.899\nAU    1176.9835     1252.898     1382.705 -934788.054\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\nTesting our null hypothesis using the Savage-Dickey ratio indicates that with p=5 lags, no country Granger-causes the economic indicators of another country. However, the lags of each country do Granger-cause their own economic indicators. We will test p = 4 lags next.\n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 419.345000 seconds\"\n\n\n             CN           US           JP          AU\nCN -267991.2859     1000.911     1075.129    1072.120\nUS    1185.1994 -1634313.502     1442.189    1496.028\nJP     903.4911     1257.867 -1630903.634    1337.353\nAU    1283.7351     1367.249     1445.831 -961567.913\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\nTesting our null hypothesis using the Savage-Dickey ratio indicates that with p=4 lags, no country Granger-causes the economic indicators of another country. However, the lags of each country do Granger-cause their own economic indicators. We will test the model for pre-covid and post-covid period next.\n\n\nShow code\np = 5\nkappa1 = 0.2^2\nkappa2 = 10^2\ndata = final_df[final_df$Time &lt;=as.yearqtr('2020 Q1'),2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\nS1 = 50000\nS2 = 50000\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nY = as.matrix(Y)\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nposterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nS = 500\nbayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 794.595000 seconds\"\n\n\nShow code\nprint(bayesfactor)\n\n\n             CN            US           JP           AU\nCN -281998.1392      973.1642     1056.472     925.4674\nUS    1099.3353 -1251164.6365     1156.950    1171.8745\nJP     782.9349     1125.0988 -1261986.661    1101.2836\nAU    1114.7368     1166.8979     1124.335 -726447.7069\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\nTesting our null hypothesis using the Savage-Dickey ratio indicates that with p=4 lags, no country Granger-causes the economic indicators of another country in the pre-Covid period from 1994 Q1 to 2019 Q4. Next, we test the hypothesis on data from Q1 2004, two years after China joined the World Trade Organization.\n\n\nShow code\np = 5\nkappa1 = 0.2^2\nkappa2 = 10^2\ndata = final_df[final_df$Time &gt;=as.yearqtr('2007 Q1'),2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\nS1 = 50000\nS2 = 50000\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nY = as.matrix(Y)\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nposterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nS = 500\nbayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      \n      bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 734.097000 seconds\"\n\n\nShow code\nprint(bayesfactor)\n\n\n             CN           US           JP           AU\nCN -503928.9576     805.5069     559.2700     810.7126\nUS     512.0541 -534553.6767     794.4756     808.6964\nJP     435.0394     681.8808 -538763.0290     704.4582\nAU     697.0203     808.4421     795.2099 -341018.5352\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n\n\nShow code\np = 5\nkappa1 = 0.2^2\nkappa2 = 10^2\ndata = final_df[(final_df$Time &lt;=as.yearqtr('2020 Q1'))&(final_df$Time &gt;=as.yearqtr('2007 Q1')),2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\nS1 = 50000\nS2 = 50000\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nY = as.matrix(Y)\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nposterior_V = solve(t(X)%*%X + solve(V_prior) + diag(lambda, nrow = ncol(X)))\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples = BVAR_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)\nposterior_samples_A = posterior_samples[[1]]\nposterior_samples_Sigma = posterior_samples[[2]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nS = 500\nbayesfactor = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      \n      bayesfactor[country_a, country_b] = mean(unlist(calc_bayes_factor(country_a, country_b, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 740.827000 seconds\"\n\n\nShow code\nprint(bayesfactor)\n\n\n             CN           US           JP           AU\nCN -334805.5105     532.3481     425.6160     542.7338\nUS     499.5787 -336461.7507     524.1738     540.4940\nJP     400.0171     510.4894 -348412.1619     523.9333\nAU     533.9248     539.0220     508.9101 -218777.3443\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence"
  },
  {
    "objectID": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "href": "index.html#large-bvar-model-with-ma1-gaussian-innovations",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.2 Large BVAR model with MA(1) Gaussian Innovations",
    "text": "4.2 Large BVAR model with MA(1) Gaussian Innovations\n\n4.2.1 Model Building Code and Validation\n\n\nShow code\nBVAR_MA_fit &lt;- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N){\n  Y = (data[(p+1):nrow(data),])\n  N = ncol(Y)\n  Ty = nrow(Y)\n  S = S1 + S2\n  X = matrix(1,nrow(Y),1) \n  K = 1+p*N\n  tau = 0.15\n  for (i in 1:p){\n    X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n  }\n  X = as.matrix(X)\n  Y = as.matrix(Y)\n  \n  nu_bar = Ty + nu_prior\n  psi = 0\n  H_psi = matrix(0, nrow = Ty, ncol = Ty)\n  O_psi = diag(rep(1, Ty))\n  O_psi[1,1] = 1+psi^2\n  for (t in 1:Ty){\n    H_psi[t,t] = 1\n    if (t &lt; Ty){\n      H_psi[t+1, t] = psi\n    }\n  }\n  A_posterior = array(rnorm(prod(c(c(K,N),S))),c(c(K,N),S))\n  Sigma_posterior = array(dim = c(N,N,S))\n  Omega_posterior = array(dim = c(Ty, Ty, S+1))\n  Omega_posterior[,,1] = H_psi %*% O_psi %*% t(H_psi)\n  \n  accept = 0\n  pb &lt;- progress_bar$new(total = S)\n  \n  for (s in 1:S){\n    pb$tick()\n\n    chol_Omega = chol(Omega_posterior[,,s]) #########change \n    chol_Omega_inv = solve(chol_Omega)\n    #V_bar = solve(t(X)%*%X + solve(V_prior))\n    V_bar = solve(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%X) + solve(V_prior))\n    L = t(chol(V_bar))\n    #A_bar = V_bar%*%(t(X)%*%Y + solve(V_prior)%*%A_prior)\n    A_bar = V_bar%*%(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%Y) + solve(V_prior)%*%A_prior)\n    #S_bar = S_prior +  t(Y)%*%Y + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar\n    S_bar = S_prior +  t(chol_Omega_inv%*%Y)%*%(chol_Omega_inv%*%Y) + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar\n    S_bar_inv = solve(S_bar)\n    Sigma_posterior[,,s] = solve(rWishart(1, df=nu_bar, Sigma=S_bar_inv)[,,1])\n    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,1]) ############change \n    ###############MH Step    ##################################\n    psi_candidate = rnorm(n = 1, mean = psi, sd = tau)\n    if(abs(psi_candidate) &gt; 1){\n      alpha = 0\n    }\n    else{\n      H_psi = matrix(0, nrow = Ty, ncol = Ty)\n      O_psi = diag(rep(1, Ty))\n      O_psi[1,1] = 1+psi_candidate^2\n      for (i in 1:Ty){\n        H_psi[i,i] = 1\n        if (i &lt; Ty){\n          H_psi[i+1, i] = psi_candidate\n        }\n      }\n      \n      \n      Omega_draw = H_psi %*% O_psi %*% t(H_psi)\n      chol_Omega_draw = chol(Omega_draw)\n      chol_Omega_draw_inv = solve(chol_Omega_draw)\n      Sigma_posterior_inv = solve(Sigma_posterior[,,s])\n\n      Likelihood_ratio = ((1+psi_candidate^2)/(1+psi^2))^(N/2) * exp(-1/2*(tr(\n        Sigma_posterior_inv%*%t(chol_Omega_draw_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_draw_inv%*%(Y-X%*%A_posterior[,,s]))\n        ) - tr(\n          Sigma_posterior_inv%*%t(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))%*%(chol_Omega_inv%*%(Y-X%*%A_posterior[,,s]))))\n        )\n      prior_ratio = dtruncnorm(x = psi_candidate, mean = psi_prior, sd = Vpsi_prior)/dtruncnorm(x = psi, mean = psi_prior, sd = Vpsi_prior)\n      proposal_ratio = dnorm(x = psi, mean = psi_candidate, sd = tau)/dnorm(x=psi_candidate, mean = psi, sd = tau)\n      alpha = min(1, Likelihood_ratio, prior_ratio, proposal_ratio)\n    }\n    u = runif(1, min = 0, max = 1)\n    if (u &lt;= alpha){\n      accept = accept + 1\n      psi = psi_candidate\n      Omega_posterior[,,s+1] = Omega_draw\n      }\n    else{\n      Omega_posterior[,,s+1] = Omega_posterior[,,s]}\n    ##################################################    #####\n  }\n  A_posterior = A_posterior[,,(S1+1):(S1+S2)]\n  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]\n  Omega_posterior = Omega_posterior[,,(S1+2):(S1+S2+1)]\n  print(\"acceptance rate of the Metropolis Hasting Step: \")\n  print(accept/S)\n  return(list(A_posterior, Sigma_posterior, Omega_posterior))\n}\n\n\n\n\nShow code\np = 1\nkappa1 = 0.1^2\nkappa2 = 10^2\n\nN = ncol(random_walk_sample)\nA_prior     = matrix(0, 1+p*N, ncol = N)\nV_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior     = diag(1, N)\nnu_prior    = ncol(random_walk_sample)+1\nS1 = 1000\nS2 = 1000\nvh0_prior = 5\nsh0_prior = 0.04\nrho_prior = 0.9\nVrho_prior = 0.04\npsi_prior = 0\nVpsi_prior = 1\n\nstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2015\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on 1000 bi-variate random walk for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on 1000 bi-variate random walk for 2000 iterations took: 4393.880000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\n# print('Posterior mean of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,mean),3)\n# print('Posterior standard deviation of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,sd),3)\n# print('Posterior mean of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,mean),3)\n# print('Posterior standard deviation of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,sd),3)\n# print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)\n# print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)\n\n\n\n\nShow code\np = 5\nkappa1 = 0.02^2\nkappa2 = 10^2\ndata = final_df[,2:ncol(final_df)]\nN = ncol(data)\nA_prior = matrix(0,1+p*N,N)\nA_prior[2:(N + 1),] = diag(N)\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nS_prior = diag(1, N)\nnu_prior  = ncol(data)+1\ndata = final_df[,2:ncol(final_df)]\nS1 = 5000\nS2 = 5000\n\nvh0_prior = 5\nsh0_prior = 0.04\nrho_prior = 0.9\nVrho_prior = 0.04\npsi_prior = 0\nVpsi_prior = 1\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2367\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on data for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 4393.880000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\n# print('Posterior mean of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,mean),3)\n# print('Posterior standard deviation of autoregressive parameter:')\n# round(apply(posterior_samples_A_MA,1:2,sd),3)\n# print('Posterior mean of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,mean),3)\n# print('Posterior standard deviation of row specific covariance matrix:')\n# round(apply(posterior_samples_Sigma_MA,1:2,sd),3)\n# print('Posterior mean of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,mean),3)\n# print('Posterior standard deviation of column specific covariance matrix (first 10 rows and columns):')\n# round(apply(posterior_samples_Omega_MA[1:10, 1:10,],1:2,sd),3)\n\n\n2. Prediction Plots \n\n\nShow code\nBVAR_MA_predict = function(posterior_samples_A_MA, posterior_samples_Sigma_MA ,posterior_samples_Omega_MA,  data, h){\n  S = dim(posterior_samples_A_MA)[3]\n  N = dim(posterior_samples_A_MA)[2]\n  p = (dim(posterior_samples_A_MA)[1] -1) / N\n  Y = (data[(p+1):nrow(data),])\n  #last p observations \n  x = Y[(nrow(Y)-p+1):nrow(Y),]\n  # reverse the order t-p t-(p-1)...\n  x = x[p:1,]\n  Y_h   = array(NA,c(h,N,S))\n  for (s in 1:S){\n    A     = posterior_samples_A_MA[,,s]\n    Sigma = posterior_samples_Sigma_MA[,,s]\n    Omega = posterior_samples_Omega_MA[]\n    N = ncol(A)\n    p = (nrow(A) -1) / N\n    Y = (data[(p+1):nrow(data),])\n    x  = Y[(nrow(Y)-p+1):nrow(Y),]\n    x = x[p:1,]\n    \n    for (i in 1:h){\n      #x_{{t+1}= (1 y_t y_{t-1},... y_{t-p+1})\n      x_T               = c(1,purrr::as_vector(t(x)))\n      #y_t+h = mvnnorm(1, x_t%*%A)\n      Period_Y               = mvtnorm::rmvnorm(1, mean = x_T%*%A, sigma=Sigma)\n      colnames(Period_Y) = colnames(x)\n      x            = rbind(Period_Y,x[1:(p-1),])\n      Y_h[i,,s]   = Period_Y[1:N]\n      }\n  }\n  return(Y_h)\n}\n\n\n\n\nShow code\ndata = final_df[,2:ncol(final_df)]\nh = 8\nstart = system.time({\n  BVAR_MA_prediction = BVAR_MA_predict(posterior_samples_A_MA = posterior_samples_A_MA, \n                                       posterior_samples_Sigma_MA = posterior_samples_Sigma_MA,\n                                       posterior_samples_Omega_MA = posterior_samples_Omega_MA,\n                                       data = data, h = h)\n}) \nprint(sprintf(\"Predict %d periods ahead MA BVAR on data for %d iterations took: %f seconds\", h, S1 + S2, start[[\"elapsed\"]]))\n\n\n[1] \"Predict 8 periods ahead MA BVAR on data for 10000 iterations took: 295.702000 seconds\"\n\n\n\n\n\n\n\n\n\n\n\n3. Test for Granger Causality\n\n\nShow code\np = 5\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\n\nS = 500\ncalc_bayes_factor_MA = function(country1, country2, posterior_samples_A_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S) {\n  var_list = c('_GDP', '_CPI', '_XCH')\n  \n  country1_suffix = if (country1 == \"US\") var_list[1:(length(var_list)-1)] else var_list\n  country2_suffix = if (country2 == \"US\") var_list[1:(length(var_list)-1)] else var_list\n\n  country1_vars = paste0(country1, country1_suffix)\n  country2_vars = paste0(country2, country2_suffix)\n\n  A_0_mat = apply(posterior_samples_A_MA, 1:2, mean)\n  dimnames(A_0_mat) &lt;- list(c(\"constant\", rep(colnames(data), p)), colnames(data))\n  A_0_df &lt;- setNames(as.data.frame(A_0_mat), colnames(A_0_mat))\n\n  # Set specified entries to zero based on countries\n  A_0_df[country2_vars, country1_vars] &lt;- 0\n\n  A_0_mat &lt;- as.matrix(A_0_df)\n  \n  ratio_list &lt;- vector(\"list\", length = S)\n  total_S = dim(posterior_samples_Omega_MA)[3]\n  for (s in 1:S) {\n    posterior_V = solve(t(X) %*% posterior_samples_Omega_MA[,,(total_S - s +1)] %*% X + solve(V_prior) + diag(lambda, ncol(X)))\n    prior_Sigma = MCMCpack::riwish(v = nu_prior, S = S_prior)\n    M = c(A_prior)\n    Q = kronecker(prior_Sigma, V_prior)\n    A_0 = c(A_0_mat)\n    prior_prob = mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)\n    \n    M = c(posterior_samples_A_MA[,,(total_S - s +1)])\n    Q = kronecker(posterior_samples_Sigma_MA[,,(total_S - s +1)], posterior_V)\n    posterior_prob = mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)\n    ratio_list[[(total_S - s +1)]] = posterior_prob - prior_prob\n  }\n  return(ratio_list)\n}\n\n\n\n\nShow code\nbf_ma = list()\ni = 1\nS = 500\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 729.752000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n             CN           US           JP           AU\nCN -102995.2548     376.6218     329.8678     374.3145\nUS     384.1965 -519894.1348     346.6272     384.7220\nJP     374.3626     376.6692 -519025.1346     380.5245\nAU     376.5729     384.0064     318.8114 -300157.7369\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n\n\nShow code\np = 5\ndata = final_df[final_df$Time &lt;= '2020 Q1', 2:ncol(final_df)]\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nS = 500\nS1 = 5000\nS2 = 5000\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.2785\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on data for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 729.752000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nbf_ma = list()\ni = 1\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 727.850000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n            CN          US           JP          AU\nCN -152484.355    1175.141     967.5032    1099.247\nUS    1197.693 -669448.716    1055.1428    1212.656\nJP    1023.618    1224.422 -717239.2782    1159.456\nAU    1194.419    1220.634     286.9042 -401502.781\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n\n\nShow code\np = 5\ndata = final_df[final_df$Time &gt;= '2007 Q1', 2:ncol(final_df)]\n#data = final_df[,2:ncol(final_df)]\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nS = 500\nS1 = 5000\nS2 = 5000\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.3803\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on data for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 727.850000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\ncalc_bayes_factor_MA = function(country1, country2, posterior_samples_A_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S) {\n  var_list = c('_GDP', '_CPI', '_XCH')\n  \n  country1_suffix = if (country1 == \"US\") var_list[1:(length(var_list)-1)] else var_list\n  country2_suffix = if (country2 == \"US\") var_list[1:(length(var_list)-1)] else var_list\n\n  country1_vars = paste0(country1, country1_suffix)\n  country2_vars = paste0(country2, country2_suffix)\n\n  A_0_mat = apply(posterior_samples_A_MA, 1:2, mean)\n  dimnames(A_0_mat) &lt;- list(c(\"constant\", rep(colnames(data), p)), colnames(data))\n  A_0_df &lt;- setNames(as.data.frame(A_0_mat), colnames(A_0_mat))\n\n  # Set specified entries to zero based on countries\n  A_0_df[country2_vars, country1_vars] &lt;- 0\n\n  A_0_mat &lt;- as.matrix(A_0_df)\n  \n  ratio_list &lt;- vector(\"list\", length = S)\n  total_S = dim(posterior_samples_Omega_MA)[3]\n  for (s in 1:S) {\n    posterior_V = solve(t(X) %*% posterior_samples_Omega_MA[,,(total_S - s +1)] %*% X + solve(V_prior) + diag(lambda, ncol(X)))\n    prior_Sigma = MCMCpack::riwish(v = nu_prior, S = S_prior)\n    M = c(A_prior)\n    Q = kronecker(prior_Sigma, V_prior)\n    A_0 = c(A_0_mat)\n    prior_prob = mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)\n    \n    M = c(posterior_samples_A_MA[,,(total_S - s +1)])\n    Q = kronecker(posterior_samples_Sigma_MA[,,(total_S - s +1)], posterior_V)\n    posterior_prob = mvtnorm::dmvnorm(x = A_0, mean = M, sigma = Q, log = TRUE)\n    ratio_list[[(total_S - s +1)]] = posterior_prob - prior_prob\n  }\n  return(ratio_list)\n}\nbf_ma = list()\ni = 1\nS = 500\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 728.445000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n             CN           US           JP           AU\nCN -416741.7546     764.5480     290.2978     777.6725\nUS     597.3636 -440938.1889    -221.1377     774.1116\nJP     592.5318     702.9991 -466473.5633     712.0873\nAU     680.3600     762.6540     185.4056 -282903.5990\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence\n\n\n\n\n\nShow code\np = 5\ndata = final_df[(final_df$Time &gt;= '2007 Q1') & (final_df$Time &lt;= '2020 Q1'), 2:ncol(final_df)]\n#data = final_df[,2:ncol(final_df)]\nY = (data[(p+1):nrow(data),])\nX = matrix(1,nrow(Y),1) \nfor (i in 1:p){\n  X     = cbind(X, (data[(p+1):nrow(data)-i,]))\n}\nX = as.matrix(X)\nN = ncol(data)\nK = 1+p*N\nkappa1 = 0.2^2\nkappa2 = 10^2\nV_prior = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))\nlambda = 1e-4\nS = 500\nS1 = 5000\nS2 = 5000\nfstart = system.time({\n  posterior_samples_MA = BVAR_MA_fit(data = data, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, N)\n})\n\n\n[1] \"acceptance rate of the Metropolis Hasting Step: \"\n[1] 0.4181\n\n\nShow code\nprint(sprintf(\"Fit MA BVAR on data for %d iterations took: %f seconds\", (S1+S2), start[\"elapsed\"])) \n\n\n[1] \"Fit MA BVAR on data for 10000 iterations took: 728.445000 seconds\"\n\n\nShow code\nposterior_samples_A_MA = posterior_samples_MA[[1]]\nposterior_samples_Sigma_MA = posterior_samples_MA[[2]]\nposterior_samples_Omega_MA = posterior_samples_MA[[3]]\n\ncountries_list = c('CN', 'US', \"JP\", \"AU\")\nbf_ma = list()\ni = 1\nS = 500\nbayesfactor_MA = matrix(data = NA, nrow = length(countries_list), ncol = length(countries_list), dimnames = list(countries_list, countries_list))\npb &lt;- progress_bar$new(total = length(countries_list))\nstart = system.time({\n  for (country_a in countries_list){\n    pb$tick()\n    for (country_b in countries_list){\n      i = i+1\n      bayesfactor_MA[country_a, country_b] = mean(unlist(calc_bayes_factor_MA(country_a, country_b,posterior_samples_A_MA, posterior_samples_Omega_MA, v_prior, S_prior, A_prior, nu_prior, S)))\n      gc()\n    }\n  }\n})\nprint(sprintf(\"Running Bayes factor calculation for %d iterations took: %f seconds\", S, start[\"elapsed\"])) \n\n\n[1] \"Running Bayes factor calculation for 500 iterations took: 724.450000 seconds\"\n\n\nShow code\nprint(bayesfactor_MA)\n\n\n             CN           US            JP           AU\nCN -344028.8831     445.3087      42.36911     455.2980\nUS     433.8611 -339822.2064    -973.22651     448.3851\nJP     370.0285     420.4815 -378577.96418     444.7938\nAU     472.2898     467.4190    -601.59551 -221156.6646\n\n\n\n\n\n\n\n\n\n\n\n\n\nChina\nUnited States\nJapan\nAustralia\n\n\n\n\nChina\nStrong Evidence\nNo Evidence\nNo Evidence\nNo Evidence\n\n\nUS\nNo Evidence\nStrong Evidence\nNo Evidence\nNo Evidence\n\n\nJapan\nNo Evidence\nNo Evidence\nStrong Evidence\nNo Evidence\n\n\nAustralia\nNo Evidence\nNo Evidence\nNo Evidence\nStrong Evidence"
  },
  {
    "objectID": "index.html#large-bvar-model-with-common-stochastic-volatility",
    "href": "index.html#large-bvar-model-with-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.3 Large BVAR model with Common Stochastic Volatility",
    "text": "4.3 Large BVAR model with Common Stochastic Volatility\n\n4.3.1 Model Building Code and Validation"
  },
  {
    "objectID": "index.html#large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "href": "index.html#large-bvar-model-with-ma1-gaussian-innovations-and-common-stochastic-volatility",
    "title": "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies",
    "section": "4.4 Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility",
    "text": "4.4 Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility\n\n4.4.1 Model Building Code and Validation\n\n\n4.4.2 Empirical Result\n```"
  }
]