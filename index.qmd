---
title: "Global Echoes: Analyzing the Interplay of Economic Indicators Across Leading Economies"
author: "Rui Liu"

execute:
  echo: false
  
bibliography: references.bib
---

# 1. The Question Objective, and Motivation
**Objective:**
Construct a Bayesian Vector Autoregression model to forecast major macroeconomic indicators for the United States, Australia, Japan, and China to facilitate an investigation into the prospective interdependencies between the economies of these nations.

**Question:**
This research project will examine how trade relationships, investment flows, monetary policy environments, and economic performances within the United States, Australia, Japan and China mutually influence each other, and assess the implications of these interactions for predicting future values of these economic indicators.

**Motivation:**
Since the onset of the COVID-19 pandemic, the global economic landscape has witnessed a
series of unprecedented shifts in key macroeconomic indicators, spurred by governments'
adoption of varied expansionary monetary policies. Initially, to buffer their economies,
many nations implemented expansive monetary strategies, later swiftly transitioning to
interest rate hikes in a bid to manage surging inflation rates—a scenario not seen in
decades. The pandemic's disruption to trade further exacerbated inflationary pressures for
some economies, highlighting the intricate interdependencies among major economies
with significant trade and financial ties. This period recorded stark contrast in inflation
levels, with unprecedented highs in the US and Australia and notably low inflation in China
and Japan. Amidst this turmoil, a divergence in economic paths also became apparent, the
United States and Australia have witness robust economic rebounds, whereas China and
Japan saw more tepid recoveries. This research aims to dissect the nuanced web of
economic interdependencies between the United States, Australia, Japan, and China,
analyzing how their trade relationships, investment flows, and monetary policy
environments have mutually influenced their economic performances. Additionally, it
seeks to understand the ramifications of these dynamics for the predictive accuracy of
future economic indicators, offering insights into the evolving global economic order.

# 2. Data and their properties

**Proposed Dataset:**
This research project will utilize data from the International Monetary Fund’s (IMF) extensive database, which offers a comprehensive collection of global economic information. The IMF’s collection includes several key databases such as the World Economic Outlook Databases, International Financial Statistics (IFS), Government Finance Statistics. The analysis will predominantly focus on the IFS database, which encompasses an sizeable collection of financial and economic data from across the global, featuring 1,681 distinct indicators such as consumer price index, interest rates, exchange rates, national accounts, government finance statistics. The data is available in various frequencies – annual, semi-annual, quarterly, monthly, daily, and weekly. As this research is primarily
focused on analyzing macroeconomic data that are published on a monthly or quarterly basis, quarterly data from Q1 2011 to Q3 2023 will be used. The analysis will examine key macroeconomic variables including consumer price indexes, foreign direct investments, exchange rates, balance of payments, and the national gross domestic product of the United States, Australia, Japan, and China.

**Variables and Motivation:**

| Variables                                              | Original Unit          | Final Unit | Mnemonic | Code           |
|--------------------------------------------------------|------------------------|------------|----------|----------------|
| Prices, Consumer Price Index, All items, Previous period | % Change               | % Change   | CPI      | PCPI_PC_PP_PT  |
| International Investment Positions, assets, Direct Investment | US dollar, millions           | % Change   | FDI      | IAD_BP6_USD    |
| Exchange Rates, Domestic Currency per U.S. Dollar, Period Average | US dollar, millions           | % Change   | XCH      | ENDA_XDC_USD_R |
| Balance of Payments, Current Account, Goods and Services, Net | US dollar, millions          | % Change   | BOP      | BGS_BP6_USD    |
| Gross Domestic Product, Nominal, Unadjusted, Domestic Currency | Domestic Currency    | % Change   | GDP      | NGDP_NSA_XDC   |

The variables included in this study were chosen with the objective to include key economic indicators that are susceptible to changes in other nations while also having experienced significant fluctuations over the past decade. These variables were chosen not only for their ability to provide insights into the trade relations, investment dynamics and monetary policy frameworks, but also for their roles as barometers of overall economic health and performance. Foreign direct investment (FDI) is a direct indicator of cross- border investment flows and serves as a proxy for economic confidence and integration between nations. Exchange rates directly impact trade balances and investment flows, influencing economic performances. By examining the volatility and trends in exchange rates, insights can be gleaned into how monetary policies and economic conditions in one country can affect its trade partners. Balance of payments is a comprehensive measure that captures the transactions between a country and the rest of the world, offering a holistic view of its economic interactions. For example, some view changes in balance of payments as largely a result of imports and exports, which can cause one country to import the inflation of another country and vice versa. GDP growth is the ultimate measure of economic performance, encapsulating the outcome of various economic activities and policies. Analyzing GDP in the form of percentage change allows for assessing economic momentum and comparing growth rates across countries and over time, offering a clear picture of economic health and trends.
Foreign direct investment, exchange rates, balance of payments and gross domestic products were transformed into percentage change from the previous period, aiming to standardize the data, facilitate temporal comparisons and enhancing the interpretability of trends over time. The presence of cyclical trends in the variables, alongside the observed impact of lagged values on future outcomes, highlights the suitability of the Bayesian Vector Autoregression model for our analysis. This model can well capture the temporal dynamics and interdependencies inherent in these economic indicators, offering a robust framework for understanding the nuanced interactions and feedback loops that characterize their behavior over time.

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
library(IMFData)
library(patchwork)
library(zoo)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tseries)
library(knitr)
library(MASS)
library(ggplot2)
library(reshape2)
library(progress)
library(psych)
library(truncnorm)
library(extraDistr)
```

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
#################################################################################
#################################################################################
#Download Data 
#################################################################################
#################################################################################

#devtools::install_github('mingjerli/IMFData')
# https://github.com/mingjerli/IMFData
#find out available dataset in IMF data
availableDB <- DataflowMethod()
#availableDB$DatabaseID

#Findout how many dimensions are available in a given dataset. 
# Available dimension code
IFS.available.codes <- DataStructureMethod("IFS")
#names(IFS.available.codes)
# Possible code in the first dimension
#IFS.available.codes[[1]]
IFS.data.category <-  IFS.available.codes[[3]]
#Search possible code to use in each dimension. 
#Here, we want to search code related to GDP in CL_INDICATOR_IFS dimension
CodeSearch(IFS.available.codes, "CL_INDICATOR_IFS", "CPI")

#Make API call to get data
databaseID <- "IFS"
startdate = "2011-01-01"
enddate = "2023-09-01"
checkquery = FALSE

#International Investment Positions Data
#China
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'IAD_BP6_USD')
CN.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.FDI.query$Obs[[1]]
#Japan
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'IAD_BP6_USD')
JP.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.FDI.query$Obs[[1]]
#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'IAD_BP6_USD')
US.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.FDI.query$Obs[[1]]

#Australia
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'IAD_BP6_USD')
AU.FDI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.FDI.query$Obs[[1]]



#Exchange Rate
#China
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
CN.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.XCH.query$Obs[[1]]
#Japan
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
JP.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.XCH.query$Obs[[1]]
#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
US.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.XCH.query$Obs[[1]]

#Australia
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'ENDA_XDC_USD_RATE')
AU.XCH.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.XCH.query$Obs[[1]]

########################################################################
#Pause for 5 seconds before continue execution 
########################################################################
Sys.sleep(5)

#Balance of Payments 
#China
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'BGS_BP6_USD')
CN.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.BOP.query$Obs[[1]]
#Japan
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'BGS_BP6_USD')
JP.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.BOP.query$Obs[[1]]
#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'BGS_BP6_USD')
US.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.BOP.query$Obs[[1]]

#Australia
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'BGS_BP6_USD')
AU.BOP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.BOP.query$Obs[[1]]

#Bond Yield
#China
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'FITBBE_PA')
CN.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                 checkquery)
#CN.IR.query$Obs[[1]]
#Japan
########################################################################
#Pause for 5 seconds before continue execution 
########################################################################
Sys.sleep(5)

queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'FIGB_PA')
JP.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                 checkquery)
#JP.IR.query$Obs[[1]]
#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'FIGB_PA')
US.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                 checkquery)
#US.IR.query$Obs[[1]]

#Australia
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'FIGB_PA')
AU.IR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                 checkquery)
#AU.IR.query$Obs[[1]]

#Government Expenditure
#China
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
CN.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.GDP.query$Obs[[1]]
#Japan
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
JP.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.GDP.query$Obs[[1]]

########################################################################
#Pause for five seconds before continue execution 
########################################################################
Sys.sleep(5)

#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
US.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.GDP.query$Obs[[1]]

#Australia
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'NGDP_NSA_XDC')
AU.GDP.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.GDP.query$Obs[[1]]


#CPI
#China
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
CN.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.CPI.query$Obs[[1]]
#Japan
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
JP.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.CPI.query$Obs[[1]]
#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
US.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.CPI.query$Obs[[1]]

#Australia
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'PCPI_PC_PP_PT')
AU.CPI.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.CPI.query$Obs[[1]]


#Unemployment Rate
#China
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "CN", CL_INDICATOR_IFS = 'LUR_PT')
CN.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#CN.UMR.query$Obs[[1]]
#Japan
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "JP", CL_INDICATOR_IFS = 'LUR_PT')
JP.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#JP.UMR.query$Obs[[1]]
#United States
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "US", CL_INDICATOR_IFS = 'LUR_PT')
US.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#US.UMR.query$Obs[[1]]

#Australia
queryfilter <- list(CL_FREA = "Q", CL_AREA_IFS = "AU", CL_INDICATOR_IFS = 'LUR_PT')
AU.UMR.query <- CompactDataMethod(databaseID, queryfilter, startdate, enddate, 
                                  checkquery)
#AU.UMR.query$Obs[[1]]



#################################################################################
#################################################################################
#Plot Data
#################################################################################
#################################################################################



CPI_data <- data.frame(CN.CPI.query$Obs[[1]]$`@TIME_PERIOD`, CN.CPI.query$Obs[[1]]$`@OBS_VALUE`, JP.CPI.query$Obs[[1]]$`@OBS_VALUE`, 
                       US.CPI.query$Obs[[1]]$`@OBS_VALUE`, AU.CPI.query$Obs[[1]]$`@OBS_VALUE`)
names(CPI_data) = c('Time', 'China', 'Japan', 'United_States', 'Australia')
CPI_data$Time = as.yearqtr(CPI_data$Time, format = "%Y-Q%q")

CPI_data$Time = as.Date(CPI_data$Time)
CPI_data$China = as.numeric(CPI_data$China)
CPI_data$United_States = as.numeric(CPI_data$United_States)
CPI_data$Japan = as.numeric(CPI_data$Japan)
CPI_data$Australia = as.numeric(CPI_data$Australia)
CPI_data_long <- pivot_longer(CPI_data, 
                              cols = c(China, Japan, United_States, Australia), 
                              names_to = "Country", 
                              values_to = "CPI")

CPI_plot = ggplot(data = CPI_data_long, aes(x = Time, y = CPI, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "United_States" = "green", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "CPI (%)", title = "CPI Trends", color = "Country")

#ggsave("CPI.png", plot = CPI_plot, path = paste0(getwd(), "/plots", sep = ""), width = 10, height = 7)

FDI_data <- data.frame(CN.FDI.query$Obs[[1]]$`@TIME_PERIOD`, CN.FDI.query$Obs[[1]]$`@OBS_VALUE`, JP.FDI.query$Obs[[1]]$`@OBS_VALUE`, 
                       US.FDI.query$Obs[[1]]$`@OBS_VALUE`, AU.FDI.query$Obs[[1]]$`@OBS_VALUE`)
names(FDI_data) = c('Time', 'China', 'Japan', 'United_States', 'Australia')
FDI_data$Time = as.yearqtr(FDI_data$Time, format = "%Y-Q%q")

FDI_data$Time = as.Date(FDI_data$Time)
FDI_data$China = as.numeric(FDI_data$China)
FDI_data$United_States = as.numeric(FDI_data$United_States)
FDI_data$Japan = as.numeric(FDI_data$Japan)
FDI_data$Australia = as.numeric(FDI_data$Australia)
FDI_original = FDI_data
FDI_data <- FDI_data %>%
  mutate(across(c(China, Japan, United_States, Australia), 
                ~ (./lag(.) - 1) * 100, 
                .names = "{.col}"))
FDI_original_long <- pivot_longer(FDI_original, 
                              cols = c(China, Japan, United_States, Australia), 
                              names_to = "Country", 
                              values_to = "FDI")

FDI_data_long <- pivot_longer(FDI_data, 
                              cols = c(China, Japan, United_States, Australia), 
                              names_to = "Country", 
                              values_to = "FDI")

FDI_original_plot = ggplot(data = FDI_data_long, aes(x = Time, y = FDI, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "United_States" = "green", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "FDI (US dollar, millions)", title = "Foreign Direct Investments", color = "Country")

FDI_plot = ggplot(data = FDI_data_long, aes(x = Time, y = FDI, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "United_States" = "green", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "FDI (%change)", title = "Percentage change in Foreign Direct Investments", color = "Country")

#ggsave("FDI.png", plot = FDI_plot, path = paste0(getwd(), "/plots", sep = ""), width = 10, height = 7)

XCH_data <- data.frame(CN.XCH.query$Obs[[1]]$`@TIME_PERIOD`, CN.XCH.query$Obs[[1]]$`@OBS_VALUE`, JP.XCH.query$Obs[[1]]$`@OBS_VALUE`, 
                       US.XCH.query$Obs[[1]]$`@OBS_VALUE`, AU.XCH.query$Obs[[1]]$`@OBS_VALUE`)
names(XCH_data) = c('Time', 'China', 'Japan', 'United_States', 'Australia')
XCH_data$Time = as.yearqtr(XCH_data$Time, format = "%Y-Q%q")

XCH_data$Time = as.Date(XCH_data$Time)
XCH_data$China = as.numeric(XCH_data$China)
XCH_data$United_States = as.numeric(XCH_data$United_States)
XCH_data$Japan = as.numeric(XCH_data$Japan)
XCH_data$Australia = as.numeric(XCH_data$Australia)
XCH_data = subset(XCH_data, select = -United_States)
XCH_original = XCH_data
XCH_data <- XCH_data %>%
  mutate(across(c(China, Japan, Australia), 
                ~ (./lag(.) - 1) * 100, 
                .names = "{.col}"))

XCH_original$Japan = XCH_original$Japan/100
XCH_original$China= XCH_original$China/10

XCH_original_long <- pivot_longer(XCH_original, 
                              cols = c(China, Japan, Australia), 
                              names_to = "Country", 
                              values_to = "XCH")

XCH_original_long$Country_Label <- with(XCH_original_long, ifelse(Country == "China", "China (Yuan '0s)",
                                               ifelse(Country == "Japan", "Japan (Yen '00s)",
                                               ifelse(Country == "Australia", "Australia (AUD)", Country))))
XCH_data_long <- pivot_longer(XCH_data, 
                              cols = c(China, Japan, Australia), 
                              names_to = "Country", 
                              values_to = "XCH")

#XCH_original_long <- XCH_original_long %>% 
#  filter(Country != "United_States")

XCH_original_plot <- ggplot(data = XCH_original_long, aes(x = Time, y = XCH, color = Country_Label)) +
  geom_line() +
  scale_color_manual(values = c("China (Yuan '0s)" = "blue", "Japan (Yen '00s)" = "red", "Australia (AUD)" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "Exchange Rate (Currency per US dollar)", title = "Exchange Rates Against the Dollar", color = "Country")

#XCH_data_long <- XCH_data_long %>% 
#  filter(Country != "United_States")

XCH_plot = ggplot(data = XCH_data_long, aes(x = Time, y = XCH, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "Exchange Rate (%change)", title = "Percentage change in Exchange Rates Against the Dollar", color = "Country")

#ggsave("XCH.png", plot = XCH_plot, path = paste0(getwd(), "/plots", sep = ""), width = 10, height = 7)

BOP_data <- data.frame(CN.BOP.query$Obs[[1]]$`@TIME_PERIOD`, CN.BOP.query$Obs[[1]]$`@OBS_VALUE`, JP.BOP.query$Obs[[1]]$`@OBS_VALUE`, 
                       US.BOP.query$Obs[[1]]$`@OBS_VALUE`, AU.BOP.query$Obs[[1]]$`@OBS_VALUE`)
names(BOP_data) = c('Time', 'China', 'Japan', 'United_States', 'Australia')
BOP_data$Time = as.yearqtr(BOP_data$Time, format = "%Y-Q%q")

BOP_data$Time = as.Date(BOP_data$Time)
BOP_data$China = as.numeric(BOP_data$China)
BOP_data$United_States = as.numeric(BOP_data$United_States)
BOP_data$Japan = as.numeric(BOP_data$Japan)
BOP_data$Australia = as.numeric(BOP_data$Australia)
BOP_original = BOP_data
BOP_data <- BOP_data %>%
  mutate(across(c(China, Japan, United_States, Australia), 
                ~ (./lag(.) - 1) * 100, 
                .names = "{.col}"))

BOP_original_long <- pivot_longer(BOP_original, 
                              cols = c(China, Japan, United_States, Australia), 
                              names_to = "Country", 
                              values_to = "BOP")

BOP_data_long <- pivot_longer(BOP_data, 
                              cols = c(China, Japan, United_States, Australia), 
                              names_to = "Country", 
                              values_to = "BOP")

BOP_original_plot = ggplot(data = BOP_original_long, aes(x = Time, y = BOP, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "United_States" = "green", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "BOP  (US dollar, millions)", title = "Balance of Payments", color = "Country")


BOP_plot = ggplot(data = BOP_data_long, aes(x = Time, y = BOP, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "United_States" = "green", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "BOP (%change)", title = "Percentage change in Balance of Payments", color = "Country")
#ggsave("BOP.png", plot = BOP_plot, path = paste0(getwd(), "/plots", sep = ""), width = 10, height = 7)

GDP_data <- data.frame(CN.GDP.query$Obs[[1]]$`@TIME_PERIOD`, CN.GDP.query$Obs[[1]]$`@OBS_VALUE`, JP.GDP.query$Obs[[1]]$`@OBS_VALUE`, 
                       US.GDP.query$Obs[[1]]$`@OBS_VALUE`, AU.GDP.query$Obs[[1]]$`@OBS_VALUE`)
names(GDP_data) = c('Time', 'China', 'Japan', 'United_States', 'Australia')
GDP_data$Time = as.yearqtr(GDP_data$Time, format = "%Y-Q%q")

GDP_data$Time = as.Date(GDP_data$Time)
GDP_data$China = as.numeric(GDP_data$China)
GDP_data$United_States = as.numeric(GDP_data$United_States)
GDP_data$Japan = as.numeric(GDP_data$Japan)
GDP_data$Australia = as.numeric(GDP_data$Australia)

#Convert to US dollars and in billion dollars
GDP_original = GDP_data
GDP_original$China = (GDP_original$China / XCH_original$China)/10000
GDP_original$Japan = (GDP_original$Japan / XCH_original$Japan)/100000
GDP_original$Australia = (GDP_original$Australia / XCH_original$Australia)/1000
GDP_original$United_States = (GDP_original$United_States)/1000

GDP_data_percent <- GDP_data %>%
  mutate(across(c(China, Japan, United_States, Australia), 
                ~ (./lag(.) - 1) * 100, 
                .names = "{.col}"))

GDP_original_long <- pivot_longer(GDP_original, 
                              cols = c(China, Japan, United_States, Australia), 
                              names_to = "Country", 
                              values_to = "GDP")

GDP_data_long <- pivot_longer(GDP_data_percent, 
                              cols = c(China, Japan, United_States, Australia), 
                              names_to = "Country", 
                              values_to = "GDP")

GDP_original_plot = ggplot(data = GDP_original_long, aes(x = Time, y = GDP, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "United_States" = "green", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "GDP (US dollar, billions)", title = "GDP", color = "Country")


GDP_plot = ggplot(data = GDP_data_long, aes(x = Time, y = GDP, color = Country)) +
  geom_line() +
  scale_color_manual(values = c("China" = "blue", "Japan" = "red", "United_States" = "green", "Australia" = "orange")) +
  theme_minimal() +
  labs(x = "Time", y = "GDP (%change)", title = "Percentage change in GDP", color = "Country")
#ggsave("GDP.png", plot = GDP_plot, path = paste0(getwd(), "/plots", sep = ""), width = 10, height = 7)

blank_plot <- ggplot() + 
  theme_void() + 
  theme(plot.background = element_blank(), panel.grid = element_blank(), panel.border = element_blank())
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
colnames(CPI_data) = c('Time', 'CPI_CN', 'CPI_JP', 'CPI_US', 'CPI_AU')
colnames(FDI_original) = c('Time', 'FDI_CN', 'FDI_JP', 'FDI_US', 'FDI_AU')
data <- inner_join(CPI_data, FDI_original, by = "Time")


```
## Data Plots 
**Original Variables** 
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
layout <- (FDI_original_plot | BOP_original_plot) / (XCH_original_plot | GDP_original_plot)
layout
```
**Transformed Variables**
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
layout <- (CPI_plot | FDI_plot) / (XCH_plot | BOP_plot) / (GDP_plot | blank_plot)
layout 
```

## Stationarity Check 

**Stationary Tests** 

The Augmented Dickey-Fuller Test is used in this section to test the null hypothesis that a unit root is present in the time series and the time series is non-stationary. 


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
#install.packages("tseries")

perform_adf_test <- function(series) {
  adf_test <- adf.test(series, alternative = "stationary", k = 4)
  list(
    statistic = adf_test$statistic,
    lag.order = as.numeric(adf_test$parameter),
    p.value = adf_test$p.value
  )
}

results <- sapply(CPI_data[,2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for CPI Data by Country")
```
The lag order chosen in the ADF test is 4, which is appropriate given our data is of quarterly frequency. The result of the ADF test on the CPI data shows that we do not have enough evidence to reject the null hypothesis that the CPI series is unit root non-stationary at 1% significance level. 
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
results <- sapply(FDI_data[2:nrow(FDI_data),2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for Foreign Direct Investment (% change) Data by Country")
```
The ADF results shows that we have enough evidence to reject the null hypothesis that the foreign direct investment (% change) data is not unit-root stationary for Australia, the United States and Japan at 1% significance level and we do have enough evidence to reject the null hypothesis for China at 1% significance level. 





```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
results <- sapply(XCH_data[2:nrow(XCH_data),2:4], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for Exchange Rate (% change) Data by Country")
```

The result of the ADF test on the exchange rate data shows that we do not have enough evidence to reject the null hypothesis that the exchange rate (%change) against the dollar series is unit root non-stationary at 1% significance level. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}

results <- sapply(BOP_data[2:nrow(BOP_data),2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for Balance of Payments (% change) Data by Country")
```
The result of the ADF test on the balance of payments data shows that we do not have enough evidence to reject the null hypothesis that the balance of payments (%change) series is unit root non-stationary at 5% significance level for all countries except for China. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}

results <- sapply(GDP_data_percent[2:nrow(GDP_data_percent),2:5], perform_adf_test)
results_df <- as.data.frame(t(results), stringsAsFactors = FALSE)
names(results_df) <- c("Dickey-Fuller Statistic", "Lag Order", "P-value")
results_df$Country <- rownames(results_df)
rownames(results_df) <- NULL
kable(results_df, caption = "ADF Test Results for GDP (% change) Data by Country")
```

The result of the ADF test on the GDP data shows that we do not have enough evidence to reject the null hypothesis that the GDP (%change) series is unit root non-stationary at 5% significance level.


The results from the ADF tests indicates that further transformations are needed to achieve stationarity for most of the variables included in this research. 


# 3. The Model and Hypothesis

We will employ four models to address our proposed problem. Firstly, we will use a standard Bayesian Vector Autoregressive (BVAR) model with independently and identically distributed innovations, as outlined in details in @wozniakBsvarsBayesianEstimation2022. Additionally, we will investigate a large BVAR model with flexible error covariance structures, following the methodology proposed by @Chan_2015. Specifically, the second model will incorporate MA(1) Gaussian innovations to better account for potential model misspecifications such as omitted variable bias and to facilitate shrinkage in VAR coefficients. Our third model will be a BVAR incorporating common stochastic volatilities to allow for time varying distribution of volatility terms, which will be useful to handle the impact of specific events like the Covid-19 Global Pandemic. The final model will combined a common stochastic volatility frame work with MA(1) Gaussian innovations, offering a robust approach to volatility modelling. 

## 3.1 Model 1: Standard BVAR(p) Model

### 3.1.1 Model Specification
$$ Y = XA+E$$
$$E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, I_T)$$
$$
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
$$

where 

- $T$ is the number of time periods under consideration
- $N$ is the number of variables, in our case, N = 20
- $P$ is the number of lags
- $Y$ is a $T \times N$ matrix of variables of response variables we aim to model.
- $A$ is a $K \times N$ matrix of coefficients, $K = (1+𝑝\times N)$.
- $E$ is a $T \times N$ matrix of the error terms
- $X$ is a $T \times (1+𝑝\times N)$ matrix of covariates
- $\Sigma$ is a $N \times N$ matrix representing the row-specific covariance matrix
- $I_T$ is a $T \times T$ identity matrix representing the column specific covariance matrix
- $E|X$ follows a matrix-variate normal distribution with mean $0_{T \times N}$, row specific covariance matrix $\Sigma_{N \times N}$ and column specific covariance matrix $I_T$




The Bayesian Vector Autoregression model as formulated above provides a robust framework for investigating the relationships among selected economic indicators across different nations. By employing this model, this research aims to quantitatively measure the influence of one country’s economic indicators on another, such as how lagged changes in China’s inflation rate, may influence the GDP growth rate of the United States and vice versa. The BVAR model, with its estimation of coefficients across various lags, offers a deep understanding of both immediate and more delayed economic interactions, which is crucial to analyzing the cyclical nature of trade relationships, investment flows, monetary policy environments, and economic performances and the transmission of these metrics across borders.

The strength of this BVAR model lies in its ability to incorporate prior economic knowledge and beliefs into the estimation process. By setting prior distributions for the matrix of coefficients A and the covariance matrix $\Sigma$, the model can be tailors to reflect established economic theories regarding international economic linkages and the time it takes for policy changes in one country to affect another. By calibrating the prior variances, particularly for the autoregressive coefficients, we can integrate prior knowledge or hypotheses, such as the presence of unit roots or the diminishing influence of distant lags on current values, into the analysis. When interpreting the estimation output, attention will be given to the posterior means and variances of the coefficients, which represent the model’s “learnt” understanding of the underlying economic structure. The analysis will be supplemented by forecast error variance decompositions to better understand the proportion of the movements in economic indicators that can be accounted for by their own shocks versus shocks to other variables.

The economic context underscoring this analysis is the increased globalization over the past decade, marking an era where economies are more intertwined than ever through trade, capital flows, and policy decisions. This period has witnessed not only the strengthening of global economic ties but also recent calls from political leaders advocating for a reduction in globalization. These contrasting dynamics highlight the complexity of the current global economic landscape, where the push for deeper integration coexists with growing sentiments for retrenchment. This dual trend sets the stage for our investigation, providing a rich context to explore how economic variables across nations influence each other amidst fluctuating levels of global interconnectedness. In this environment, understanding the cross-country spillover effects is vital for policymakers and businesses alike, as decisions made in one country can have far-reaching implications. By addressing these aspects, this research will contribute to the discourse on economic policy formulation, risk assessment, and strategic planning. 


### 3.1.2 Prior Settings 
We will employ a Normal-Inverse Wishart distribution for the joint distribution of coefficient matrices A and the row-specific variance matrix $\Sigma$, and a Minnesota prior on the coefficients A. Specifically, we have:

$$\Sigma \sim \mathcal{IW}(S_0, \nu_0) $$


$$p(\Sigma) \propto |\Sigma|^{-\frac{\nu_0+N+1}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_0))$$

$$A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)$$

$$p(A|\Sigma) \propto |\Sigma|^{-\frac{K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^T(A-A_0)))$$

$$(A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)$$

$$p(A,\Sigma) \propto |\Sigma|^{-\frac{K+\nu_0+N+1}{2}} \times exp(-\frac{1}{2}tr(\Sigma^{-1}S_0))\times exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^T(A-A_0)))$$

where 

$$V_A = diag(\kappa_2 \quad \kappa_1(\mathbf{p} \otimes I_N^T))$$

- $$\mathbf{p} = [1 \quad 2 \quad ... \quad p]$$
$$I_N = [1 \quad 1 \quad ... \quad 1] \in \mathbb{R}^N$$
- $$\kappa_1 \text{ is the overall shrinkage level for autoregressive slopes}$$ 
- $$\kappa_2 \text{ is the overall shrinkage lvel for the constant term }$$ 


Additionally, we adopt commonly used values for the hyperparameters as established in the literature.

$$A_0 = 0$$

$$v_0 = N+3$$

$$S_0 = I_N$$

$$\kappa_1 = 0.2^2 \quad \kappa_2 = 10^2$$

The hyperparameters $\kappa_1$ and $\kappa_2$ are specified in a way such that the coefficient associated with a lag l variable is shrunk more heavily to - as lag length increases whereas the intercepts are not shrunk to 0. 


### 3.1.3 Posterior Distributions

The posterior distribution specified above has the form 

$$p(Y|A, \Sigma) = (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(Y-XA))$$

and the joint posterior distribution

$$p(A, \Sigma \mid Y)  = \frac{p(A,\Sigma,Y)}{p(Y)}\propto p(A, \Sigma, Y) \propto p(Y|A,\Sigma)\times p(A,\Sigma) =  p(Y|A,\Sigma) p(A \mid \Sigma) p(\Sigma)$$


$$\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(Y-XA))) \times$$

$$\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))$$


$$\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times $$

$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^TY-\bar{A}^T\bar{V}^{-1}\bar{A})\times$$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))$$
Hence, 

$$p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)$$


$$p(A \mid Y, X, \Sigma) = \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$$

$$p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$$

where

$$\bar{V} = (X^TX + V_A^{-1})^{-1}$$



$$\bar{A} = \bar{V}(X^TY + V_A^{-1}A_0)$$

$$\bar{\nu} = T + \nu_0$$

$$\bar{S} = S_0 + Y^TY + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}$$


### 3.1.4 Estimation Procedure 

In this setting, we have 

- $(A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)$

then, prior draws can be sampled from 

- $\Sigma \sim \mathcal{IW}(S_0, \nu_0)$
- $A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)$


we use the following Gibb's Sampler algorithm to sample from the posterior distribution: 

- Initialize $\Sigma$ at $\Sigma^0$ 
- For $s = 1,...,S_1+S_2$
    1. Draw a sample $A^{(s)}$ from $p(A \mid Y, X, \Sigma^{(s-1)}) \sim \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$
    2. Draw a sample $\Sigma^{(s)}$ from $p(\Sigma \mid Y, X, A^{(s)}) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$
    
We discard the first $S_1$ sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain $S_2$ sampled draws from the joint posterior distribution. 

$$\left\{A^{(s)}, \Sigma^{(s)}\right\}_{s=S_1+1}^{S_1+S_2}$$

The draws from joint predictive density can then be obtained using the following algorithm: 

1. Sample S draws $\left\{ A^{(s)}, \Sigma^{(s)} \right\}_{s=1}^{S}$ from $p(A,\Sigma|Y, X)$ 
2. Sample S draws $\left\{ Y_{t+h}^{(s)} \right\}_{s=1}^S$ from $Y_{t+h}^{(s)} \sim \mathcal{N}_{hN}(Y_{t+h|t}(A^{(s)}), \mathbb{V}ar[Y_{t+h|t}]|A^{(s)}, \Sigma^{(s)}]$


## 3.2 Model 2: Large BVAR model with MA(1) Gaussian Innovations 
Incorporating MA(1) Gaussian innovations in a large BVARs model lead to a significant enhancement over traditional BVAR models, especially when forecasting macroeconomic variables for several reasons. Firstly, the variances of economic shocks is rarely constant over time. For example, volatility tends to cluster during periods of economic crisis and is more tranquil during stable times. Incorporating stochastic volatility allows the model to adapt to changing volatility in the data, improving forecasting performance especially in the presence of financial market instability or economic policy shifts. In addition, by allowing for serial correlation in the innovations term, we will be able to capture the momentum or persistence in economic variables that is often observed in real-world data. Recognizing that shocks may have a lasting impact over several periods can enhance the model's ability to predict future values by considering the path-dependent nature of the economy. Our BVAR model with common stochastic volatility is a natural extension to the standard BVAR model and formulated as below: 

3.2.1 Model Specification 

$$ Y = XA+E$$
$$E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, \Omega_{T \times T})$$
$$
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
$$

where 

- $T$ is the number of time periods under consideration
- $N$ is the number of variables, in our case, N = 20
- $P$ is the number of lags
- $Y$ is a $T \times N$ matrix of variables of response variables we aim to model.
- $A$ is a $K \times N$ matrix of coefficients, $K = (1+𝑝\times N)$.
- $E$ is a $T \times N$ matrix of the error terms
- $X$ is a $T \times (1+𝑝\times N)$ matrix of covariates
- $\Sigma$ is a $N \times N$ matrix representing the row-specific covariance matrix
- $I_T$ is a $T \times T$ identity matrix representing the column specific covariance matrix
- $E|X$ follows a matrix-variate normal distribution with mean $0_{T \times N}$, row specific covariance matrix $\Sigma_{N \times N}$ and column specific covariance matrix $I_T$

with MA(1) innovations, we have, for i = 1,....,N and t = 1,...,T, $$u_{it} = \eta_{it} + \psi \eta_{i,t-1}$$ where $|\psi|<1$ and $\eta_{it} \sim \mathcal{N}(0,1)$

In matrix notation, we have: 

$$u_i = H_{\psi} \eta_i$$ 

where 


$$
u_i = \begin{bmatrix}
u_{i1} \\
u_{i2} \\
\vdots \\
u_{iT}
\end{bmatrix} \qquad 
\eta_i = \begin{bmatrix}
\eta_{i1} \\
\eta_{i2} \\
\vdots \\
\eta_{iT}
\end{bmatrix} \qquad 
H_{\psi} = \left(
\begin{array}{cccc}
1 & 0  & \cdots & 0 \\
\psi & 1  & \cdots & 0 \\
\vdots & \ddots & \ddots & \vdots \\
0 & \cdots & \psi & 1
\end{array}
\right)
$$
Hence, we have 

$$u_1 \sim \mathcal{N}(0, H_{\psi}O_{\psi}H_{\psi}^T) \qquad \qquad \Omega = H_{\psi}O_{\psi}H_{\psi}^T$$
Note that the covariance matrix $\Omega$ depends on $\psi$ only. 

### 3.2.2 Prior Specification 

We consider a prior independent distributions for $(A, \Sigma, \Omega)$, specifically, we have: $$P(A, \Sigma, \Omega) = P(A, \Sigma) \times P(\Omega)$$

We will employ a Normal-Inverse Wishart distribution for the joint distribution of A and $\Sigma$ and before, $$(A,\Sigma) \sim \mathcal{NIW}_{K \times N}(A_0, V_A, S_0, \nu_0)$$

We apply a truncated normal prior on $\psi$, $$\psi \sim \mathcal{N}(\psi_0, V_{\psi})$$

For estimation purposes, we initialize with the following setting: 

- $u_{i1} \sim \mathcal{N}(0, 1+\psi^2)$
- $\psi_0 = 0$ and $V_{\psi} = 1$ so that the prior centers around 0 with a relatively large variance and has support within (-1, 1)

### 3.2.3 Posterior Distributions 

1. The posterior distribution of Y specified above has the form: 

$$p(Y|A, \Sigma) = (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}|\Omega|^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA))$$

$$= (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}(1+\psi^2)^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T(H_{\psi}O_{\psi}H_{\psi}^T)^{-1}(Y-XA))$$

2. The joint posterior distribution of A and $\Sigma$ can be derived as follows:

$$p(A, \Sigma \mid Y, \Omega)  = \frac{p(A,\Sigma,Y, \Omega)}{p(Y, \Omega)}$$
$$\propto p(A, \Sigma, Y, \Omega) \propto p(Y \mid A,\Sigma, \Omega)\times p(A,\Sigma, \Omega)$$
$$=  p(Y \mid A,\Sigma, \Omega) p(A, \Sigma) p(\Omega) = p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma) p(\Omega)$$

$$\propto p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma)$$



$$\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA)) \times$$

$$\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))$$


$$\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times $$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\Omega^{-1}Y-\bar{A}^T\bar{V}^{-1}\bar{A})\times$$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))$$

Hence, 

$$p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)$$


$$p(A \mid Y, X, \Sigma, \Omega) = \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$$

$$p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$$

where

$$\bar{V} = (X^T\Omega^{-1}X + V_A^{-1})^{-1}$$



$$\bar{A} = \bar{V}(X^T\Omega^{-1}Y + V_A^{-1}A_0)$$

$$\bar{\nu} = T + \nu_0$$

$$\bar{S} = S_0 + Y^T\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}$$
3. The posterior distribution for the parameter $\psi$ can be obtained as follows: 
$$P(\psi|Y, A, \Sigma) = \frac{P(\psi, Y, A, \Sigma)}{P(Y, A, \Sigma)} \propto P(\psi, Y, A, \Sigma) = P(Y|A, \Sigma, \psi) \times P(A,\Sigma, \psi) $$

$$= P(Y|A, \Sigma, \psi) \times P(A, \Sigma) \times P(\psi) \propto P(Y|A, \Sigma, \psi) \times P(\psi)$$

we can sample from the posterior distribution of $\psi$ using an independence-chain Metropolis-Hastings algorithm. 

### 3.2.4 Estimation Procedure 

We obtain posterior estimates of $A, \Sigma, \psi$ using a Gibbs sampler, specifically, we initialize $\psi^{(0)}$ and for s = 1,...,S1+S2, we sequentially sample: 

- $\Sigma^{(s)} | Y, X, \psi^{(s-1)} \sim \mathcal{IW}_N(\bar{S}, \bar{\nu})$
- $A^{(s)} | Y, X, \Sigma^{(s)}, \psi^{(s-1)} \sim \mathcal{MN}_{k \times N}(\bar{A}, \Sigma^{(s)}, \bar{V})$
- $\psi^{(s)} | Y, X, A^{(s)}, \Sigma^{(s)} \propto P(Y|A^{(s)}, \Sigma^{(s)}, \psi^{s-1})\times p(\psi)$

The Metropolis-Hastings Algorithm for Sampling $\psi$ is given as follows: 



**Initialization:**

- Choose an initial value $\psi^{(0)}$ within the bounds $(-1, 1)$. 

**Proposal Distribution:**

- Select the proposal distribution $q(\psi' | \psi^{(t)}) \sim N(\psi^{(t)}, \tau^2)$. $\tau^2$ is a tuning parameter that controls the step size.
    
**Sampling Loop:**

For t = 1,...,$T_1+T_2$

- Generate a candidate $\psi'^{(t)}$ from $q(\psi' | \psi^{(t-1)})$.

- Check if $\psi'$ is within the bounds $(-1, 1)$. If not, reject $\psi'$ (set $\alpha = 0$).

- Compute the acceptance ratio $\alpha$:
            $$ \alpha(\psi^{(t-1)}, \psi') = \min\left(1, \frac{p(Y | A, \Sigma, \psi') p(\psi') q(\psi^{(t-1)} | \psi')}{p(Y | A, \Sigma, \psi^{(t-1)}) p(\psi^{(t-1)}) q(\psi' | \psi^{(t-1)})}\right)$$

**Decide to accept or reject:**

- Generate a random number $u$ from $U[0,1]$.

- If $u \leq \alpha$, accept $\psi'$ and set $\psi^{(t)} = \psi'$.

- Otherwise, reject $\psi'$ and set $\psi^{(t)} = \psi^{(t-1)}$.
  
**Burn in period**

- Discard the first $T_1$ samples to allow the algorithm to converge to the true distribution 

**Obtain one sample of $\psi$**

- Randomly draw a $\psi^*$ from the $T_2$ draws and set $\psi^{(s)} = \psi^*$

We monitor the acceptance rate and adjust $\tau^2$ as necessary to achieve an optimal rate of about 20-40\%.

We note that since $\Omega$ is a band matrix, we do not need compute $\Omega^{-1}$, instead, we obtain the Cholesky decomposition $C_{\Omega}$ of $\Omega$, which has a time complexity of $O(T)$. Terms involving $\Omega^{-1}$ such as $X^T\Omega^{-1}X$ can be obtained by: $$X^T\Omega^{-1}X = X^T(C_{\Omega}^{-1})^{T}C_{\Omega}^{-1}X=(C_{\Omega}^{-1}X)^T(C_{\Omega}^{-1}X) = \tilde{X}^T\tilde{X}$$

## 3.3 Model 3: Large BVAR model with ommon Stochastic Volatility

## 3.4 Model 4: Large BVAR model with MA(1) Gaussian innovations and Common Stochastic Volatility


### 3.4.1 Model Specification


$$ Y = XA+E$$ 

where

$$
A = \begin{bmatrix}
\mu_0^T \\
A_1^T \\
\vdots \\
A_p^T
\end{bmatrix}
, \quad
Y = \begin{bmatrix}
y_1^T \\
y_2^T \\
\vdots \\
y_T^T
\end{bmatrix}
, \quad
x_t = \begin{bmatrix}
1 \\
y_{t-1}^T \\
\vdots \\
y_{t-p}^T
\end{bmatrix}
, \quad
X = \begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_T^T
\end{bmatrix}
, \quad
E = \begin{bmatrix}
e_1^T \\
e_2^T \\
\vdots \\
e_T^T
\end{bmatrix}
$$
where 

- $T$ is the number of time periods under consideration
- $N$ is the number of variables, in our case, N = 20
- $P$ is the number of lags
- $Y$ is a $T \times N$ matrix of variables of response variables we aim to model.
- $A$ is a $K \times N$ matrix of coefficients, $K = (1+𝑝\times N)$
- $E$ is a $T \times N$ matrix of the error terms
- $X$ is a $T \times (1+𝑝\times N)$ matrix of covariates
- $\Sigma$ is a $N \times N$ matrix representing the row-specific covariance matrix


the same as before. But, instead of the column specific matrix as the identity matrix, we specify the column specific matrix as a diagonal matrix $\Omega$. Specifically, we have: 

$$\epsilon_t = u_t + \psi_1 u_{t-1}$$

$$u_t \sim \mathcal{N}(0,e^{h_t} \Sigma)$$

$$h_t = \rho h_{t-1} + u_t^h \quad \text{ follows an Autoregressive process of lag 1 AR(1), and}$$

$$u_t^h \sim N(0,\sigma_h^2)$$

$$\Omega = \left(
\begin{array}{cccc}
(1 + \psi_1^2) e^{h_1} & \psi_1 e^{h_1} & \cdots & 0 \\
\psi_1 e^{h_1} & \psi_1^2 e^{h_1} + e^{h_2} & \cdots & \vdots \\
0 & \cdots & \ddots & \vdots \\
\vdots & \cdots & \psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \psi_1 e^{h_{T-1}} \\
0 & \cdots & \psi_1 e^{h_{T-1}} & \psi_1^2 e^{h_{T-1}} + e^{h_T}
\end{array}
\right)$$

In this specification, each element of $e_t$ may have distinct variances, and the variances of all innovations can be scaled by a common factor. This approach is economically intuitive, as the volatility of macroeconomic variables often exhibit co-movement. It is important to emphasize that each component of $e_t$ must adhere to the same univariate time series model. 

### 3.4.2 Prior Specification


Here, we consider a priori independent distributions for $(A, \Sigma, \Omega)$, namely: 

$$p(A, \Sigma, \Omega) = p(A, \Sigma) \times p(\Omega)$$
Given this structure, we can sample from the posterior distribution by sequentially sampling from: 

- $P(A, \Sigma | Y, X, \Omega)$
- $P(\Omega | Y, X, A, \Sigma)$

The prior distribution of $(A,\Sigma)$ follow the same normal inverse Wishart prior distribution as outlined in model one. But the variance matrix $V_A$ for A is different: 

$$V_A = diag(v_{A,ii})$$

$$v_{A,ii} = \begin{cases} 
\kappa_1(\frac{l^2}{\hat{s}_r}) & \text{for a coefficient associated to lag l of variable r} \\ 
\kappa_2& \text{for an intercept}
\end{cases}$$

where $\hat{s}_r$ is the sample variance of an AR(4) model for the variable r. 


$$E|X \sim \mathcal{MN}_{T \times N}(0_{T \times N},\Sigma_{N \times N}, \Omega_{T \times T})$$

and, 


For the moving average coefficients, we adopt an uninformative truncated normal prior for the MA coefficient $\psi$:

$$
\psi \sim \mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|<1)
$$

and we set $\psi_0 = 0$ and $V_\psi = 1$ so that the prior centers around 0 with a relatively large variance and has support within (-1,1). Further more, we assume independent priors for $\sigma^2_h$ and $\rho$:

$$\sigma_h^2 \sim \mathcal{IG}(\nu_{h_0}, s_{h_0})$$

$$\rho \sim \mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|<1)$$

We set the hyperparameters $\nu_{h_0} = 5$, $s_{h_0} = 0.04$, $\rho_0 = 0.9$ and $V_\rho = 0.04$ so that the prior mean of $\sigma_h^2$ is 0.01 and $\rho$ is centered at 0.9. 

### 3.4.3 Posterior Distribution

The posterior distribution specified above has the following form: 

$$p(Y|A, \Sigma) = (2\pi)^{-\frac{Tn}{2}}|\Sigma|^{-\frac{T}{2}}|\Omega|^{-\frac{N}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA))$$

and the joint posterior distribution

$$p(A, \Sigma \mid Y, \Omega)  = \frac{p(A,\Sigma,Y, \Omega)}{p(Y, \Omega)}$$
$$\propto p(A, \Sigma, Y, \Omega) \propto p(Y \mid A,\Sigma, \Omega)\times p(A,\Sigma, \Omega)$$
$$=  p(Y \mid A,\Sigma, \Omega) p(A, \Sigma) p(\Omega) = p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma) p(\Omega)$$

$$\propto p(Y \mid A,\Sigma, \Omega) p(A \mid \Sigma) p(\Sigma)$$



$$\propto |\Sigma|^{-\frac{T}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}(Y-XA)^T\Omega^{-1}(Y-XA)) \times$$

$$\mid \Sigma \mid^{-\frac{\nu_0+N+K}{2}}exp(-\frac{1}{2}tr(\Sigma^{-1}S_o))exp(-\frac{1}{2}tr(\Sigma^{-1}(A-A_0)^TV_A^{-1}(A-A_0))$$


$$\propto |\Sigma|^{-\frac{T+N+K+\nu_0+1}{2}} \times \exp (-\frac{1}{2}tr(\Sigma^{-1}S_0)) \times $$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A_0^TV_a^{-1}A_0+Y^T\Omega^{-1}Y-\bar{A}^T\bar{V}^{-1}\bar{A})\times$$
$$exp(-\frac{1}{2}tr(\Sigma^{-1}(A-\bar{A})^T\bar{V}^{-1}(A-\bar{A})))$$

Hence, 

$$p(A, \Sigma \mid Y, X) = p(A \mid Y, X, \Sigma) p(\Sigma \mid Y, X)$$


$$p(A \mid Y, X, \Sigma, \Omega) = \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$$

$$p(\Sigma \mid Y, X) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$$

where

$$\bar{V} = (X^T\Omega^{-1}X + V_A^{-1})^{-1}$$



$$\bar{A} = \bar{V}(X^T\Omega^{-1}Y + V_A^{-1}A_0)$$

$$\bar{\nu} = T + \nu_0$$

$$\bar{S} = S_0 + Y^T\Omega^{-1}Y + A_0^TV_A^{-1}A_0 - \bar{A}^T\bar{V}^{-1}\bar{A}$$

### 3.4.4 Estimation Procedure 


In this setting, we have 

- $(A,\Sigma) \sim \mathcal{NIW}_{K \times N} (A_0, V_A, S_0, \nu_0)$
- $p(A, \Sigma, \Omega) = p(A, \Sigma) \times p(\Omega)$

then, prior draws can be sampled from 

- $\Sigma \sim \mathcal{IW}(S_0, \nu_0)$
- $A|\Sigma \sim \mathcal{MN}_{K \times N}(A_0, \Sigma, V_A)$
- $\Omega = \left(
\begin{array}{cccc}
(1 + \psi_1^2) e^{h_1} & \psi_1 e^{h_1} & \cdots & 0 \\
\psi_1 e^{h_1} & \psi_1^2 e^{h_1} + e^{h_2} & \cdots & \vdots \\
0 & \cdots & \ddots & \vdots \\
\vdots & \cdots & \psi_1^2 e^{h_{T-2}} + e^{h_{T-1}} & \psi_1 e^{h_{T-1}} \\
0 & \cdots & \psi_1 e^{h_{T-1}} & \psi_1^2 e^{h_{T-1}} + e^{h_T}
\end{array}
\right)$
- $\epsilon_t = u_t + \psi_1 u_{t-1}$

- $\psi \sim \mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|<1)$

- $u_t \sim \mathcal{N}(0,e^{h_t} \Sigma)$

- $h_t = \rho h_{t-1} + u_t^h$

- $\rho \sim \mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|<1)$

- $u_t^h \sim N(0,\sigma_h^2)$

- $\sigma_h^2 \sim \mathcal{IG}(\nu_{h_0}, s_{h_0})$



To sample $S_1+S_2$ draws of $\left\{\Omega^{(s)}\right\}_{s=1}^{S_1+S_2}$

1. Sample $S_1+S_2$ draws of $\left\{\sigma^{2,(s)}_h\right\}_{s=1}^{S_1+S_2}$ from $\mathcal{IG}(v_{h_0}, s_{h_0})$
2. Sample $S_1+S_2$ draws of $\left\{\rho^{(s)}\right\}_{s=1}^{S_1+S_2}$ from $\mathcal{N}(\rho_0, V_\rho) \mathbb{1}(|\rho|<1)$
3. For each $\sigma^{2,(s)}_h$, sample $\left\{u_t^{h,(s)}\right\}_{t=1}^T$ from $N(0,\sigma_h^{2,(s)})$
4. For t = 1,...,T and s = 1,..., $S_1+S_2$, compute $h_t^{(s)} = \rho h_{t-1}^{(s)} + u_t^{h,(s)}$
5. Sample $S_1+S_2$ draws of $\left\{u_t^{(s)}\right\}_{s=1}^{S_1+S_2}$ from $u_t \sim \mathcal{N}(0,e^{h_t^{(s)}} \Sigma)$ for t = 1,...,T
6. Sample $S_1+S_2$ draws of $\left\{\psi^{(s)}\right\}_{s=1}^{S_1+S_2}$ from $\mathcal{N}(\psi_0, V_\psi) \mathbb{1}(|\psi|<1)$
7. for each t = 1,...,T and s = 1,...$S_1+S_2$, compute $\epsilon_t^{(s)} = u_t^{(s)} + \psi^{(s)}u_{t-1}^{(s)}$

After we have obtained $\left\{\Omega^{(s)}\right\}_{s=1}^{S_1+S_2}$, we can use the following Gibb's Sampler algorithm to sample from the posterior distribution $p(A \mid Y, X, \Sigma, \Omega)$: 


- Initialize $\Sigma$ at $\Sigma^0$ 
- For $s = 1,...,S_1+S_2$
    1. Draw a sample $A^{(s)}$ from $p(A \mid Y, X, \Omega^{(s)}, \Sigma^{(s-1)}) \sim \mathcal{MN}_{k \times N}(\bar{A}, \Sigma, \bar{V})$
    2. Draw a sample $\Sigma^{(s)}$ from $p(\Sigma \mid Y, X, A^{(s)}, \Omega^{(s)}) = \mathcal{IW}_N(\bar{S}, \bar{\nu})$
    
We discard the first $S_1$ sample draws to allow the algorithm to converge to the stationary posterior distributiion to obtain $S_2$ sampled draws from the joint posterior distribution. 

$$\left\{A^{(s)}, \Sigma^{(s)}\right\}_{s=S_1+1}^{S_1+S_2}$$

Sampling from the joint predictive density is the same as before. 

# 4. Model Estimation
## 4.1 Standard Bayesian VAR
### 4.1.1 Model Building Code and Validation 

We verify that our model can replicate the true parameter of the data generate process by: 
1. Generate artificial data containing 1000 observations simulated from a bi-variate Gaussian random walk process with the covariance matrix equal to the identity matrix of order 2. That is, 

$$\mathbf{y_t} = \begin{pmatrix} y_t,1 \\ y_t,2\end{pmatrix} = \mathbf{y_{t-1}} + \mathbf{\epsilon_t} = \begin{pmatrix}y_{t-1,1}\\y_{t-1, 2}\end{pmatrix} + \begin{pmatrix}\epsilon_{t,1}\\ \epsilon_{t, 2}\end{pmatrix}$$ and 

$$\mathbf{\epsilon} \sim iid \mathcal{N}(\mathbf{0}, \mathbf{I_2} )$$



```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
library(IMFData)
library(patchwork)
library(zoo)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tseries)
library(knitr)
library(MASS)
library(ggplot2)
library(reshape2)
library(progress)
library(psych)
library(truncnorm)
library(extraDistr)

set.seed(123)

n = 1000  
cov_matrix = diag(2)  

random_walk_sample = matrix(nrow = n, ncol = 2)
#initialize the random walk at 0,0
random_walk_sample[1, ] = c(0, 0)

for (i in 2:n) {
  random_walk_sample[i, ] = random_walk_sample[i - 1, ] + mvrnorm(n = 1, mu = c(0, 0), Sigma = cov_matrix)
  }
mat_plot = random_walk_sample 
colnames(mat_plot) = c("Series1", "Series2")
mat_plot = as.data.frame(mat_plot)
mat_plot$Index = 1:nrow(mat_plot) 
mat_plot = melt(mat_plot, id.vars = "Index")

ggplot(mat_plot, aes(x = Index, y = value, color = variable)) +
    geom_line() +
    labs(title = "1000 observations simulated from a bi-variate Gaussian random walk process", x = "", y = "") +
    theme_minimal()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
#if (!require(matrixNormal)) {install_packages('matrixNormal')}
BVAR_fit <- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N){
  N = ncol(Y)
  Ty = nrow(Y)
  S = S1 + S2
  Y = (data[(p+1):nrow(data),c(1,2)])
  X = matrix(1,nrow(Y),1) 
  for (i in 1:p){
    X     = cbind(X, (data[(p+1):nrow(data)-i,c(1,2)]))
  }
  V_bar = solve(t(X)%*%X + solve(V_prior))
  A_bar = V_bar%*%(t(X)%*%Y + solve(V_prior)%*%A_prior)
  nu_bar = Ty + nu_prior
  #S_bar  = S_prior + t(Y)%*%Y + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar = S_prior +  t(Y)%*%Y + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar_inv = solve(S_bar)

  #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = apply(Sigma_posterior,3,solve)
  Sigma_posterior  = array(Sigma_posterior,c(N,N,S))
  A_posterior = array(rnorm(prod(c(dim(A_bar),S))),c(dim(A_bar),S))
  L = t(chol(V_bar))
  for (s in 1:S){
    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,s])
    }
  A_posterior = A_posterior[,,(S1+1):(S1+S2)]
  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]
  return(list(A_posterior, Sigma_posterior))
}
```
Then we estimate a model with a constant term and 1 lag using the simulated data, show that the posterior mean of the autoregressive and the covariance matrices are close to an identity matrix and that the posterior mean of the constant term is close to a vector of zeros.


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=13}
p = 1
kappa1 = 0.02^2
kappa2 = 10^2
N = ncol(random_walk_sample)
A_prior             = matrix(0,1+p*N,N)
A_prior[2:(N + 1),] = diag(N)
V_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior     = diag(1, N)
nu_prior    = ncol(random_walk_sample)+1
S1 = 1000 
S2 = 2000
Y = (random_walk_sample[(p+1):nrow(random_walk_sample),c(1,2)])

posterior_samples = BVAR_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)

posterior_samples_A = posterior_samples[[1]]
posterior_samples_Sigma = posterior_samples[[2]]
print('Posterior mean of autoregressive parameter:')
round(apply(posterior_samples_A,1:2,mean),3)
print('Posterior standard deviation of autoregressive parameter:')
round(apply(posterior_samples_A,1:2,sd),3)
print('Posterior mean of covariance matrix:')
round(apply(posterior_samples_Sigma,1:2,mean),3)
print('Posterior standard deviation of covariance matrix:')
round(apply(posterior_samples_Sigma,1:2,sd),3)
```

### 4.1.2 Empirical Result

## 4.2 Large BVAR model with MA(1) Gaussian Innovations 
### 4.2.1 Model Building Code and Validation 
```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}


BVAR_MA_fit <- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N){
  Y = (data[(p+1):nrow(data),c(1,2)])
  N = ncol(Y)
  Ty = nrow(Y)
  S = S1 + S2
  X = matrix(1,nrow(Y),1) 
  K = 1+p*N
  for (i in 1:p){
    X     = cbind(X, (data[(p+1):nrow(data)-i,c(1,2)]))
  }
  
  nu_bar = Ty + nu_prior
  psi = 0
  H_psi = matrix(0, nrow = Ty, ncol = Ty)
  O_psi = diag(rep(1, Ty))
  O_psi[1,1] = 1+psi^2
  for (t in 1:Ty){
    H_psi[t,t] = 1
    if (t < Ty){
      H_psi[t+1, t] = psi
    }
  }
  A_posterior = array(rnorm(prod(c(c(K,N),S))),c(c(K,N),S))
  Sigma_posterior = array(dim = c(N,N,S))
  Omega_posterior = array(dim = c(Ty, Ty, S+1))
  Omega_posterior[,,1] = H_psi %*% O_psi %*% t(H_psi)
  
  
  pb <- progress_bar$new(total = S)
    
  for (s in 1:S){
    pb$tick()
    chol_Omega = chol(Omega_posterior[,,1]) #########change 
    chol_Omega_inv = solve(chol_Omega)
    #V_bar = solve(t(X)%*%X + solve(V_prior))
    V_bar = solve(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%X) + solve(V_prior))
    L = t(chol(V_bar))
    #A_bar = V_bar%*%(t(X)%*%Y + solve(V_prior)%*%A_prior)
    A_bar = V_bar%*%(t(chol_Omega_inv%*%X)%*%(chol_Omega_inv%*%Y) + solve(V_prior)%*%A_prior)
    #S_bar = S_prior +  t(Y)%*%Y + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
    S_bar = S_prior +  t(chol_Omega_inv%*%Y)%*%(chol_Omega_inv%*%Y) + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
    S_bar_inv = solve(S_bar)
    Sigma_posterior[,,s] = solve(rWishart(1, df=nu_bar, Sigma=S_bar_inv)[,,1])
    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,1]) ############change 
  }
  A_posterior = A_posterior[,,(S1+1):(S1+S2)]
  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]
  return(list(A_posterior, Sigma_posterior))}
```

```{r}
data = random_walk_sample
N = ncol(data)
p = 1
kappa1 = 0.02^2
kappa2 = 10^2
A_prior             = matrix(0,1+p*N,N)
A_prior[2:(N + 1),] = diag(N)
V_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior     = diag(1, N)
nu_prior    = ncol(random_walk_sample)+1
S1 = 100
S2 = 200

posterior_samples = BVAR_MA_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior)
posterior_samples_A = posterior_samples[[1]]
posterior_samples_Sigma = posterior_samples[[2]]
print('Posterior mean of autoregressive parameter:')
round(apply(posterior_samples_A,1:2,mean),3)
print('Posterior standard deviation of autoregressive parameter:')
round(apply(posterior_samples_A,1:2,sd),3)
print('Posterior mean of covariance matrix:')
round(apply(posterior_samples_Sigma,1:2,mean),3)
print('Posterior standard deviation of covariance matrix:')
round(apply(posterior_samples_Sigma,1:2,sd),3)

```



## 4.4  Large BVAR model with MA(1) Gaussian Innovations and Common Stochastic Volatility
### 4.4.1 Model Building Code and Validation 


```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
#if (!require(matrixNormal)) {install_packages('matrixNormal')}

BVAR_MA_fit <- function(data, p, S1, S2, kappa1, kappa2, A_prior, V_prior, S_prior, nu_prior, N, T, vho_prior, sh0_prior, rho_prior, Vrho_prior, psi_prior, Vpsi_prior){
  N = ncol(data)
  Ty = nrow(data)
  X = matrix(1,Ty,1)
  for (i in 1:p){
    X     = cbind(X,data[p+1:Ty-i,])
  }
  S = S1 + S2
  K = 1+p*N
 
  nu_bar = Ty + nu_prior
  #S_bar  = S_prior + t(data)%*%data + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
 
  #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)

 
  sigma_h_squared = rinvgamma(n = 1, alpha = vh0_prior, beta = sh0_prior)
  rho = rtnorm(n = 1, mean = rho_prior, sqrt(Vrho_prior), a = -1, b = 1)
  u_h = rnorm(n = Ty, mean = 0, sd = sqrt(sigma_h_squared))
  h = array(dim = Ty)
  u = array(dim = c(N,1,Ty))
  h [1] = rho * 0 + u_h[1]
  psi = rtnorm(n = 1, mean = psi_prior, sd = sqrt(Vpsi_prior), a = -1, b = 1)
  epsilon = array(dim = c(N,1,Ty))
  Omega = array(0, dim = c(Ty,Ty))
    
  for (t in 2:Ty){
    h[t] = rho*h[t-1] + u_h[t]
  } 
  
  Sigma_prior = rWishart(n = 1, df = nu_prior, Sigma = S_prior)[,,1]
  Sigma_prior = solve(Sigma_prior)
  
  for (t in 1:Ty){
    u_var = exp(h[t])*Sigma_prior
    u[,,t] = mvrnorm(n = 1, mu = rep(0,N), Sigma = u_var)
    if (t == 1){
      epsilon[,,t] = u[,,t]
      Omega[t,t] = (1+psi^2)*exp(h[t])
    }
    else{
      epsilon[,,t] = u[,,t] + psi*u[,,t-1]
      Omega[t,t] = psi^2*exp(h[t-1]) + exp(h[t])
      Omega[t-1,t] = psi * exp(h[t-1])
      Omega[t,t-1] = psi * exp(h[t-1]) 
    }
  }
  
  V_bar = solve(t(X)%*%solve(Omega)%*%X + solve(V_prior))
  A_bar = V_bar%*%(t(X)%*%solve(Omega)%*%data + solve(V_prior)%*%A_prior)
  nu_bar = Ty + nu_prior
  #S_bar  = S_prior + t(data)%*%data + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar = S_prior +  t(data)%*%solve(Omega)%*%data + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar_inv = solve(S_bar)
  #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = apply(Sigma_posterior,3,solve)
  Sigma_posterior  = array(Sigma_posterior,c(N,N,S))
  A_posterior = array(rnorm(prod(c(dim(A_bar),S))),c(dim(A_bar),S))
  L = t(chol(V_bar))
  
  for (s in 1:S){
    A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,s])
    }
  
  A_posterior = A_posterior[,,(S1+1):(S1+S2)]
  Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]
  return(list(A_posterior, Sigma_posterior))
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
p = 1
kappa1 = 0.1^2
kappa2 = 10^2
N = ncol(random_walk_sample)
A_prior     = matrix(0, 1+p*N, ncol = N)
V_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior     = diag(1, N)
nu_prior    = ncol(random_walk_sample)+1
S1 = 1000 
S2 = 2000
vh0_prior = 5
sh0_prior = 0.04
rho_prior = 0.9
Vrho_prior = 0.04
psi_prior = 0
Vpsi_prior = 1

posterior_samples_MA = BVAR_MA_fit(data = random_walk_sample, p = p, S1= S1, S2 = S2, kappa1 = kappa1, kappa2 = kappa2, A_prior = A_prior, V_prior = V_prior, S_prior = S_prior, nu_prior = nu_prior, vho_prior = vho_prior, sh0_prior = sh0_prior, rho_prior = rho_prior, Vrho_prior = Vrho_prior, psi_prior = psi_prior, Vpsi_prior = Vpsi_prior)

posterior_samples_A_MA = posterior_samples_MA[[1]]
posterior_samples_Sigma_MA = posterior_samples_MA[[2]]
print('Posterior mean of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,mean),3)
print('Posterior standard deviation of autoregressive parameter:')
round(apply(posterior_samples_A_MA,1:2,sd),3)
print('Posterior mean of covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,mean),3)
print('Posterior standard deviation of covariance matrix:')
round(apply(posterior_samples_Sigma_MA,1:2,sd),3)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, , fig.width=12, fig.height=6}
#Code for S draws f Omega

p = 1
kappa1 = 0.1^2
kappa2 = 10^2
N = ncol(random_walk_sample)
A_prior     = matrix(0, 1+p*N, ncol = N)
V_prior     = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N)))
S_prior     = diag(1, N)
nu_prior    = ncol(random_walk_sample)+1
vh0_prior = 5
sh0_prior = 0.04
rho_prior = 0
Vrho_prior = 0.04
psi_prior = 0
Vpsi_prior = 1

data = random_walk_sample
N = ncol(data)
Ty = nrow(data)
X = matrix(1,Ty,1)
S1 = 50
S2 = 50
S = S1 + S2
set.seed(123)
for (i in 1:p){
  X     = cbind(X,data[p+1:Ty-i,])
}

#dim 1 variables
sigma_h_squared = rinvgamma(n = S, alpha = vh0_prior, beta = sh0_prior)
rho = rtnorm(n = S, mean = rho_prior, sqrt(Vrho_prior), a = -1, b = 1)
psi = rtnorm(n = S, mean = psi_prior, sd = sqrt(Vpsi_prior), a = -1, b = 1)

#dim NxN variables
Sigma_prior = array(dim = c(N,N,S))

#dim Tx1  variables
u_h = array(dim = c(Ty,1,S))
h = array(dim = c(Ty,1,S))

# dim T by N variables
u = array(dim = c(N,1,Ty, S))
epsilon = array(dim = c(N,1,Ty, S))
uvar = array(dim = c(N,N,Ty,S))
#dim TxT variables
Omega = array(0, dim = c(Ty,Ty,S))

for (s in 1:S){
  u_h[,,s] = rnorm(n = Ty, mean = 0, sd = sqrt(sigma_h_squared[s]))
  h[1,1,s] = rho[s] * 0 + u_h[1,1,s]
  Sigma_prior[,,s] = rWishart(n = 1, df = nu_prior, Sigma = S_prior)[,,1]
  Sigma_prior[,,s] = solve(Sigma_prior[,,s])
  for (t in 2:Ty){
    h[t,1,s] = rho[s]*h[t-1,1,s] + u_h[t,1,s]
  }
  for (t in 1:Ty){
    uvar[,,t,s] = exp(h[t,1,s])*Sigma_prior[,,s]
    u[,,t,s] = mvrnorm(n=1,mu = rep(0,N), Sigma = uvar[,,t,s])
    if(t==1){
      epsilon[,,t,s] = u[,,t,s]
      Omega[t,t,s] = (1-psi[s]^2)*exp(h[t,1,s])
    }
    else{
      epsilon[,,t,s] = u[,,t,s] + psi[s]*u[,,t-1,s]
      Omega[t,t,s] = psi[s]^2 * exp(h[t-1,1,s])+exp(h[t,1,s])
      Omega[t-1,t,s] = psi[s] * exp(h[t-1,1,s])
      Omega[t,t-1,s] = psi[s] * exp(h[t-1,1,s])
    }
  }
}



for (s in 1:S){
  V_bar = solve(t(X)%*%solve(Omega[,,s])%*%X + solve(V_prior))
  A_bar = V_bar%*%(t(X)%*%solve(Omega[,,s])%*%data + solve(V_prior)%*%A_prior)
  nu_bar = Ty + nu_prior
  #S_bar  = S_prior + t(data)%*%data + t(A_prior)%*%diag(1/diag(V_prior))%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar = S_prior +  t(data)%*%solve(Omega[,,s])%*%data + t(A_prior)%*%solve(V_prior)%*%A_prior - t(A_bar)%*%solve(V_bar)%*%A_bar
  S_bar_inv = solve(S_bar)
  #Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = rWishart(S, df=nu_bar, Sigma=S_bar_inv)
  Sigma_posterior = apply(Sigma_posterior,3,solve)
  Sigma_posterior  = array(Sigma_posterior,c(N,N,S))
  A_posterior = array(rnorm(prod(c(dim(A_bar),S))),c(dim(A_bar),S))
  L = t(chol(V_bar))
  A_posterior[,,s]= A_bar + L%*%A_posterior[,,s]%*%chol(Sigma_posterior[,,s])
  }
A_posterior = A_posterior[,,(S1+1):(S1+S2)]
Sigma_posterior = Sigma_posterior[,,(S1+1):(S1+S2)]

round(apply(A_posterior,1:2,mean),3)
round(apply(A_posterior,1:2,sd),3)
round(apply(Sigma_posterior,1:2,mean),3)
round(apply(Sigma_posterior,1:2,sd),3)
```
### 4.4.2 Empirical Result

```


<!-- Furthermore, a non-Gaussian framework allows the modeling of asymmetries in how economies react to positive versus negative shocks. Traditional BVAR models with Gaussian assumptions is often unable to capture such nuances, while flexible error structures can incorporate the differing impacts of shocks of different magnitudes or directions. 
economic variables are often characterized by distributions that are far from Gaussian, often featuring fat tails and skewness. Secondly, -->